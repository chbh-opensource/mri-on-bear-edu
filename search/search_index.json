{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MRI on BEAR","text":"<p>MRI on BEAR is a collection of educational resources created by members of the Centre for Human Brain Health (CHBH), University of Birmingham, to provide a basic introduction to fundamentals in magnetic resonance imaging (MRI) data analysis, using the computational resources available to the University of Birmingham research community.</p>"},{"location":"#about-this-website","title":"About this website","text":"<p>This website contains workshop materials created for the MSc module 'Magnetic Resonance Imaging in Cognitive Neuroscience' (MRICN) and its earlier version (Fundamentals in Brain Imaging) at the School of Psychology, University of Birmingham. It is a ten-week course consisting of lectures and workshops introducing the main techniques of functional and structural brain mapping using MRI with a strong emphasis on - but not limited to - functional MRI (fMRI). Topics include the physics of MRI, experimental design for neuroimaging experiments and the analysis of fMRI, and other types of MRI data. This website includes only the workshop materials, which provide a basic training in analysis of brain imaging data and data visualization. </p> <p>Learning objectives</p> <p>At the end of the course you will be able to:</p> <ul> <li>Demonstrate an understanding of the basic concepts involved in MRI</li> <li>Show an understanding of how to design fMRI experiments</li> <li>Have the ability to work with BlueBEAR in a Linux environment and to use appropriate software to view and interpret MRI data</li> <li>Be able to analyse simple fMRI experiments and conduct basic tractography analysis</li> </ul> <p>For externals not on the course</p> <p>Whilst we have made these resources publicly available for anyone to use, please BEAR in mind that the course has been specifically designed to run on the computing resources at the University of Birmingham.</p>"},{"location":"#teaching-staff","title":"Teaching Staff","text":"Dr Magdalena Chechlacz <p>Role: Course Lead</p> <p>Magdalena Chechlacz is an Assistant Professor in Cognition and Ageing at the School of Psychology, University of Birmingham. She initially trained and carried out a doctorate in cellular and molecular biology (2002). After working as a biologist (Larry L. Hillblom Foundation Fellowship at the University of California, San Diego) she decided on a career change to a more human-oriented science and neuroimaging. In order to gain formal training in cognitive neuroscience and neuroimaging, she completed a second doctorate in psychology at the University of Birmingham under the supervision of Glyn Humphreys (2012). From 2013 to 2016, she held a British Academy Postdoctoral Fellowship and EPA Cephalosporin Junior Research Fellowship, Linacre College at the University of Oxford. In 2016, Dr Chechlacz returned to the School of Psychology, University of Birmingham as a Bridge Fellow.</p>  m.chechlacz@bham.ac.uk  0000-0003-1811-3946 <p></p> <p> </p> Aamir Sohail <p>Role: Teaching Assistant</p> <p>Aamir Sohail is an MRC Advanced Interdisciplinary Methods (AIM) DTP PhD student based at the Centre for Human Brain Health (CHBH), University of Birmingham, where he is supervised by Lei Zhang and Patricia Lockwood. He completed a BSc in Biomedical Science at Imperial College London, followed by an MSc in Brain Imaging at the University of Nottingham. He then worked as a Junior Research Fellow at the Centre for Integrative Neuroscience and Neurodynamics (CINN), University of Reading. Outside of research, he is also passionate about facilitating inclusivity and diversity in academia, as well as promoting open and reproducible science.</p>  axs2210@bham.ac.uk  sohaamir  AamirNSohail  0009-0000-6584-4579  sohaamir.github.io <p></p> <p>Accessing additional course materials</p> <p>If you are a CHBH member and would like access to additional course materials (lecture recordings etc.), please contact one of the teaching staff members listed above.</p> <p></p>"},{"location":"contributors/","title":"Contributors","text":"<p>Many thanks to our contributors for creating and maintaining these resources!</p> <sub>Andrew Quinn</sub>\ud83d\udea7 \ud83d\udd8b <sub>Aamir Sohail</sub>\ud83d\udea7 \ud83d\udd8b <sub>James Carpenter</sub>\ud83d\udd8b <sub>Magda Chechlacz</sub>\ud83d\udd8b <p>Acknowledgements</p> <p>Thank you to Charnjit Sidhu for their assistance with running the course!</p> License <p>MRI on BEAR is hosted on GitHub. All content in this book (ie, any files and content in the <code>docs/</code> folder) is licensed under the Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license. Please see the <code>LICENSE</code> file in the GitHub repository for more details.</p>"},{"location":"resources/","title":"Additional Resources","text":"<p>For those wanting to develop their learning beyond the scope of the module, here is a (non-exhaustive) list of links and pages for neuroscientists covering skills related to working with neuroimaging data, both with the concepts and practical application. </p> <p>Contributing to the list</p> <p>Feel free to suggest additional resources to the list by opening a thread on the GitHub page!</p> <p>FSL Wiki</p> <p>Most relevant to the course is the FSL Wiki, the comprehensive guide for FSL by the Wellcome Centre for Integrative Neuroimaging at the University of Oxford.</p>"},{"location":"resources/#existing-lists-of-resources","title":"Existing lists of resources","text":"<p>Here are some current 'meta-lists' which already cover a lot of resources themselves: </p> <ul> <li>Methods in Neuro Steven Weisberg's GitHub extensive list of resources covering the physics of MRI/fMRI, computational/programming, tools for the analysis of MRI/fMRI data, and online datasets, as part of his 'Methods in Neuroimaging' course at the University of Florida.</li> <li>Hitchhacker's guide to the brain A 'docs' style website with lists of resources for each stage of neuroimaging analysis including file organisation, planning. preregistration, data collection, pre-processing and analysis, and sharing data. By Remi Gau, McGill University and others.</li> <li>On-line neuroimaging resources A farily comprehensive list of 'softwares, databases, tutorials, blogs and other resources relevant to learn about neuroimaging or to help perform neuroimaging analysis'. Curated by Remi Gau, McGill University.</li> <li>Dartbrains A notebook style introduction to neuroimaging in Python. The materials cover how scanner generates data, how psychological states can be probed in the scanner, and how this data can be processed and analyzed. Created by Luke Chang, Dartmouth College.</li> <li>Awesome Magnetic Resonance Imaging (MRI) 'A curated list of delightful Magnetic Resonance (MR) courses, books, lectures, papers, blogs and free resources.' Created by Daniel Gomez, Harvard/MIT.</li> <li>Awesome Neuroscience 'A curated list of awesome neuroscience libraries, software and any content related to the domain.'  Created by Akash Tandon.</li> <li>fMRI-Resources A GitHub list not dissimilar to this one, providing information and resources on functional MRI. Created by John Pyles.</li> </ul>"},{"location":"resources/#neuroimaging","title":"Neuroimaging","text":"Conceptual understanding <p>Struggling to grasp the fundamentals of MRI/fMRI? Want to quickly refresh your mind on the physiological basis of the BOLD signal? Well, these resources are for you!</p> <ul> <li>Principles of fMRI The OG YouTube series for understanding the conceptual basis of MRI/fMRI. Created by Martin Lindquist and Tor Wager of Dartmouth College.</li> <li>Neuroimaging Research Methods Another YouTube channel for learning about MRI/fMRI including research methods. Created by Rasmus Birn, University of Wisconsin-Madison.</li> <li>Introduction to Principles of MRI A short book and associated simulation code for learning the principles of magnetic resonance imaging (MRI). Created by Peder Larson for students at UCSF.</li> <li>Questions and Answers in MRI Ever had a question about the basis of MRI/fMRI? Written from the perspective of a physicist, this website was specifically made to answer these questions. Created by Allen Elster, Washington University School of Medicine.</li> <li>fMRI Bootcamp A lecture series on fMRI, both conceptual and methodological by Rebecca Saxe, MIT.</li> <li>MIT 9.13 The Human Brain, Spring 2019 A course which 'surveys the core perceptual and cognitive abilities of the human mind and explores how they are implemented in the brain'. Delivered by Nancy Kanwisher, MIT.</li> </ul> Analysis of fMRI data <ul> <li>Introduction to Working with MRI Data in Python A Software Carpentries course covering MRI file types, organisational formats (e.g., BIDS) and working with open datasets.</li> <li>Andy's Brain Book The OG of neuroimaging tutorials. I don't know many trainee neuroimagers who haven't used Andy's amazing guides. Highly recommended to also check out his YouTube channel as well. Created by Andrew Jahn, University of Michigan.</li> <li>NI-edu A website covering two courses, \u201cfMRI-introduction\u201d (basic concepts and methodology of functional MRI (fMRI) research) and \u201cfMRI-pattern-analysis\u201d (machine-learning based \u2018decoding\u2019 and representational similarity analysis (RSA)), which are in a notebook format. Created by Lucas Snoek, University of Amsterdam.</li> <li>U of A: Neuroimaging Core Documentation Docs covering a range of neuroimaging tutorials including BIDS, ANTS, FSL, ITK-SNAP and more. Created by Dianne Paterson, University of Arizona.</li> <li>Data analysis for Neuroimaging (DAFNI) Denis Schluppeck's materials for the MSc Cognitive Neuroscience course at the University of Nottingham, covering SPM, git, FSL and MATLAB.</li> <li>Practice and theory of brain imaging A comprehensive course on neuroimaging in Python, with modules on reproducibility in programming/neuroimaging. Created by the Nipraxis team (Matthew Brett, Chris Markiewicz, Oscar Estaban, Zvi Baratz, Peter Rush).</li> <li>Psych 214 \u2013 functional MRI methods A 'hands-on course teaching the principles of functional MRI (fMRI) data analysis' created for students at UC Berkeley by Matthew Brett and JB Poline.</li> </ul>"},{"location":"resources/#programming","title":"Programming","text":"Unix/Linux <ul> <li>The Unix Shell Software Carpentries workshop on Unix.</li> <li>Ubuntu Linux Guide Nutanix's guide to Linux on Ubuntu.</li> </ul>"},{"location":"resources/#textbooks","title":"Textbooks","text":"<ul> <li> <p>An Introduction to Resting State fMRI Functional Connectivity (2017, Oxford University Press) by Janine Bijsterbosch, Stephen M. Smith, and Christian F. Beckmann</p> </li> <li> <p>Handbook of Functional MRI Data Analysis (2011, Cambridge University Press) by Russell A. Poldrack, Jeanette A. Mumford, and Thomas E. Nichols</p> </li> <li> <p>Introduction to Functional Magnetic Resonance Imaging (1998, Cambridge University Press) by Richard B. Buxton</p> </li> <li> <p>Introduction to Neuroimaging Analysis (2018, Oxford University Press) by Mark Jenkinson and Michael Chappell</p> </li> <li> <p>Short Introduction to Brain Anatomy for Neuroimaging (2018, Oxford University Press) by Mark Jenkinson and Michael Chappell</p> </li> <li> <p>Short Introduction to the General Linear Model (2018, Oxford University Press) by Mark Jenkinson and Michael Chappell</p> </li> <li> <p>Short Introduction to MRI Physics (2018, Oxford University Press) by Mark Jenkinson and Michael Chappell</p> </li> </ul>"},{"location":"setting-up/","title":"Accessing BlueBEAR and the BEAR Portal","text":"<p>Before you start with any workshop materials, you will need to familiarise yourself with the CHBH\u2019s primary computational resource, BlueBEAR. The following pages are aimed at helping you get started. </p> <p>To put these workshop materials into practical use you will be expected to understand what BlueBEAR is, what it is used for and to make sure you have access.</p> <p>Student Responsibility</p> <p>If you are an MSc student taking the MRICN module, please note that while there will be help available during all in person workshops, in case you have any problems with using the BEAR Portal, it is your responsibility to make sure that you have access, and that you are familiar with the information provided in pre-workshop materials. Failing to gain an understanding of BlueBEAR and using the BEAR Portal will prevent you from participating in the practical sessions and completing the module\u2019s main assessment (data analysis). </p>"},{"location":"setting-up/#what-are-bear-and-bluebear","title":"What are BEAR and BlueBEAR?Signing in to the BEAR Portal","text":"<p>BEAR stands for Birmingham Environment for Academic Research and is a collection of services provided specifically for researchers at the University of Birmingham. BEAR services are used by researchers at the Centre for Human Brain Health (CHBH) for various types of neuroimaging data analysis.</p> <p>BEAR services and basic resources - such as the ones we will be using for the purpose of the MRICN module - are freely available to the University of Birmingham research community. Extra resources which may be needed for some research projects can be purchased e.g., access to dedicated nodes and extra storage. This is something your PI/MSc/PhD project supervisor might be using and will give you access to.</p> <p> </p> <p>BlueBEAR refers to the Linux High Performance Computing (HPC) environment which:</p> <ol> <li>Enables researchers to run jobs simultaneously on many servers (thus providing fast and efficient processing capacity for data analysis)</li> <li>Gives easy access to multiple apps, software libraries (e.g., software we will be using in this module to analyse MRI data), as well as various software development tools</li> </ol> <p>As computing resources on BlueBEAR rely on Linux, in Workshop 1 you will learn some basic commands, which you will need to be familiar with to participate in subsequent practical sessions and to complete the module\u2019s main assessment (data analysis assessment). More Linux commands and basic principle of scriptings will be introduced in subsequent workshops.</p> <p>There are two steps to gaining access to BlueBEAR:</p> <ul> <li>Being a member on an active BEAR project </li> <li>Having a BEAR Linux account</li> </ul> <p>Gaining access to BEAR Projects</p> <p>Only a member of academic staff e.g., your project supervisor or module lead, can apply for a BEAR project. As a student you cannot apply for a BEAR project. If you are registered as a student on the MRICN module, you should have already been added as member to the project <code>chechlmy-chbh-mricn</code>. If not please contact one of the teaching staff.</p> <p>Even if you are already a member of a BEAR project giving you BlueBEAR access, you will still need to activate your BEAR Linux account via the self-service route or the service desk form. The information on how to do it and step-by-step instructions are available on the BEAR website, see the following link.</p> <p>Please follow these steps as above to make sure you have a BEAR Linux account before starting with workshop 1 materials. To do this you will need to be on campus or using the University Remote Access Service (VPN). </p> <p>After you have activated your BEAR Linux account, you can now sign-in to the BEAR Portal.</p> <ul> <li>In a web browser navigate to https://portal.bear.bham.ac.uk to access the BEAR Portal</li> <li>To log in, please use your university username and password </li> <li>First select the \u2018University of Birmingham\u2019 button as below and next log in via the University\u2019s Single Sign-On page </li> </ul> <p>BEAR Portal access requirements</p> <p>Remember that the BEAR Portal is only available on campus or using the VPN!</p> <p> </p> <p>If your log in is successful, you will be directed to the main BEAR Portal page as below. This means that you have successfully launched the BEAR Portal.</p> <p> </p> <p>If you get to this page, you are ready for Workshop 1. For now, you can log out. If you have any problems logging on to BEAR Portal, please email chbh-help@contacts.bham.ac.uk for help and advice. </p>"},{"location":"setting-up/#bear-storage","title":"BEAR Storage","text":"<p>The storage associated with each BEAR project is called the BEAR Research Data Store (RDS). Each BEAR project gets 3TB of storage space for free, but researchers (e.g., your MSc project supervisor) can pay for additional storage if needed. The RDS should be used for all data, job scripts and output on BlueBEAR.</p> <p> </p> <p>If you are registered as a student on the MRICN module, all the data and resources you will need to participate in the MRICN workshops and to complete the module\u2019s main assessment have been added to the MRICN module RDS, and you have been given access to the folder <code>/rds/projects/c/chechlmy-chbh-mricn</code>. When working on your MSc project using BEAR services, your supervisor will direct you to the relevant RDS project.</p> <p>External access to data</p> <p>If you are not registered on the module and would like access to the data, please contact one of the teaching staff members.</p>"},{"location":"setting-up/#finding-additional-information","title":"Finding additional information","text":"<p>There is extensive BEAR technical documentation provided by the University of Birmingham BEAR team (see links below). While for the purpose of this module, you are not expected to be familiar with all the provided there information, you might find it useful if you want to know more about computing resources available to researchers at CHBH via BEAR services, especially if you will be using BlueBEAR for other purposes (e.g., for your MSc project).</p> <p>You can find out more about BEAR, BlueBEAR and RDS on the dedicated BEAR webpages: </p> <ul> <li> <p>University of Birmingham BEAR Homepage</p> </li> <li> <p>More information on BlueBEAR</p> </li> <li> <p>More information on Research Data Storage</p> </li> </ul>"},{"location":"workshop1/intro-to-bluebear/","title":"Introduction to the BlueBEAR Portal","text":"<p>At this point you should know how to log in and access the main BEAR Portal page. </p> <p>Please navigate to https://portal.bear.bham.ac.uk, log in and launch the BEAR Portal; you should get to the page as below.</p> <p> </p> <p>BlueBEAR Portal is a web-based interface enabling access to various BEAR services and BEAR apps including:</p> <ul> <li>Files in RDS storage</li> <li>Data science apps and software</li> <li>BlueBEAR GUI </li> <li>Code Server Editor</li> <li>Submitting jobs to run on BlueBEAR cluster</li> <li>Information on currently running jobs and interactive sessions.</li> </ul> <p> BlueBEAR portal is basically a user friendly alternative to using the command line interface, your computer terminal.</p> <p> </p> <p>To view all files and data you have access to on BlueBEAR, click on 'Files' as illustrated above. You will see your home directory (your BEAR Linux home directory), and all RDS projects you are a member of. </p> <p>You should be able to see <code>/rds/projects/c/chechlmy-chbh-mricn</code> (MRICN module\u2019s RDS project). By selecting the 'Home Directory' or any 'RDS project' you will open a second browser tab, displaying the content. In the example below, you see the content of one of Magda's projects.</p> <p> </p> <p>Inside the module\u2019s RDS project, you will find that you have a folder labelled xxx, where xxx is your University of Birmingham ADF username. If you navigate to that folder <code>rds/projects/c/chechlmy-chbh-mricn/xxx</code>, you will be able to perform various file operations from there. However, for now, please do not move, download, or delete any files.</p> <p>Data confidentiality</p> <p>Please also note that the MRI data you will be given to work with should be used on BlueBEAR only and not downloaded on your personal desktop or laptop!</p>"},{"location":"workshop1/intro-to-bluebear/#launching-the-bluebear-gui","title":"Launching the BlueBEAR GUI","text":"<p>The BlueBEAR Portal options in the menu bar, 'Jobs', 'Clusters' and 'My Interactive Sessions' can be used to submit and edit jobs to run on the BlueBEAR cluster and to get information about your currently running jobs and interactive sessions. Some of these processes can be also executed using Code Server Editor (VS Code) accessible via Interactive Apps. We won\u2019t explore these options in detail now but some of these will be introduced later when needed. </p> <p> </p> <p>For example, from the 'Cluster' option you can jump directly on BlueBEAR terminal and by using this built-in terminal, submit data analysis jobs and/or employ own contained version of neuroimaging software rather than software already available on BlueBEAR. We will cover containers, scripting and submitting jobs in later workshops. For now, just click on this option and see what happens; you can subsequently exit/close the terminal page.</p> <p>Finally, from the BlueBEAR Portal menu bar you can select 'Interactive Apps' and from there access various GUI applications you wish to use, including JupyterLab, RStudio, MATLAB and most importantly the BlueBEAR GUI, which we will be using to analyse MRI data in the subsequent workshops. </p> <p> </p> <p>Please select 'BlueBEAR GUI'. This will bring up a page for you to specify options for your job to start the BlueBEAR GUI. You can leave some of these options as default. But please change \u201cNumber of Hours\u201d to 2 (our workshops will last 2 hours; for some other analysis tasks you might need more time) and make sure that the selected 'BEAR Project' is <code>chechlmy-chbh-mricn</code>. Next click on Launch.</p> <p> </p> <p>It will take few minutes for the job to start. Once it\u2019s ready you\u2019ll see an option to connect to the BlueBEAR GUI. Click on 'Launch BlueBEAR GUI'. </p> <p> </p> <p>Once you have launched the BlueBEAR GUI, you are now in a Linux environment, on a Linux Desktop. The following section will guide you on navigating and using this environment effectively.</p> <p>Re-launching the BlueBEAR GUI</p> <p>In the main window of the BlueBEAR portal you will be able to see that you have an Interactive session running (the information above will remain there). This is important as if you close the Linux Desktop by mistake, you can click on Launch BlueBEAR GUI again to open it.</p>"},{"location":"workshop1/intro-to-linux/","title":"Introduction to Linux","text":"<p>Linux is a computer Operating System (OS) similar to Microsoft Windows or Mac OS. Linux is very widely used in the academic world especially in the sciences. It is derived from one of the oldest and most stable and used OS platforms around, Unix. We use Linux on BlueBEAR. Many versions of Linux are freely available to download and install, including CentOS (Community ENTerprise Operating System) and Debian, which you might be familiar with. You can also use these operating systems with Microsoft Windows in Dual Boot Environment on your laptop or desktop computer. </p> <p>Linux and neuroimaging</p> <p>Linux is particularly suited for clustering computers together and for efficient batch processing of data. All major neuroimaging software runs on Linux. This includes FSL, SPM, AFNI, and many others. Linux, or some version of Unix, is used in almost all leading neuroimaging centres. Both MATLAB and Python also run well in a Linux environment.</p> <p>If you work in neuroimaging, it is to your advantage to become familiar with Linux. The more familiar you are, the more productive you will become. For some of you, this might be a challenge. The environment will present a new learning experience, one that will take time and effort to learn. But in the end, you should hopefully realize that the benefits of learning to work in this new computer environment are indeed worth the effort. </p> <p>Linux is not like the Windows or Mac OSX environments. It is best used by typing commands into a Terminal client and by writing small batch command programs. Frequently you may not even need to use the mouse. Using the Linux environment alone may take some getting used to, but will become more familar throughout the course, as we use them to navigate through our file system and to script our analyses. For now, we will simply explore using the Linux terminal and simple commands.</p>"},{"location":"workshop1/intro-to-linux/#using-the-linux-terminal","title":"Using the Linux Terminal","text":"<p>BlueBEAR GUI enables to load various apps and applications by using the Linux environment and a built-in Terminal client. Once you have launched the BlueBEAR GUI, you will see a new window and from there you can open the Terminal client. There are different ways to open Terminal in BlueBEAR GUI window as illustrated below.</p> <p>Either by selecting from the drop-down menu:</p> <p> </p> <p> Or by selecting the folder at the bottom of the screen:</p> <p> </p> <p> In either case you will load the terminal:</p> <p> </p> <p> Once you have started the terminal you, you will be able to load required applications (e.g., to start the FSL GUI). FSL (FMRIB Software Library) is a neuroimaging software package we will be using in our workshops for MRI data analysis. </p> <p>When using the BlueBEAR GUI Linux desktop, you can simultaneously work in four separate spaces/windows. For example, if you are planning on using multiple apps, rather than opening multiple terminals and apps in the same space, you can move to another space. You can do that by clicking on \u201cworkspace manager\u201d in Linux desktop window.</p> <p> </p> <p>Linux is fundamentally a command line-based operating system, so although you can use the GUI interface with many applications, it is essential you get used to issuing commands through the Terminal interface to improve your work efficiency. </p> <p>Make sure you have an open Terminal as per the instructions above. Note that a Terminal is a text-based interface, so generally the mouse is not much use. You need to get used to taking your hand off the mouse and letting go of it. Move it away, out of reach. You can then get used to using both hands to type into a Terminal client. </p> <p><code>[chechlmy@bear-pg0210u07a ~]$</code> as shown above in the Terminal Client is known as the system prompt. The prompt usually identifies the user and the system hostname. You can type commands at the system prompt (press the Enter key after each command to make it run). The system then returns output based on your command to the same Terminal. </p> <p>Try typing <code>ls</code> in the Terminal.</p> <p>This command tells Linux to print a list of the current directory contents. We will get back later to basic Linux commands, which you should learn to use BlueBEAR for neuroimaging data analysis. </p> <p>Why bother with Linux?</p> <p>You may wonder why you should invest the time to learn the names of the various commands needed to copy files, change directories and to do general things such as run image analysis programs via the command line. This may seem rather clunky. However, the commands you learn to run on the command line in a terminal can alternatively be written in a text file. This text file can then be converted to a batch script that can be run on data sets using the BlueBEAR cluster, potentially looping over hundreds or thousands of different analyses, taking many days to run. This is vastly more efficient and far less error prone than using equivalent graphical tools to do the same thing, one at a time.</p> <p>When you open a new terminal window it opens in a particular directory. By default, this will be your home directory: </p> <p><code>/rds/homes/x/xxx</code> </p> <p>or the Desktop folder in your home directory:</p> <p><code>/rds/homes/x/xxx/Desktop</code>(where x is the first letter of your last name and xxx is your University of Birmingham ADF username).</p> <p>On BlueBEAR files are stored in directories (folders) and arranged into a tree hierarchy. </p> <p>Examples of directories on BlueBEAR include:</p> <ul> <li><code>/rds/homes/x/xxx</code> (your home directory) </li> <li><code>/rds/projects/c/chechlmy-chbh-mricn</code> (our module RDS project directory) </li> </ul> <p>Directory separators on Linux and Windows</p> <p>/ (forward slash) is the Linux directory separator. Note that this is different from Windows (where the backward slash \\ is the directory separator).</p> <p>The current directory is always called <code>.</code> (i.e. a single dot).</p> <p>The directory above the current directory is always called <code>..</code> (i.e. dot dot) </p> <p>Your home directory can always be accessed using the shortcut <code>~</code> (the tilde symbol). Note that this is the same as <code>/rds/homes/x/xxx</code>.</p> <p>You need to remember this to use and understand basic Linux Commands.</p>"},{"location":"workshop1/intro-to-linux/#basic-linux-commands","title":"Basic Linux Commands","text":"<p>pwd (Print Working Directory) </p> <p>In a Terminal type <code>pwd</code> followed by the return (enter) key to find out the name of the directory where you are.  You are always in a directory and can (usually) move to directories above you or below to subdirectories. </p> <p>For example if you type <code>pwd</code> in your terminal you will see: <code>/rds/homes/x/xxx</code>    (e.g., <code>/rds/homes/c/chechlmy</code>)</p> <p>cd (Change Directory) </p> <p>In a Terminal window, type <code>cd</code> followed by the name of a directory to gain access to it. Keep in mind that you are always in a directory and normally are allowed access to any directories hierarchically above or below.</p> <p>Type in your terminal the examples below:</p> <p><code>cd /rds/projects</code></p> <p><code>cd /rds/homes/</code></p> <p><code>cd ..</code> (to change to the directory above using .. shortcut) </p> <p>To find out where you are now, type <code>pwd</code>:</p> <p>(answer: <code>/rds</code>)</p> <p>If the directory is not located above or below the current directory, then it is often less confusing to write out the complete path instead.  Try this in your terminal:</p> <p><code>cd /rds/homes/x/xxx/Desktop</code> (where x is the first letter of your last name and xxx is your ADF username)</p> <p>Changing directories with full paths</p> <p>Note that it does not matter what directory you are in when you execute this command, the directory will always be changed based on the full pathway you specified. </p> <p>Remember that the tilde symbol <code>~</code> is a shortcut for your home directory. Try this: </p> <pre><code>cd /rds/projects \ncd ~ \npwd\n</code></pre> <p>You should be now back in your home directory.</p> <p>ls (List Files) </p> <p>The <code>ls</code> command (lowercase L, S) allows you to see a summary list of the files and directories located in the current directory. Try this: </p> <pre><code>cd /rds/projects/c\nls\n</code></pre> <p>(you should now see a long list of various BEAR RDS projects)</p> <p>Before moving to the next section, please close your terminal by clicking on \u201cx\u201d in the top right of the Terminal.</p> <p>cp (Copy files/directories) </p> <p>The <code>cp</code> command will copy files and/or directories FROM a source TO a destination in the current working directory. This command will create the destination file if it doesn't exist. In some cases, to do that you might need to specify a complete path to a file location.</p> <p>Here are some examples (please do not type them, they are only examples):</p> Command Function <code>cp myfile yourfile</code> Basic file copy (in current directory) <code>cp data data_copy</code> Copy a directory (but not sub-directories) <code>cp -r ~fred/data .</code> Recursively copy <code>fred</code> dir to current dir <code>cp ~fred/fredsfile myfile</code> Copy remote file and rename it <code>cp ~fred/* .</code> Copy all files from <code>fred</code> dir to current dir <code>cp ~fred/test* .</code> Copy all files that begin with test e.g. <code>test</code>, <code>test1.txt</code> <p>In the subsequent workshops we will practice using the <code>cp</code> command. For now, looking at the examples above to understand its usage. There are also some exercises below to check your understanding.</p> <p>mv, rmdir and mkdir (Moving, removing and making files/directories) </p> <p>The <code>mv</code> command will move files FROM a source TO a destination. It works like copy, except the file is actually moved. If applied to a single file, this effectively changes the name of the file. (Note there is no separate renaming command in Linux). The command also works on directories. </p> <p>Here are some examples (again please do not type these in): </p> Command Function <code>mv myfile yourfile</code> renames file <code>mv ~/data/somefile somefile</code> moves file <code>mv ~/data/somefile yourfile</code> moves and renames <code>mv ~/data/* .</code> moves multiple files <p>There are also the <code>mkdir</code> and <code>rmdir</code> commands:</p> <ul> <li><code>mkdir</code> \u2013 to make a new directory e.g.        <code>mkdir testdir</code> </li> <li><code>rmdir</code> \u2013 to remove an empty directory e.g.   <code>rmdir testdir</code> </li> </ul> <p>You can try these two commands. Open a new Terminal and type:</p> <pre><code>mkdir testdir\nls\n</code></pre> <p>In your home directory you will see now a new directory <code>testdir</code>. Now type:</p> <pre><code>rmdir testdir\nls\n</code></pre> <p>You should notice that the <code>testdir</code> has been removed from your home directory.</p> <p>To remove a file you can use the <code>rm</code> command. Note that once files are deleted at the command line prompt in a terminal window, unlike in Microsoft Windows, you cannot get files back from the wastebin.</p> <p>e.g.    <code>rm junk.txt</code> (this is just an example, do not type it in your terminal)</p> <p>Clearing your terminal</p> <p>Often when running many commands, your terminal will be full and difficult to understand. To clear the terminal screen type <code>clear</code>. This is an especially helpful command when you have been typing lots of commands and need a clean terminal to help you focus.</p> Linux commands in general  <p>Note that most commands in Linux have a similar syntax:  <code>command name [modifiers/options] input output</code></p> <p>The syntax of the command is very important. There must be spaces in between the different parts of the command. You need to specify input and output. The modifiers (in brackets) are optional and may or may not be needed depending on what you want to achieve. </p> <p>For example, take the following command: </p> <p><code>cp -r /rds/projects/f/fred/data ~/tmp</code>  (This is an example, do not type this) </p> <p>In the above example <code>-r</code> is an option meaning 'recursive' often used with <code>cp</code> and other commands, used in this case to copy a directory including all its content from one directory to another directory.</p>"},{"location":"workshop1/intro-to-linux/#opening-fsl-on-the-bluebear-gui","title":"Opening FSL on the BlueBEAR GUI","text":"<p>FSL (FMRIB Software Library) is a software library containing multiple tools for processing, statistical analyses, and visualisation of magnetic resonance imaging (MRI) data. Subsequent workshops will cover usage of some of the FSL tools for structural, functional and diffusion MRI data. This workshop only covers how to start FSL app on BlueBEAR GUI Linux desktop, and some practical aspects of using FSL, specifically running it in the terminal either in the foreground or in the background. </p> <p>There are several different versions of FSL software available on BlueBEAR. You can search which versions of FSL are available on BlueBEAR as well as all other available software using the following link: https://bear-apps.bham.ac.uk</p> <p>From there you will also find information how to load different software. Below you will find an example of loading one of the available versions of FSL.</p> <p>To open FSL in terminal, you first need to load the FSL module. To do this, you need to type in the Terminal a specific command. </p> <p>First, either close the Terminal you have been previously using and open a new one, or simply clean it. Next, type:</p> <p><code>module load FSL/6.0.5.1-foss-2021a</code></p> <p>You will see various processes running the terminal. Once these have stopped and you see a system prompt in the terminal, type: </p> <p><code>fsl</code></p> <p>This <code>fsl</code> command will initiate the FSL GUI as shown below.</p> <p> </p> <p>Now try typing <code>ls</code> in the same terminal window and pressing return. </p> <p>Notice how nothing appears to happen (your keystrokes are shown as being typed in but no actual event seems to be actioned). Indeed, nothing you type is being processed and the commands are being ignored. That is because the <code>fsl</code> command is running in the foreground in the terminal window. Because of that it is blocking other commands from being run in the same terminal.</p> <ul> <li>Now close FSL by clicking on the 'Exit' button in the FSL GUI.  </li> </ul> <p>Notice now that control has been returned to the Terminal and how commands you type are now being acted on. Try typing <code>ls</code> again; it should now work in the Terminal.</p> <ul> <li> <p>Go back to the terminal window again, but this time type <code>fsl &amp;</code> at the system prompt and press return. Again, you should see the FSL GUI pop up. </p> </li> <li> <p>Now try typing <code>ls</code> in the same Terminal. </p> </li> </ul> <p>Notice that your new commands are now being processed. The <code>fsl</code> command is now running in the background in the Terminal allowing you to run other commands in parallel from the same Terminal.  Typing the <code>&amp;</code> after any command makes it run in the background and keeps the Terminal free for you to use. </p> <p>Sometimes you may forget to type <code>&amp;</code> after a command. </p> <ul> <li>Close all open windows, open a new terminal and type <code>fsl</code> (without the &amp;) so that it is running in the foreground. </li> <li>Now hold down the CTRL key and the z key together 'CTRL-z'. </li> </ul> <p>You should get a message like <code>\u201c[1]+ Stopped fsl\u201d</code>. You will notice that the FSL GUI is now unresponsive (try clicking on some of the buttons). The <code>fsl</code> process has been suspended. </p> <ul> <li>To make it run again in the background type <code>bg</code> in the terminal window (followed by pressing the return key). </li> </ul> <p>You should find the FSL GUI is now responsive again and input to the terminal now works once more. If you clicked the 'Exit' button when the FSL GUI was unresponsive, FSL might close now.</p> <p>Running and killing commands in the terminal</p> <p>If, for some reason, you want to make the command run in the foreground then rather than typing <code>bg</code> (for background) instead type <code>fg</code> (for foreground).  If you want to kill (rather than suspend) a command that was running in the foreground, press CTRL-c (CTRL key and c key).</p> <p>Linux: some final useful tips</p> <p>TIP 1:</p> <p>When typing a command - or the name of a directory or file - you never need to type everything out. The terminal will self-complete the command or file name if you type the TAB key as you go along. Try using TAB key when typing commands or complete path to specific directory.</p> <p>TIP 2:</p> <p>If you need help understanding what the options are, or how to use a command, try adding <code>--help</code> to the end of your command. For example, for better understanding of the <code>du</code> options, type: </p> <p><code>du --help [enter]</code></p> <p>TIP 3:</p> <p>There are many useful online lists of these various commands, for example: www.ss64.com/bash</p> <p>Exercise: Basic Linux commands</p> <p>Please complete the following exercises, you should hopefully know which Linux commands to use!</p> <ul> <li>clean up your Terminal </li> <li><code>cd</code> back to your home directory </li> <li>make sure you are in your home directory</li> <li>make a new directory called <code>test</code></li> <li>rename this directory to <code>test1</code> and make another directory called <code>test2</code></li> <li>move or copy directory <code>test1</code> to your folder on modules\u2019s RDS project (i.e., <code>rds/projects/c/chechlmy-chbh-mricn/xxx</code>)</li> <li>delete the <code>test1</code> and <code>test2</code> directories and confirm it</li> </ul> <p>If unsure, check your results with someone else or ask for help!</p> <p> The correct commands are provided below. (click to reveal)</p> Linux Commands Exercise (Answers) <ol> <li> <p><code>clear</code></p> </li> <li> <p><code>cd ~</code> or <code>cd /rds/homes/x/xxx</code></p> </li> <li> <p><code>pwd</code></p> </li> <li> <p><code>mkdir test</code></p> </li> <li> <p><code>mv test test1</code> <code>mkdir test2</code></p> </li> <li> <p><code>cp -r test1 /rds/projects/c/chechlmy-chbh-mricn/xxx/</code> or <code>mv test1 /rds/projects/c/chechlmy-chbh-mricn/xxx/</code></p> </li> <li> <p><code>rm -r test1 test2</code> <code>ls</code></p> </li> </ol> <p>Workshop 1: Further Reading and Reference Material</p> <p>Here are some additional resources that introduce users to Linux:</p> <ul> <li>A useful textbook on essential Linux commands is the Linux Pocket Guide by Daniel J. Barratt.</li> <li>The Carpentries (an organisation providing free training for various software engineering and data science skills) have an introduction to UNIX course.</li> <li>Iowa State University also have a course introducing users to UNIX.</li> </ul> <p>The copy of this workshop notes can be found on Canvas 39058 - LM Magnetic Resonance Imaging in Cognitive Neuroscience in Week 01 workshop materials.</p>"},{"location":"workshop1/workshop1-intro/","title":"Workshop 1 - Introduction to BlueBEAR and Linux","text":"<p>Welcome to the first workshop of the MRICN course!</p> <p>Overview of Workshop 1</p> <p>Topics for this workshop include:</p> <ul> <li>Introduction to BlueBEAR portal</li> <li>Using the BlueBEAR Graphical User Interface (GUI) environment</li> <li>Files and Directories in BEAR Portal</li> <li>Introduction to Linux</li> <li>Using the Linux Terminal</li> <li>Basic Linux Commands</li> </ul> <p>Pre-requisites for the workshop</p> <p>Please ensure that you have completed the 'Setting Up' section of this course, as you will require access to the BEAR Portal for this workshop.</p> <p>The copy of this workshop notes can be found on Canvas 39058 - LM Magnetic Resonance Imaging in Cognitive Neuroscience in Week 01 workshop materials.</p>"},{"location":"workshop2/mri-data-formats/","title":"Working with MRI Data - Files and Formats","text":"MRI Image Fundamentals <p>When you acquire an MRI image of the brain, in most cases it is either a 3D image i.e., a volume acquired at one single timepoint (e.g., T1-weighted, FLAIR scans) or a 4D multi-volume image acquired as a timeseries (e.g., fMRI scans). Each 3D volume consists of multiple 2D slices, which are individual images.</p> <p>The volume consists of 3D voxels, with a typical size between 0.25 to 4mm, but not necessarily same in all three directions. For example, you can have voxel size [1mm x 1mm x 1mm] or [0.5mm x 0.5mm x 2mm]. The voxel size represents image resolution.</p> <p>The final important feature of an MRI image is field of view (FOV), a matrix of voxels represented as the voxel size multiplied by number of voxels. It provides information about the coverage of the brain in your MRI image. The FOV is sometime provided for the entire 3D volume or the individual 2D slice. Sometimes, the FOV is defined based on slice thickness and number of acquired slices.  </p> Image and standard space <p>When you acquire MRI images of the brain, you will find that these images will be different in terms of head position, image resolution and FOV, depending on the sequence and data type (e.g., T1 anatomical, diffusion MRI, fMRI). We often use term \u201cimage space\u201d to depict these differences i.e., structural (T1), diffusion or functional space. </p> <p>In addition, we also use term \"standard space\" to represent standard dimensions and coordinates of the template brain, which are used when reporting results of group analyses. Our brains differ in terms of size and shape and thus for the purpose of our analyses (both single-subject and group-level) we need to use standard space. The most common brain template is the MNI152 brain (an average of 152 healthy brains). </p> <p>The process of alignment between different image spaces is called registration or normalization, and its purpose is to make sure that voxel and anatomical locations correspond to the same parts of the brain for each image type and/or participant.</p>"},{"location":"workshop2/mri-data-formats/#mri-data-formats","title":"MRI Data Formats","text":"<p>MRI scanners collect MRI data in an internal format that is unique to the scanner manufacturer, e.g., Philips, Siemens or GE. The manufacturer then allows you to export the data into a more usable intermediate format. We often refer to this intermediate format as raw data as it is not directly usable and needs to be converted before being accessible to most neuroimaging software packages. </p> <p>The most common format used by various scanner manufacturers is the DICOM format. DICOM images corresponding to a single scan (e.g., a T1-weighted scan) might be one large file or multiple files (1 per each volume or one per each slice acquired). This will depend on the scanner and data server used to retrieve/export data from the scanner. There are other data formats e.g., PAR/REC that are specific to Philips scanners. The raw data needs to be converted into a format that the analysis packages can use. </p> <p>Retrieving MRI data at the CHBH</p> <p>At CHBH we have a Siemens 3T PRISMA scanner. When you acquire MRI scans at CHBH, data is pushed directly to a data server in the DICOM format. This should be automatic for all research scans. In addition, for most scans, this data is also directly converted to NIfTI format. So, at the CHBH you will likely retrieve MRI data from the scanner in NIfTI format.</p> <p> </p> <p>NIfTI (Neuroimaging Informatics Technology Initiative) is the most widely used format for MRI data, accessible by majority of the neuroimaging software packages e.g., FSL or SPM. Another older data format which is still sometimes used, is Analyze (with each image consisting of two files <code>.img</code> and <code>.hdr</code>).</p> <p>NIfTI format files have either the extension <code>.nii</code> or <code>.nii.gz</code> (compressed <code>.nii</code> file), where there is only one NIfTI image file per scan. DICOM files usually have a suffix of <code>.dcm</code>, although these files might be additionally compressed with <code>gzip</code> as <code>.dcm.gz</code> files.</p>"},{"location":"workshop2/mri-data-formats/#working-with-mri-data","title":"Working with MRI Data","text":"<p>We will now ourselves convert some DICOM images to NIfTI, using some data collected at the CHBH.</p> <p>Servers do not always provide MRI data as NIfTIs</p> <p>While at CHBH you can download the MRI data in NIfTI format, this might not be the case at some other neuroimaging centres. Thus, you should learn how to do it yourself. </p> <p>The data is located in <code>/rds/projects/c/chechlmy-chbh-mricn/module_data/CHBH</code>.</p> <p>First, log in into the BlueBEAR Portal and start a BlueBEAR GUI session (2 hours). Open a new terminal window and navigate to your MRICN project folder:</p> <p><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx</code>  [where XXX=your ADF username] </p> <p>Next copy the data from CHBH scanning sessions:</p> <pre><code>cp -r /rds/projects/c/chechlmy-chbh-mricn/module_data/CHBH .\npwd\n</code></pre> <p>After typing <code>pwd</code>, the terminal should show <code>/rds/projects/c/chechlmy-chbh-mricn/xxx</code> (i.e., you should be inside your MRICN project folder).</p> <p>Then type:</p> <pre><code>cd CHBH \nls\n</code></pre> <p>You should see data from 3 scanning sessions. Note that there are two files per scan session. One is labelled <code>XXX_dicom.zip</code>. This contains the DICOM files of all data from the scan session. The other file is labelled <code>XXX_nifti.zip</code>. This contains the NIFTI files of the same data, converted from DICOM. </p> <p>In general, both DICOM and NifTI data should be always copied from the server and saved by the researcher after each scan session. The DICOM file is needed in case there are problems with the automatic conversion to NIfTI. However, most of the time the only file you will need to work with is the <code>XXX_nifti.zip</code> file containing NIfTI versions of the data. </p> <p>We will now unpack some of the data to explore the data structure. In your terminal, type: </p> <pre><code>unzip 20191008#C4E7_nifti.zip\ncd 20191008#C4E7_nifti\nls\n</code></pre> <p>You should see six files listed as below, corresponding to 3 scans (two fMRI scans and one structural scan):</p> <pre><code>2.5mm_2000_fMRI_v1_6.json \n2.5mm_2000_fMRI_v1_6.nii.gz \n2.5mm_2000_fMRI_v1_7.json \n2.5mm_2000_fMRI_v1_7.nii.gz \nT1_vol_v1_5.json \nT1_vol_v1_5.nii.gz \n</code></pre> <p>JSON files</p> <p>You may have noticed that for each scan file (NifTI file, <code>.nii.gz</code>), there is also an autogenerated <code>.json file</code>. This is an information file (in an open standard format) that contains important information for our data analysis. For example, the <code>2.5mm_2000_fMRI_v1_6.json</code> file contains slice timing information about the exact point in time during the 2s TR (repetition time) when each slice is acquired, which can be used later in the fMRI pre-processing. We will come back to this later in the course.</p> <p>For now, let's look at another dataset. In your terminal type:</p> <pre><code>cd ..\nunzip 20221206#C547_nifti.zip\ncd 20221206#C547_nifti\nls\n</code></pre> <p>You should now see a list of 10 files, corresponding to 3 scans (two diffusion MRI scans and one structural scan). For each diffusion scan, in addition to the <code>.nii.gz</code> and <code>.json</code> files, there are two additional files, <code>.bval</code> and <code>.bvec</code> that contain important information about gradient strength and gradient directions (as mentioned in the MRI physics lecture). These two files are also needed for later analysis (of diffusion MRI data).</p> <p>We will now look at a method for converting data from the DICOM format to NIfTI.  </p> <pre><code>cd ..\nunzip 20191008#C4E7_dicom.zip\ncd 20191008#C4E7_dicom\nls\n</code></pre> <p>You should see a list of 7 sub-directories. Each top level DICOM directory contains sub-directories with each individual scan sequence. The structure of DICOM directories can vary depending on how it is stored/exported on different systems. The 7 sub-directories here contain data for four localizer scans/planning scans, two fMRI scans and one structural scan. Each sub-directory contains several <code>.dcm</code> files.</p> <p>There are several software packages which can be used to convert DICOM to NIfTI, but <code>dcm2niix</code> is the most widely used. It is available as standalone software, or part of MRIcroGL a popular tool for brain visualization similar to FSLeyes. <code>dcm2niix</code> is available on BlueBEAR, but to use it you need to load it first using the terminal.</p> <p>To do this, in the terminal type:</p> <p><code>module load bear-apps/2022b</code></p> <p>Wait for the apps to load and then type:</p> <p><code>module load dcm2niix/1.0.20230411-GCCcore-12.2.0</code></p> <p>To convert the <code>.dcm</code> files in one of the sub-directories to NIfTI using <code>dcm2niix</code> from terminal, type:</p> <p><code>dcm2niix T1_vol_v1_5</code></p> <p>If you now check the <code>T1_vol_v1_5</code> sub-directory, you should find there a single <code>.nii</code> file and a <code>.json</code> file.</p> <p>Converting more MRI data</p> <p>Now try to convert to NIfTI the <code>.dcm</code> files from the scanning session <code>20221206#C547</code> with 3 DICOM sub-directories, the two diffusion scans <code>diff_AP</code> and <code>diff_PA</code> and one structural scan MPRAGE. </p> <p>To do this, you will first need to change current directory, unzip, change directory again and then run the <code>dcm2niix</code> command as above.</p> <p>If you have done it correctly you will find <code>.nii</code> and <code>.json</code> files generated in the structural sub-directories, and in the diffusion sub-directories you will also find <code>.bval</code> and <code>.bvec</code> files.</p> <p>Now that we have our MRI data in the correct format, we will take a look at the brain images themselves using FSLeyes.</p>"},{"location":"workshop2/visualizing-mri-data/","title":"MRI data visualization with FSLeyes","text":"<p>FSL (FMRIB Software Library) is a comprehensive neuroimaging software library for the analysis of structural and functional MRI data. FSL is widely used, freely available, runs on both Linux and Mac OS as well as on Windows via a Virtual Machine. </p> <p>FSLeyes is the FSL viewer for 3D and 4D data. FSLeyes is available on BlueBEAR, but you need to load it first. You can just load FSLeyes as a standalone software, but as it is often used with other FSL tools, you often want to load both (FSL and FSLeyes). </p> <p>In this session we will only be loading FSLeyes by itself, and not with FSL.</p> <p>FSL Wiki</p> <p>Remember that the FSL Wiki is an important source for all things FSL!</p>"},{"location":"workshop2/visualizing-mri-data/#getting-started-with-fsleyes","title":"Getting started with FSLeyes","text":"<p>Assuming that you have started directly from the previous page, first close your previous terminal (to close <code>dcm2nii</code>). Then open a new terminal and to navigate to the correct folder, type in your terminal:</p> <p><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/CHBH</code></p> <p>To open FSLeyes, type:</p> <p><code>module load FSL/6.0.5.1-foss-2021a-fslpython</code></p> <p>There are different version of FSL on BlueBEAR, however this is the one which you need to use it together with FSLeyes.</p> <p>Wait for FSL to load and then type:</p> <p><code>module load FSLeyes/1.3.3-foss-2021a</code></p> <p>Again, wait for FSLeyes to load (it may take a few minutes). After this, to open FSLeyes, type in your terminal:</p> <p><code>fsleyes &amp;</code></p> <p>The importance of '&amp;'</p> <p>Why do we type <code>fsleyes &amp;</code> instead of <code>fsleyes</code>?</p> <p> You should then see the setup below, which is the default FSLeyes viewer without an image loaded.</p> <p> </p> <p>You can now load/open an image to view. Click 'File' \u2192 'Add from file' (and then select the file in your directory e.g., <code>rds/projects/c/chechlmy-chbh-mricn/xxx/CHBH/visualization/T1.nii</code>).</p> <p>You can also type directly in the terminal <code>fsleyes file.nii.gz</code> where you replace <code>file.nii.gz</code> with the name of the actual file you want to open.  However, you will need to include the full path to the file if you are not in the same directory when you open the terminal window e.g. <code>fsleyes rds/projects/c/chechlmy-chbh-mricn/xxx/CHBH/visualization/T1.nii</code></p> <p>You should now see a T1 scan loaded in ortho view with three canvases corresponding to the sagittal, coronal, and axial planes.</p> <p> </p> <p>Please now explore the various settings in the ortho view panel:</p> <p> </p> <p>Also notice the abbreviations on the three canvases:</p> <ul> <li>L/R: Left/Right</li> <li>S/I: Superior/Inferior</li> <li>A/P: Anterior/Posterior</li> </ul> <p>FSL comes with a collection of\u00a0NIFTI standard templates, which are used for image registration and normalisation (part of MRI data analysis). You can also load these templates in FSLeyes.</p> <p>To load a template, Click 'File' \u2192 'Add Standard' (for example select the file named <code>MNI152_T1_2mm.nii.gz</code>. If you still have the <code>T1.nii</code> image open, first close this image (by selecting 'Overlay' \u2192 'Remove') and then load the template.</p> <p>The image below depicts the various tools that you can use on FSLeyes, give them a go!</p> <p> </p> <p> We will now look at fMRI data. First close the previous image ('Overlay' \u2192 'Remove') and then load the fMRI image. To do this, click 'File' \u2192 'Add from file'  and then select the file <code>rds/projects/c/chechlmy-chbhmricn/xxx/CHBH/visualization2.5mm_2000_fMRI.nii.gz</code>.</p> <p>Your window should now look like this:</p> <p> </p> <p>Remember this fMRI data file is a 4D image \u2013 a set of 90-odd volumes representing a timeseries. To cycle through volumes, use the up/down buttons or type in a volume in the 'Volume' box to step through several volumes.</p> <p> </p> <p>Now try playing the 4D file in 'Movie' mode by clicking this button. You should see some slight head movement over time. Click the button again to stop the movie.</p> <p> </p> <p>As the fMRI data is 4D, this means that every voxel in the 3D-brain has a timecourse associated with it. Let's now have a look at this. </p> <p>Keeping the same dataset open (<code>2.5mm_2000_fMRI.nii.gz</code>) and now in the FSLeyes menu, select 'View' \u2192 'Time series'. </p> <p>FSLeyes should now look like the picture below. </p> <p> </p> <p>What exactly are we looking at?</p> <p>The functional image displayed here is the data straight from the scanner, i.e., raw, un-preprocessed data that has not been analyzed. In later workshops we will learn how to view analyzed data e.g., display statistical maps etc. </p> <p>You should see a timeseries shown at the bottom of the screen corresponding to the voxel that is selected in the main viewer. Move the mouse to select other voxels to investigate how variable the timecourse is. </p> <p>Within the timeseries window, hit the '+' button to show the 'Plot List' characteristics for this timeseries.</p> <p> </p> <p>Compare the timeseries in different parts of the brain, just outside the brain (skull and scalp), and in the airspace outside the skull. You should observe that these have very different mean intensities.</p> <p>The timeseries of multiple different voxels can be compared using the '+' button. Hit '+' and then select a new voxel. Characteristics of the timeseries such as plotting colour can also be changed using the buttons on the lower left of the interface.</p>"},{"location":"workshop2/visualizing-mri-data/#atlas-tools","title":"Atlas tools","text":"<p>FSL comes not only with a collection of\u00a0NIFTI standard templates but also with several built-in atlases, both probabilistic and histological (anatomical), comprising cortical, sub-cortical, and white matter parcellations.  You can explore the full list of included atlases here.</p> <p>We will now have a look at some of these atlases. </p> <p>Firstly, close all open files in FSLeyes (or close FSLeyes altogether and start it up again in your terminal by running <code>fsleyes &amp;</code>).</p> <p>In the FSLeyes menu, select 'File' \u2192 'Add Standard' and then choose the file called <code>MNI152_T1_2mm.nii.gz</code>  (this is a template brain in MNI space).</p> <p>The MNI152 atlas</p> <p>Remember that the MNI152 atlas is a standard brain template created by averaging 152 MRI scans of healthy adults widely used as a reference space in neuroimaging research.</p> <p>Now select from the menu 'Settings' \u2192 'Ortho View 1' and tick the box for 'Atlases' at the bottom.</p> <p> </p> <p>You should now see the 'Atlases' panel open as shown below.  </p> <p> </p> <p>The 'Atlases' panel is organized into three sections: </p> <ul> <li>Atlas information</li> <li>Atlas search</li> <li>Atlas management</li> </ul> <p>The 'Atlas information' tab provides information about the current display location, relative to one or more atlases selected in this tab. We will soon see how to use this information.</p> <p> </p> <p>The 'Atlas search' tab can be used to search for specific regions by browsing through the atlases. We will later look how to use this tab to create region-of-interest (ROI) masks. </p> <p> </p> <p>The 'Atlas management' tab can be used to add or delete atlases. This is an advanced feature, and we will not be using it during our workshops.</p> <p>We will now have a look at how to work with FSL atlases. First we need to choose some atlases to reference. In the 'Atlases' \u2192 'Atlas Information' window (bottom of screen in middle panel) make sure the following are ticked: </p> <ul> <li>Harvard-Oxford Cortical Structural Atlas</li> <li>Harvard-Oxford Subcortical Structural Atlas</li> <li>Juelich Histological Atlas</li> <li>Talairach Daemon Labels</li> </ul> <p>Now let's select a point in the standard brain. Move the cursor to the voxel position: [x=56, y=61, z=27] or enter the voxel location in the 'Location' window (2nd column). </p> <p>MNI Co-ordinate Equivalent</p> <p>Note that the equivalent MNI coordinates (shown in the 1st column/Location window) are [-22,-4,-18].</p> <p>It may not be immediately obvious what part of the brain you are looking at. Look at the 'Atlases' window. The report should say something like:</p> <pre><code>Harvard-Oxford Cortical Structural Atlas \nHarvard-Oxford Subcortical Structural Atlas \n98% Left Amygdala\n</code></pre> <p>Checking the brain region with other atlases</p> <p>What do the Juelich Histological Atlas &amp; Talairach Daemon Labels report?</p> <p>The Harvard-Oxford and Juelich are both probabilistic atlases. They report the percentage likelihood that the area named matches the point where the cursor is. </p> <p>The Talairach Atlas is a simpler labelling atlas. It is based on a single brain (of a 60-year-old French woman) and is an example of a deterministic atlas. it reports the name of the nearest label to the cursor coordinates. </p> <p>From the previous reports, particularly the Harvard-Oxford Subcortical Atlas and the Juelich Atlas, it should be obvious that we are most likely in the left amygdala. </p> <p>Now click the '(Show/Hide)' link after the Left Amygdala result (as shown below):</p> <p> </p> <p>This shows the (max) volume that the probabilistic Harvard-Oxford Subcortical Atlas has encoded for the Left Amygdala. The cursor is right in the middle of this volume.</p> <p> </p> <p>In the 'Overlay list' click and select the top amygdala overlay. You will note that the min/max ranges are set to 0 and 100. If it\u2019s not, change it to 0 and 100. These reflect the % likelihood of the labelling being correct.</p> <p> </p> <p>If you increase the min value from 0% to 50%, then you will see the size of the probability volume for the left amygdala will decrease. </p> <p>It now shows only the volume where there is a 50% or greater probability that this label is correct.</p> <p> </p> <p>Click the (Show/Hide) link after the Left Amygdala; the amygdala overlay will disappear.</p> <p>Exercise: Coordinate Localization</p> <p>Have a go at localizing exactly what the appropriate label is for these coordinates: </p> <ul> <li>Voxel coordinates [40, 51, 40] or MNI [10, -24, 8]</li> <li>Voxel coordinates [40, 51, 40] or MNI [10, -24, 8]</li> <li>Voxel coordinates [65, 29, 20] or MNI [-41, -68, -32]</li> </ul> <p>If unsure check your results with someone else, or ask for help! </p> <p>Make sure all overlays are closed (but keep the <code>MNI152_T1_2mm.nii.gz</code> open) before moving to the next section.</p>"},{"location":"workshop2/visualizing-mri-data/#using-atlas-tools-to-find-a-brain-structure","title":"Using atlas tools to find a brain structure","text":"<p>It is often helpful to locate where a specific structure is in the brain and to visually assess its size and extent.</p> <p>Let's suppose we want to visualize where Heschl's Gyrus is. In the bottom 'Atlases' window, click on the second tab ('Atlas search').</p> <p>In the Search box, start typing the word 'Heschl\u2026'. You should find that the system quickly locates an entry for Heschl's Gyrus in the Harvard-Oxford Cortical Atlas. Click on it to select.</p> <p>Now if you now the tick box immediately below next to the Heschl's Gyrus, an overlay will be added to the 'Overlay' list on the bottom (see below). Heschl's Gyrus should now be visible in the main image viewer. </p> <p>Now click on the '+' button next to the tick box. This will centre the viewing coordinates to be in the middle of the atlas volume (see below).</p> <p> </p> <p>Exercise: Atlas visualization</p> <p>Now try this for yourself: </p> <ul> <li>Remove the Heschl's Gyrus visualization. You can tick it off in the 'Atlases' window, or select Heschl's Gyrus in the 'Overlay list' window, and then either toggle its visibility off (click the eye icon) or remove it ('Menu' \u2192 'Overlay' \u2192 'Remove').</li> <li>Visualize the Lingual Gyrus and Left Hippocampus. To avoid confusion, change the colour of the Lingual Gyrus visualization from red/yellow to green and Left Hippocampus to blue.</li> </ul> <p>You can change the colour of the overlays by selecting the option below:</p> <p> </p> <p>Other options also exist to help you navigate the brain and recognize the different brain structures and their relative positions. </p> <p>Make sure you have firstly closed/removed all previous overlays. Now, select the 'Atlas Search' tab in the 'Atlases' window again.  This time, in the left panel listing different atlases, tick on the option for only one of the atlases, such as the Harvard-Oxford Cortical Structural Atlas, and make sure all others are unticked. </p> <p> </p> <p></p> <p>Now you should see all of the areas covered by the Harvard-Oxford cortical atlas shown on the standard brain. You can click around with the cursor, the labels for the different areas can be seen in the bottom right panel.</p> <p> </p> <p></p> <p>In addition to atlases covering various grey matter structures, there are also two white matter atlases: the JHU ICBM-DTI-81 white-matter labels atlas &amp; JHU white-matter tractography atlas.  If you tick (select) these atlases as per previous instructions (hint using the 'Atlas search' tab), you will see a list of all included white matter tracts (pathways) as shown below:</p> <p> </p>"},{"location":"workshop2/visualizing-mri-data/#using-atlas-tools-to-create-a-region-of-interest-mask","title":"Using atlas tools to create a region-of-interest mask","text":"<p>You can also use atlas tools in FSLeyes to not only locate specific brain structures but also to create masks for ROI (region-of-interest) analysis. We will now create ROI masks (one grey matter mask and one white matter) using FSL tools and built-in atlases.</p> <p>To start, please close 'FSLeyes' entirely, either by clicking 'x' in the right corner of the FSLeyes window or by selecting 'FSLeyes' \u2192 'Close'. Then close your current terminal and open a new terminal window. </p> <p>Then do the following:</p> <ul> <li>Navigate to your project directory and make a new directory called <code>ROImasks</code>. Navigate into this directory. </li> <li>Then load <code>fsl</code> and open FSLeyes in the background.</li> </ul> <p>Here are the commands to do this:</p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/\nmkdir ROImasks\ncd ROImasks\nmodule load FSL/6.0.5.1-foss-2021a-fslpython \nmodule load FSLeyes/1.3.3-foss-2021a\nfsleyes &amp; \n</code></pre> <p>Wait for FSLeyes to load, then:</p> <ul> <li>Load the MNI template by clicking 'File' \u2192 'Add Standard' \u2192 'MNI152_T1_2mm' and open the 'Atlases' panel. </li> <li>Go to the 'Atlas search' tab, select the 'Harvard Oxford Cortical Atlas' and tick the 'Middle Frontal Gyrus' (from the list below 'Search' box) to add overlay to the 'Overlay list'. </li> <li>Select the 'Middle Frontal Gyrus' (<code>harvardoxford-cortical_prob_Middle_Frontal_Gyrus</code>) from the 'Overlay' list and save it in your <code>ROImasks</code> directory as <code>MFG</code> (select 'Overlay' \u2192 'Save' \u2192 Name: MFG).</li> </ul> <p>You should now see the MFG overlay in the overlay list (as below) and have a <code>MFG.nii.gz</code> file in the <code>ROImasks</code> directory. You can check this by typing <code>ls</code> in the terminal.</p> <p> </p> <p>We will now create a white matter mask. Here are the steps:</p> <ul> <li>First, remove the MFG overlay ('Menu' \u2192 'Overlay' \u2192 'Remove').</li> <li>Then go to the 'Atlas search' tab, and select the 'JHU white-matter tractography' atlas and tick the 'Forceps minor' (from the list below 'Search' box) to add overlay to the 'Overlay' list. </li> <li>Finally, select the added 'Forceps minor' overlay from the 'Overlay' list and save it in your <code>ROImasks</code> directory as FM ('Overlay' \u2192 'Save' \u2192 Name: FM). </li> </ul> <p>You should now see the FM overlay in the overlay list (as below) and also have a <code>FM.nii.gz</code> file in the <code>ROImasks</code> directory. </p> <p> </p> <p>You now have two \u201cprobabilistic ROI masks\u201d. To use these masks for various analyses, you need to first binarize these images.</p> <p>Why binarize?</p> <p>Why do you think we need to binarize the mask first? There are several reasons, but primarily it creates clear boundaries between regions which simplifies our statistical analysis and reduces computation.</p> <p>To do this, first close FSLeyes. Make sure that you are in the <code>ROImasks</code> directory and check if you have the two masks.  If you type <code>pwd</code> in the terminal, you should get the output <code>rds/projects/c/chechlmy-chbh-mricn/xxx/ROImasks</code> (where XXX=your ADF username) and when you type <code>ls</code>, you should see <code>FM.nii.gz</code> and <code>MFG.nii.gz</code>.</p> <p>To binarize the masks, you can use one of the FSL tools for image manipulation, <code>fslmaths</code>. The basic structure of an <code>fslmaths</code> command is: </p> <p><code>fslmaths input image [modifiers/options] output</code></p> <p>Type in your terminal:</p> <pre><code>fslmaths FM.nii.gz -bin FM_binary\nfslmaths MFG.nii.gz -bin MFG_binary\n</code></pre> <p>This simply takes your ROI mask, binarizes it and saves the binarized mask with the <code>_binary</code> name.</p> <p>You should now have 4 files in the ROImasks directory.</p> <p>Now open FSLeyes and examine one of the binary masks you just created. First load a template (Click 'File' \u2192 'Add Standard' \u2192 'MNI152_T1_2mm') and add the binary mask (e.g., Click 'File' \u2192 'Add from file' \u2192 'FM_binary.nii.gz'). </p> <p>You can see the difference between the probabilistic and binarized ROI masks below:</p> <p>Probabilistic ROI mask</p> <p>Binary ROI mask</p> <p>To use ROI masks in your analysis, you might also need to threshold it i.e., to change/restrict the probability of the volume. We previously did this for the amygdala manually (e.g., from 0-100% to 50%-100%).  The choice of the threshold might depend on the type of analysis and the type of ROI mask you need to use. The instructions below explain how to threshold and binarize your ROI image in one single step using <code>fslmaths</code>.</p> <p>Open your terminal and make sure that you are in the <code>ROImasks</code> directory (<code>pwd</code>). To both threshold and binarize the MFG mask, type:</p> <p><code>fslmaths MFG.nii.gz -thr 25 -bin MFGthr_binary</code></p> <p>(option <code>-thr</code> is used to threshold the image below a specific number, in this case 25 corresponding to 25% probability)</p> <p>Now let's compare the thresholded and unthresholded MFG binarized masks.</p> <ul> <li>Go back to FSLeyes and add the unthresholded MFG binary mask (e.g., Click 'File' \u2192 'Add from file' \u2192 'MFG_binary.nii.gz'). </li> <li>Add the second, thresholded and binarized MFG mask (<code>MFGthr_binary.nii.gz</code>), and to avoid confusion, change the colour of the second mask to blue. You can either toggle its visibility on and off (click the eye icon) to compare mask or use the 'Opacity' button. </li> </ul> <p>You can see the difference in size between the two below:</p> <p>Binarized MFG mask</p> <p>Binarized and thresholded MFG mask</p> <p>Exercise: Atlases and masks</p> <p>Have a go at the following exercises:</p> <ul> <li>Explore different atlases to localize various cortical, subcortical or white matter structures (take inspiration from your MSc project, recent papers or seminars)</li> <li>Using atlas tools, create binary (un-thresholded) masks for the left and right Superior longitudinal fasciculus (hint: use JHU white-matter tractography atlas)</li> <li>Using atlas tools, create binary and thresholded (at different levels of probability 5, 25 and 75%) masks for the right thalamus (hint: use Harvard-Oxford Subcortical Structural Atlas)</li> </ul> <p>If unsure, check your results with someone else or ask for help!</p> <p>Workshop 2: Further Reading and Reference Material</p> <p>FSLeyes is not the only MRI visualization tool available. Here are some others:</p> <ul> <li>fslview (older version of FSL viewer) </li> <li>MRIcroGL (for high quality images)</li> <li>AFNI - Analysis of Functional NeuroImages</li> <li>Mango/Papaya</li> </ul> <p>More details of what is available on BEAR at the CHBH can be found at the BEAR Technical Docs website.</p>"},{"location":"workshop2/workshop2-intro/","title":"Workshop 2 - MRI data formats, data visualization and atlas tools","text":"<p>Welcome to the second workshop of the MRICN course! Prior lectures introduced you to the basics of the physics and technology behind MRI data acquisition.  In this workshop we will explore, MRI image fundamentals, MRI data formats, data visualization and atlas tools. </p> <p>Overview of Workshop 2</p> <p>Topics for this workshop include:</p> <ul> <li>The fundamentals of MRI data, including file types and formats</li> <li>Converting between different MRI data files (e.g., DICOM to NIFTI)</li> <li>Introduction to FSLeyes and basic navigation</li> <li>Loading atlases and creating regions-of-interest (ROIs)</li> <li>Binarizing and thresholding ROIs</li> </ul> <p>You will need this information before you can analyse data, regardless if using structural or functional MRI data.</p> <p>For the purpose of the module we will be using BlueBEAR. You should remember from Workshop 1, how to access the BlueBEAR Portal and use the BlueBEAR GUI. </p> <p> </p> <p>You have already been given access to the RDS project, <code>rds/projects/c/chechlmy-chbh-mricn</code>. Inside the module\u2019s RDS project, you will find that you have a folder labelled <code>xxx</code> (<code>xxx</code> = University of Birmingham ADF username). </p> <p>If you navigate to that folder <code>(rds/projects/c/chechlmy-chbh-mricn/xxx)</code>, you will be able to perform the various file operations from there during workshops.</p> <p>The copy of this workshop notes can be found on Canvas 39058 - LM Magnetic Resonance Imaging in Cognitive Neuroscience in Week 02 workshop materials.</p>"},{"location":"workshop3/diffusion-intro/","title":"Diffusion MRI basics - visualization and preprocessing","text":"<p>In this workshop and the workshop next week, we will follow some basic steps in the diffusion MRI analysis pipeline below.  The instructions here are specific to tools available in FSL, however other neuroimaging software packages can be used to perform similar analyses.  You might also recall from lectures that models other than diffusion tensor and methods other than probabilistic tractography are also often used. </p> FSL diffusion MRI analysis pipeline <p> </p> <p></p> <p>First, if you have not already, log in into the BlueBEAR Portal and start a BlueBEAR GUI session (2 hours). You should know how to do it from the previous workshops.  Open a new terminal window and navigate to your MRICN project folder:</p> <p><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx</code> [where XXX=your ADF username] </p> <p>Please check your directory by typing <code>pwd</code>. This should return: <code>/rds/projects/c/chechlmy-chbh-mricn/xxx</code>.</p> <p>Where has all my data gone?</p> <p>Before this workshop, any old directories and files from previous workshops have been removed (you will not need it for subsequent workshops and storing unnecessary data would result in exceeding allocated quota).  Your XXX directory should therefore be empty. </p> <p>Next you need to copy over the data for this workshop.</p> <p><code>cp -r /rds/projects/c/chechlmy-chbh-mricn/module_data/diffusionMRI/ .</code>    (make sure you do not omit spaces and .)</p> <p>This might take a while, but once it has completed, change into that downloaded directory:</p> <p><code>cd diffusionMRI</code> (your <code>XXX</code> subdirectory you should now have the folder <code>diffusionMRI</code>)</p> <p>Type <code>ls</code>. You should now see three subdirectories/folders (<code>DTIfit</code>, <code>TBSS</code> and <code>tractography</code>). Change into the <code>DTIfit</code> folder:</p> <p><code>cd DTIfit</code></p>"},{"location":"workshop3/diffusion-intro/#viewing-diffusion-data-using-fsleyes","title":"Viewing diffusion data using FSLeyes","text":"<p>We will first look at what diffusion images look like and explore text files which contain information about gradient strength and gradient directions.</p> <p>In your terminal type <code>ls</code>. This should return:</p> <pre><code>p01/\np02/\n</code></pre> <p>So, the folder <code>DTIfit</code> contains data from two participants contained within the <code>p01</code> and <code>p02</code> folders. </p> <p>Inside each folder (<code>p01</code> and <code>p02</code>) you will find a T1 scan, uncorrected diffusion data (<code>blip_up.nii.gz</code>, <code>blip_down.nii.gz</code>) acquired with two opposing PE-directions (<code>AP/blip_up</code> and <code>PA/blip_down</code>) and corresponding <code>bvals</code> (e.g., <code>blip_up.bval</code>) and <code>bvecs</code> (e.g., <code>blip_up.bvec</code>) files. </p> <ul> <li>The <code>bvals</code> files contain b-values (scalar values for each applied gradient). </li> <li>The <code>bvecs</code> files contain a list of gradient directions (diffusion encoding directions), including a [3x1] vector for each gradient. </li> </ul> <p>The number of entries in <code>bvals</code> and <code>bvecs</code> files equals the number of volumes in the diffusion data files. </p> <p>Finally, inside <code>p01</code> and <code>p02</code> there is also subdirectory data with distortion-corrected diffusion images.</p> <p>We will start with viewing the uncorrected data. Please navigate inside the <code>p01</code> folder, open FSLeyes and then load one of the uncorrected diffusion images:</p> <pre><code>cd p01\nmodule load FSL/6.0.5.1-foss-2021a-fslpython\nmodule load FSLeyes/1.3.3-foss-2021a\nfsleyes &amp;\n</code></pre> <p>The image you have loaded is 4D and consists of 64 volumes acquired with different diffusion encoding directions. Some of the volumes are non-diffusion images (b-value = 0), while most are diffusion weighted images.  The first volume, which you can see after loading the file, is a non-diffusion weighted image as demonstrated below. </p> <p> </p> <p>Viewing separate volumes</p> <p>You can view the separate volumes by changing the number in the Volume box or playing movie mode. Note that the volume count starts from 0.  You should also note that there are significant differences in the image intensity between different volumes. </p> <p>Now go back to volume 0 and - if needed - stop movie mode. In the non-diffusion weighted image, the ventricles containing CSF are bright and the rest of the image is relatively dark.  Now change the volume number to 2, which is a diffusion weighted image (with a b-value of approximately 1500). </p> <p>The intensity of this volume is different. To see anything, please change max. intensity to 400. Now the ventricles are dark and you can see some contrast between different voxels. </p> <p> </p> <p>Let's view the content of the <code>bvals</code> and <code>bvecs</code> files by using the <code>cat</code> command. In your terminal type:</p> <p><code>cat blip_down.bval</code></p> <p> </p> <p>The first number is 0. This indicates that indeed the first volume (volume 0) is a non-diffusion weighted image and the third volume (volume 2) is diffusion weighted volume with b=1500.  Based on the content of this <code>bval</code> file, you should be able to tell how many diffusion-weighted volumes were acquired and how many without any diffusion weighting (b0 volumes). </p> <p>Comparing diffusion-weighted volumes</p> <p>Please compare this with the file you loaded into FSLeyes. </p> <p>Now type:</p> <p><code>cat blip_down.bvec</code></p> <p> </p> <p>You should now see 3 separate rows of numbers representing the diffusion encoding directions (3x1 vector for each acquired volume; x,y,z directions) and that for volume 2 the diffusion encoding is represented by the vector [0.578, 0.671, 0.464]. </p> Distortion correction <p>As explained in the lectures, diffusion imaging suffers from various distortions (susceptibility, eddy-currents and movement induced distortions). These need to be corrected before further analysis.  The most most noticeable geometric distortions are susceptibility-induced distortions caused by field inhomogeneities, and so we will have a closer look at these.</p> <p>All types of distortions need correction during pre-processing steps in diffusion imaging analysis. FSL includes two tools used for distortion correction, topup and eddy.  The processing with these two tools is time and computing intensive. Therefore we will not run the distortion correction steps in the workshop but instead explore some principles behind it. </p> <p> </p> <p>For this, you are given distortion corrected data to conduct further analysis, diffusion tensor fitting and probabilistic tractography.  </p> <p>First close the current image in FSLeyes ('Overlay' \u2192 'Remove') and load both uncorrected images (<code>blip_up.nii.gz</code>, <code>blip_down.nii.gz</code>) acquired with two opposing PE-directions (PE=phase encoding). </p> <p>Compare the first volumes in each file. To do that you can either toggle the visibility on and off (click the eye icon) or use the 'Opacity' button (you should remember from the previous workshop how to do this). </p> <p> </p> <p>The circled area indicates the differences in susceptibility-induced distortions between the two images acquired with two opposing PE-directions. </p> <p>Now change the max. intensity to 400 and compare the third volumes in each file. Again, the circled area indicate the differences in distortions between the two images acquired with the two opposing PE-directions. </p> <p> </p> <p>Finally, we will look at distortion corrected data. First close the current image ('Overlay' \u2192 'Remove').</p> <p>Now in FSLeyes load <code>data.nii.gz</code> (the distortion-corrected diffusion image located inside the data subdirectory) and have a look at one of the the non-diffusion weighted and diffusion-weighted volumes. </p> <p> </p> <p>Comparing corrected to uncorrected diffusion-weighted volumes</p> <p>Can you tell the difference in the corrected compared to the uncorrected diffusion images?</p> <p>Further examining the difference between uncorrected and corrected diffusion data</p> <p>In your own time (outside of this workshop as part of independent study), load both the corrected and uncorrected data for <code>p01</code> and compare using the 'Volume' box or 'Movie' mode. Also explore the data in <code>p02</code> folder using the instructions above. </p>"},{"location":"workshop3/diffusion-intro/#creating-a-binary-mask-using-fsls-brain-extraction-tool","title":"Creating a binary mask using FSL's Brain Extraction Tool","text":"<p>In the next part of the workshop, we will look FSL's Brain Extraction Tool (BET).</p> <p>Brain extraction is a necessary pre-processing step, which removes non-brain tissue from the image. It is applied to structural images prior to tissue segmentation and is needed to prepare anatomical scans for registration of functional MRI or diffusion scans to MNI space. BET can be also used to create binary brain masks (e.g., brain masks needed to run diffusion tensor fitting, DTIfit). </p> <p>In this workshop we will look at only at creating a binary brain mask as required for DTIfit. In subsequent workshops we will look at using BET for removing non-brain tissues from diffusion and T1 scans (\u201cskull-stripping\u201d) in preparation for registration.</p> <p>First close FSLeyes and to make sure you do not have any processes running in the background, close your current terminal.</p> <p>Open a new terminal window, navigate to the <code>p02</code> subdirectory, and load FSL and FSLeyes again:</p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/diffusionMRI/DTIfit/p02\nmodule load FSL/6.0.5.1-foss-2021a-fslpython\nmodule load FSLeyes/1.3.3-foss-2021a \n</code></pre> <p>Now check the content of the <code>p02</code> subdirectory by typing <code>ls</code>. You should get the response <code>bvals</code>, <code>bvecs</code> and <code>data.nii.gz</code>.</p> <p>From the <code>data.nii.gz</code> (distortion corrected diffusion 4D image) we will extract a single volume without diffusion weighting (e.g. the first volume). You can extract it using one of FSL's utility commands, <code>fslroi</code>. </p> <p>What is <code>fslroi</code> used for?</p> <p><code>fslroi</code>is used to extract a region of interest (ROI) or subset of data from a larger 3D or 4D image file.</p> <p>In the terminal, type:</p> <p><code>fslroi data.nii.gz nodif 0 1</code></p> <p>where: </p> <ul> <li><code>data.nii.gz</code> is your input image, </li> <li><code>nodif</code> is your output image (3D non-diffusion weighted volume), </li> <li>0 and 1 indicate that you are extracting volume 0 and the you only want to extract one (1) volume</li> </ul> <p>You should have a new file <code>nodif.nii.gz</code> (type <code>ls</code> to confirm) and can now create a binary brain mask using BET.</p> <p>To do this, first open BET in terminal. You can open the BET GUI directly in a terminal window by typing:</p> <p><code>Bet &amp;</code></p> <p>Or by runnning FSL in a terminal window and accessing BET from the FSL GUI. To do it this way, type:</p> <p><code>fsl &amp;</code></p> <p>and then open the 'BET brain extraction tool' by clicking on it in the GUI.</p> <p>In either case, once BET is opened, click on advanced options and make sure the first two outputs are selected ('brain extracted image' and 'binary brain mask') as below. Select as the 'Input' image the previously created <code>nodif.nii.gz</code> and change 'Fractional Intensity Threshold' to 0.4. Then click the 'Go' button. </p> <p> </p> <p> </p> <p>Completing BET in the terminal</p> <p>After running BET you may need to hit return to get a visible prompt back after seeing \"Finished\u201d in the terminal!</p> <p>You will see 'Finished' in the terminal when you are ready to inspect the results. Close BET and open FSLeyes and load three files (<code>nodif.nii.gz</code>, <code>nodif_brain.nii.gz</code> and <code>nodif_brain_mask</code>). Compare the files. To do that you can either toggle the visibility on and off (click the eye icon) or use 'Opacity button' (you should remember from previous workshop how to do it).</p> <p>The <code>nodif_brain_mask</code> is a single binarized image with ones inside the brain and zeroes outside the brain. You need this image both for DTIfit and tractography.</p> <p>Comparing between BET and normal images</p> <p>Can you tell the difference between <code>nodif.nii.gz</code> and <code>nodif_brain.nii.gz</code>? It might be easier to compare these images if you change max intensity to 1500 and <code>nodif_brain</code> colour to green.</p>"},{"location":"workshop3/diffusion-mri-analysis/","title":"Diffusion tensor fitting and Tract-Based Spatial Statistics","text":""},{"location":"workshop3/diffusion-mri-analysis/#diffusion-tensor-fitting-dtifit","title":"Diffusion tensor fitting (DTIfit)","text":"<p>The next thing we will do is to look at how to run and examine the results of diffusion tensor fitting. </p> <p>First close FSLeyes, and to make sure you do not have any processes running in the background, close the current terminal.</p> <p>Open a new terminal window, navigate to the <code>p01</code> subdirectory, load FSL and FSLeyes again, and finally open FSL (with &amp; to background it):</p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/diffusionMRI/DTIfit/p01\nmodule load FSL/6.0.5.1-foss-2021a-fslpython\nmodule load FSLeyes/1.3.3-foss-2021a\nfsl &amp; \n</code></pre> <p>To run the diffusion tensor fit, you need 4 files as specified below:</p> <ol> <li>Distortion corrected diffusion data: <code>data.nii.gz</code></li> <li>Binary brain mask: <code>nodif_brain_mask.nii.gz</code></li> <li>Gradient directions: <code>bvecs</code> (test file with gradient directions)</li> <li>b-values: <code>bvals</code> (text file with list of b-values)</li> </ol> <p>All these files are included inside the data subdirectory <code>p01/data</code>. You will later learn how to create a binary brain mask but first we will run DTIfit.</p> <p>In the FSL GUI, first click on 'FDT diffusion', and in the FDT window, select 'DTIFIT Reconstruct diffusion tensors'. Now choose as 'Input directory' the <code>data</code> subdirectory located inside <code>p01</code> and click 'Go'. </p> <p> </p> <p>You should see something happening in the terminal and once you see 'Done!' you are ready to view the results.</p> <p> </p> <p>Click 'OK' when the message appears.</p> Different ways of running DTIfit <p>Instead of running DTIfit by choosing the 'Input' directory, you can also run it by specifying the input file manually. If you click it now, the files would be auto-filled but otherwise you would need to provide inputs as below.       </p> <p> </p> <p></p> <p>Running DTIfit in your own time</p> <p>Please do NOT run it now, but instead try it in your own time with data in the <code>p02</code> folder.</p> <p>Finally, you can also run DTIfit directly from the terminal. To do this, you would need to type <code>dtifit</code> in the terminal and choose the <code>dtifit</code> compulsory arguments:</p> Argument Description -k, --data dti data file -o, --out Output basename -m, --mask Bet binary mask file -r, --bvecs b vectors file -b, --bvals b values file <p>To run DTIfit from the terminal, you would need to navigate inside the <code>subdirectory/folder</code> with all the data and type the full <code>dtifit</code> command, specifying compulsory arguments as below:</p> <p><code>dtifit --data=data --mask=nodif_brain_mask --bvecs=bvecs --bvals=bvals --out=dti</code></p> <p>This command only works when running it from inside a folder where all the data is located, otherwise you will need to specify the full path with the data location.  This would be useful if you want to write a script; we will look at it in the later workshops.</p> <p>Running DTIfit from the terminal in your own time</p> <p>Again, please do NOT run it now but try it in your own time with data in the <code>p02</code> folder.</p> <p>The results of running DTIfit are several output files as specified below. We will look closer at the highlighted files in bold.  All of these files should be located in the <code>data</code> subdirectory, i.e. within <code>/rds/projects/c/chechlmy-chbh-mricn/xxx/diffusionMRI/DTIfit/p01/data/</code>.</p> Output File Description dti_V1 (V2, V3) 1st, 2nd, 3rd eigenvectors dti_L1 (L2, L3) 1st, 2nd, 3rd eigenvalues dti_FA Fractional Anisotropy map dti_MD Mean Diffusivity map dti_MO Mode of anisotropy (linear versus planar) dti_SO Raw T2 signal with no diffusion weighting <p>To do this, firstly close the FSL GUI, open FSLeyes and load the FA map ('File' \u2192 'Add from file' \u2192 <code>dti_FA</code>)</p> <p> </p> <p>Next add the principal eigenvector map (<code>dti_V1</code>) to your display ('File' \u2192 'Add from file' \u2192 <code>dti_V1</code>).</p> <p>FSLeyes will open the image <code>dti_V1</code> as a 3-direction vector image (RGB) with diffusion direction coded by colour.  To display the standard PDD colour coded orientation map (as below), you need to modulate the colour intensity with the FA map so that the anisotropic voxels appear bright. </p> <p>In the display panel (click on 'Settings' (the Cog icon)) and change 'Modulate' by setting it to <code>dti_FA</code>.</p> <p> </p> <p>Finally, compare the FA and MD maps (<code>dti_FA</code> and <code>dti_MD</code>). To do this, load the FA map and add the MD map.  By contrast to the FA map, the MD map appears uniform in both gray and white matter, plus higher intensities are in the CSF-filled ventricles and indicate higher diffusivity. This is opposed to dark ventricles in the FA map. </p> <p>Differences between the FA and MD maps</p> <p>Why are there such differences?</p>"},{"location":"workshop3/diffusion-mri-analysis/#tract-based-spatial-statistics-tbss","title":"Tract-Based Spatial Statistics (TBSS)Tract-Based Spatial Statistics pipeline in FSL","text":"<p>In the next part of the workshop, we will look at running TBSS, Tract-Based Spatial Statistics.</p> <p>TBSS is used for a whole brain \u201cvoxelwise\u201d cross-subject analysis of diffusion-derived measures, usually FA (fractional anisotropy).</p> <p>We will look at an example TBSS analysis of a small dataset consisting of FA maps from ten younger (y1-y10) and five older (o1-o5) participants.  Specifically, you will learn how to run the second stage of TBSS analysis, \u201cvoxelwise\u201d statistics, and learn how to display results using FSLeyes.  The statistical analysis that you will run aims to examine where on the tract skeleton younger versus older (two groups) participants have significantly different FA values.</p> <p>Before that, let's shortly recap TBSS as it was covered in the lecture. </p> <p>The steps for Tract-Based Spatial Statistics are:</p> <ol> <li>Fitting the diffusion tensor (DTIfit)</li> <li>Alignment of  all study participants\u2019 FA maps to standard space using non-linear registration</li> <li>Merging all participants\u2019 nonlinearly aligned FA maps into a single 4D image file and creating the mean FA image </li> <li>FA \u201cskeletonization\u201d (the mean FA skeleton representing the centres of major tracts specific to all participants is created)</li> <li>Each participant\u2019s aligned FA map is then projected back onto the skeleton prior to statistical analysis </li> <li>Hypothesis testing (voxelwise statistics)</li> </ol> <p>To save time, some of the pre-processing stages including generating FA maps (tensor fitting), preparing data for analysis, registration of FA maps and skeletonization have been run for you and all outputs are included in the <code>data</code> folder you have copied at the start of this workshop. </p> <p> </p> <p>You will only run the TBSS statistical analysis to explore group differences in FA values based upon age (younger versus older participants).</p> <p>First close FSLeyes (if you still have it open) and make sure that you do not have any processes running in the background by closing your current terminal.</p> <p>Then open a new terminal window, navigate to the subdirectory where pre-processed data are located and load both FSL and FSLeyes:</p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/diffusionMRI/TBSS/TBSS_analysis_p2/\nmodule load FSL/6.0.5.1-foss-2021a-fslpython\nmodule load FSLeyes/1.3.3-foss-2021a \n</code></pre> <p>Once you have loaded all the required software, we will start with exploring the pre-processed data. If you correctly followed the previous steps, you should be inside the subdirectory <code>TBSS_analysis_p2</code>.  Confirm that, and then check the content of that subdirectory by typing:</p> <p><code>pwd</code> (answer <code>/rds/projects/c/chechlmy-chbh-mricn/xxx/diffusionMRI/TBSS/TBSS_analysis_p2/</code>)</p> <p><code>ls</code> (you should see 3 data folders listed: <code>FA</code>, <code>origdata</code>, <code>stats</code>)</p> <p>We need to firstly check if all the pre-processing steps have been run correctly and that we have all the required files. </p> <p>Navigate inside the <code>stats</code> folder and check the files inside by typing in your terminal:</p> <pre><code>cd stats\nls\n</code></pre> <p>You should find inside the files listed below. </p> <ul> <li><code>all_FA</code> (4D image file with all participants\u2019 FA maps registered into standard space)</li> <li><code>mean_FA</code> (3D image file mean of all participants FA maps)</li> <li><code>all_FA_skeletonised</code> (4D image file with all participants skeletonised FA data)</li> <li><code>mean_FA_skeleton</code> (3D image file mean FA skeleton)</li> </ul> <p>Exploring the data</p> <p>If this is the case, open FSLeyes and explore these files one by one to make sure you understand what each represents. You might need to change the colour to visualise some image files. </p> <p>Remember to ask for help!</p> <p>If you are unsure about something, or need help, please ask!</p> <p>Once you have finished taking a look, close FSLeyes.</p> <p>Before using the General Linear Model (GLM) GUI to set up the statistical model, you need to determine the order in which participants\u2019 files have been entered into the single 4D skeletonized file (i.e., the data order in the <code>all_FA_skeletonised</code> file).  The easiest way to determine the alphabetical order of participants in the the final 4D file (<code>all_FA_skeletonised</code>), is to check in which order FSL lists the pre-processed FA maps inside the FA folder. You can do this in the terminal with the commands below</p> <pre><code>cd .. \ncd FA \nimglob *_FA.*\n</code></pre> <p>You should see data from the 5 older (o1-o5) followed by data fromthe  10 (y1-y10) younger participants.</p> <p>Next navigate back to the <code>stats</code> folder and open FSL:</p> <pre><code>cd ..\ncd stats\nfsl &amp;\n</code></pre> <p>Click on 'Miscellaneous tools' and select 'GLM Setup' to open the GLM GUI. </p> <p> </p> <p>In the workshop we will set up a simple group analysis (a two sample unpaired t-test).</p> <p>How to set up more complex models</p> <p>To find information re how to set up more complex models, using GUI, click on this link: https://fsl.fmrib.ox.ac.uk/fsl/docs/#/statistics/glm</p> <p>In the 'GLM Setup' window, change 'Timeseries design' to 'Higher-level/non-timeseries design' and '# inputs' to 15.</p> <p>Then click on 'Wizard' and select 'two groups, unpaired' and set 'Number of subjects in first group' to 5. Then click 'Process'.</p> <p> </p> <p>In the 'EVs' tab, name 'EV1' and 'EV2' as per your groups (old, young).</p> <p>In the contrast window set number of contrasts to 2 and re-name them accordingly to the image below: </p> <p>(C1: old &gt; young, [1 -1]) (C2: young &gt; old, [-1 1])</p> <p> </p> <p>Click 'View Design', close the image and then go back to the GLM set window and save your design with the filename <code>design</code>. Click 'Exit' and close FSL.</p> <p>To run the TBSS statistical analysis FSL's <code>randomise</code> tool is used.</p> <p>FSL's randomise</p> <p>Randomise is FSL's tool for nonparametric permutation inference on various types of neuroimaging data (statistical analysis tool). For more information click on this link: https://fsl.fmrib.ox.ac.uk/fsl/docs/#/statistics/randomise</p> <p>The basic command line to use this tool is:</p> <p><code>randomise -i &lt;input&gt; -o &lt;input&gt; -d &lt;design.mat&gt; -t &lt;design.con&gt; [options]</code></p> <p>You can explore options and the set up by typing <code>randomise</code> in your terminal.</p> <p>The basic command line to use randomise for TBSS is below:</p> <p><code>randomise -i all_FA_skeletonised -o tbss -m mean_FA_skeleton_mask -d design.mat -t design.con -n 500 --T2</code></p> <p>Check if you are inside the <code>stats</code> folder and run the command above in terminal to run your TBSS group analysis:</p> <p>The elements of this command are explained below:</p> Argument Description -i input image -o output image basename -m mask -d design matrix -t design contrast -n number of permutations --T2 TFCE <p>Why so few permutations?</p> <p>To save time we only run 500 permutations; this number will vary depending on the type of analysis, but usually it is between 5,000 to 10,000 or higher.</p> <p>The output from <code>randomise</code> will include two raw (unthresholded) tstat images, <code>tbss_tstat1</code> and <code>tbss_tstat2</code>. </p> <p>The TFCE p-value images (fully corrected for multiple comparisons across space) will be <code>tbss_tfce_corrp_tstat1</code> and <code>tbss_tfce_corrp_tstat2</code>.</p> <p>Based on the set up of your design, contrast 1 gives the older &gt; young test and contrast 2 gives the young &gt; older test; the contrast which will likely give significant results is the 2nd contrast i.e., we are expecting higher FA in younger participants (due to the age related decline in FA).</p> <p>To check that, use FSLeyes to view results of your TBSS analysis. Open FSLeyes, load <code>mean_FA</code> plus the <code>mean_FA_skeleton</code> template and add your display TFCE corrected stats-2 image:</p> <ol> <li>'File' -&gt; 'Add from file' -&gt; <code>mean_FA.nii.gz</code></li> <li>File -&gt; 'Add from file' -&gt; <code>mean_FA_skeleton.nii.gz</code> (change greyscale to green)</li> <li>File -&gt; 'Add from file' -&gt; <code>tbss_tfce_corrp_tstat2.nii.gz</code> (change greyscale to red-yellow and set up Max to 1, and Min to 0.95 or 0.99)</li> </ol> <p>Please note that TFCE-corrected images, are actually 1-p for convenience of display, so thresholding at 0.95 gives significant clusters at p corrected &lt; 0.05, and 0.99 gives significant clusters at p corrected &lt; 0.01.</p> <p>You should see the same results as below:</p> <p> </p> <p>Interpreting the results</p> <p>Are the results as expected? Why/why not?</p> <p>Reviewing the tstat1 image</p> <p>Next review the <code>tbss_tfce_corrp_tstat1.nii.gz</code> </p> <p>Further information on TBSS</p> <p>More information on TBSS, can be found on the 'TBSS' section of the FSL Wiki: https://fsl.fmrib.ox.ac.uk/fsl/docs/#/diffusion/tbss</p>"},{"location":"workshop3/workshop3-intro/","title":"Workshop 3 - Basic diffusion MRI analysis","text":"<p>Welcome to the third workshop of the MRICN course! Prior lectures in the module introduced you to basics of the diffusion MRI and its applications, including data acquisition, the theory behind diffusion tensor imaging and using tractography to study structural connectivity. The aim of the next two workshops is to introduce you to some of the core FSL tools used for diffusion MRI analysis. </p> <p>Specifically, we will explore different elements of the FMRIB's Diffusion Toolbox (FDT) to walk you through basic steps in diffusion MRI analysis. We will also cover the use of Brain Extraction Tool (BET). </p> <p>By the end of the two workshops, you should be able to understand the principles of correcting for distortions in diffusion MRI data, how to run and explore results of a diffusion tensor fit, and how to run a whole brain group analysis and probabilistic tractography. </p> <p>Overview of Workshop 3</p> <p>Topics for this workshop include:</p> <ul> <li>Visualizing diffusion data using FSLeyes (before and after distortion correction)</li> <li>Using FSL's Brain Extraction Tool (BET) to create a brain mask</li> <li>Understand and perform diffusion tensor fitting (DTIfit) to generate key diffusion metrics like FA (Fractional Anisotropy) and MD (Mean Diffusivity)</li> <li>Learn to conduct Tract-Based Spatial Statistics (TBSS) for group-level comparisons of diffusion data</li> </ul> <p>We will be working with various previously acquired datasets (similar to the data acquired during the CHBH MRI Demonstration/Site visit). We will not go into details as to why and how specific sequence parameters and specific values of the default settings have been chosen. Some values should be clear to you from the lectures or assigned on Canvas readings, please check there, or if you are still unclear, feel free to ask. </p> <p>Note that for your own projects, you are very likely to want to change some of these settings/parameters depending on your study aims and design. </p> <p>The copy of this workshop notes can be found on Canvas 39058 - LM Magnetic Resonance Imaging in Cognitive Neuroscience in Week 03 workshop materials.</p>"},{"location":"workshop8/functional-connectivity/","title":"Functional connectivity analysis of resting-state fMRI data using FSL","text":"<p>This workshop is based upon the excellent FSL fMRI Resting State Seed-based Connectivity tutorial by Dianne Paterson at the University of Arizona, which has been adapted to run on the BEAR systems at the University of Birmingham, with some additional content covering Neurosynth.</p> <p>We will run a group-level functional connectivity analysis on resting-state fMRI data of three participants, specifically examining the functional connectivity of the posterior cingulate cortex (PCC), a region of the default mode network (DMN) that is commonly found to be active in resting-state data. </p> <p>Overview of Workshop 8</p> <p>To do this, we will:</p> <ul> <li>extract a mean-timeseries for a PCC seed region for each participant,</li> <li>run single-subject level analyses, one manually and bash scripting the other two, </li> <li>run a group-level analysis using the single-level results </li> <li>figure out which brain regions our active voxels are in, using atlases in FSL, and Neurosynth.</li> </ul>"},{"location":"workshop8/functional-connectivity/#preparing-the-data","title":"Preparing the data","text":"<p>Navigate to your shared directory within the MRICN folder and copy the data over:</p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx\ncp -r /rds/projects/c/chechlmy-chbh-mricn/aamir_test/SBC .\ncd SBC\nls\n</code></pre> <p>You should now see the following:</p> <pre><code>sub1 sub2 sub3\n</code></pre> <p>Each of the folders has a single resting-state scan, called <code>sub1.nii.gz</code>,<code>sub2.nii.gz</code> and <code>sub3.nii.gz</code> respectively. </p> <p>We will now create our seed region for the PCC. To do this, firstly load FSL and <code>fsleyes</code> in the terminal by running: </p> <pre><code>module load FSL/6.0.5.1-foss-2021a\nmodule load FSLeyes/1.3.3-foss-2021a\n</code></pre> <p>Check that we are in the correct directory (<code>blah/your_username/SBC</code>):</p> <pre><code>pwd\n</code></pre> <p>and create a new directory called <code>seed</code>:</p> <pre><code>mkdir seed\n</code></pre> <p>Now when you run <code>ls</code> you should see:</p> <pre><code>seed sub1 sub2 sub3\n</code></pre> <p>Lets open FSLeyes:</p> <pre><code>fsleyes &amp;\n</code></pre> Creating the PCC mask in FSLeyes <p>We need to open the standard MNI template brain, select the PCC and make a mask.</p> <p>Here are the following steps: </p> <ol> <li>Navigate to the top menu and click on <code>File \u279c Add standard</code> and select <code>MNI152_T1_2mm_brain.nii.gz</code>.</li> <li>When the image is open, click on <code>Settings \u279c Ortho View 1 \u279c Atlases</code>. An atlas panel then opens on the bottom section.</li> <li>Select <code>Atlas information</code> (if it already hasn't loaded).</li> <li>Ensure Harvard-Oxford Cortical Structural Atlas is selected.</li> <li>Go into 'Atlas search' and type <code>cing</code> in the search box. Check the Cingulate Gyrus, posterior division (lower right) so that it is overlaid on the standard brain. (The full name may be obscured, but you can always check which region you have loaded by looking at the panel on the bottom right).</li> </ol> <p> </p> <p> At this point, your window should look something like this: </p> <p> </p> <p> To save the seed, click the save symbol which is the first of three icons on the bottom left of the window.  </p> <p> </p> <p>The window that opens up should be your project SBC directory. Open into the <code>seed</code> folder and save your seed as <code>PCC</code>. </p> Extracting the time-series <p>We now need to binarise the seed and to extract the mean timeseries. To do this, leaving FSLeyes open, go into your terminal (you may have to press Enter if some text about <code>dc.DrawText</code> is there) and type:</p> <pre><code>cd seed\nfslmaths PCC -thr 0.1 -bin PCC_bin\n</code></pre> <p>In FSLeyes now click File \u279c Add from file, and select <code>PCC_bin</code> to compare <code>PCC.nii.gz</code> (before binarization) and <code>PCC_bin.nii.gz</code> (after binarization). You should note that the signal values are all 1.0 for the binarized PCC.</p> <p> </p> <p>You can now close FSLeyes.</p> <p>For each subject, you want to extract the average time series from the region defined by the PCC mask. To calculate this value for <code>sub1</code>, do the following: </p> <pre><code>cd ../sub1\nfslmeants -i sub1 -o sub1_PCC.txt -m ../seed/PCC_bin\n</code></pre> <p>This will generate a file within the <code>sub1</code> folder called <code>sub1_PCC.txt</code>. </p> <p>We can have a look at the contents by running <code>cat sub1_PCC.txt</code>. The terminal will print out a list of numbers with the last five being:</p> <pre><code>20014.25528\n20014.919\n20010.17317\n20030.02886\n20066.05141\n</code></pre> <p>This is the mean level of 'activity' for the PCC at each time-point.</p> <p>Now let's repeat this for the other two subjects. </p> <pre><code>cd ../sub2\nfslmeants -i sub2 -o sub2_PCC.txt -m ../seed/PCC_bin\ncd ../sub3\nfslmeants -i sub3 -o sub3_PCC.txt -m ../seed/PCC_bin\n</code></pre> <p>Now if you go back to the SBC directory and list all of the files within the subject folders:</p> <pre><code>cd ..\nls -R\n</code></pre> <p>You should see the following: </p> <p> </p> <p>This is all we need to run the subject and group-level analyses using FEAT.</p>"},{"location":"workshop8/functional-connectivity/#running-the-feat-analyses","title":"Running the FEAT analyses","text":""},{"location":"workshop8/functional-connectivity/#single-subject-analysis","title":"Single-subject analysisExamining the FEAT outputScripting the other two subjects","text":"<p>Close your terminal, open another one, move to your <code>SBC</code> folder, load FSL and open FEAT: </p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/SBC\nmodule load bear-apps/2022b\nmodule load FSL/6.0.7.6\nsource $FSLDIR/etc/fslconf/fsl.sh\nFeat &amp;\n</code></pre> <p>We will run the first-level analysis for <code>sub1</code>. Set-up the following settings in the respective tabs:</p> <p>Data</p> <p>Number of inputs:</p> <ul> <li>Click 'Select 4D data', then click the folder icon, go into the <code>sub1</code> folder and choose <code>sub1.nii.gz</code>. Click OK. You will see a box saying that the 'Input file has a TR of 1...' this is fine, just click OK again.</li> </ul> <p>Output directory: </p> <ul> <li>Click into the <code>sub1</code> folder and click OK. Nothing will be in the right hand column, but that is because there are no folders within <code>sub1</code>. We will create our <code>.feat</code> folder within <code>sub1</code>. </li> </ul> <p>This is what your data tab should look like (with the input data opened for show).</p> <p> </p> <p>Pre-stats</p> <p>The data has already been pre-processed, so just set 'Motion correction' to 'None' and uncheck BET. Your pre-stats should look like this: </p> <p> </p> <p>Registration</p> <p>Nothing needs to be changed here. </p> <p>Stats</p> <p>Click on 'Full Model Setup' and do the following: </p> <ol> <li>Keep the 'Number of original EVs' as 1.</li> <li>Type PCC for the 'EV' name.</li> <li>Select 'Custom (1 entry per volume)' for the 'Basic' shape. Click into the <code>sub1</code> folder and select <code>sub1_PCC.txt</code>. This is the mean time series of the PCC for sub-001 and is the statistical regressor in our GLM model. This is different from analyses of task-based data which will usually have an <code>events.tsv</code> file with the onset times for each regressor of interest.</li> <li>Select 'None' for 'Convolution', and uncheck both 'Add temporal derivate' and 'Apply temporal filtering'. </li> </ol> <p>What are we doing specifically?</p> <p>The first-level analysis will subsequently identify brain voxels that show a significant correlation with the seed (PCC) time series data.</p> <p>Your window should look like this: </p> <p> </p> <p>In the same General Linear Model window, click the 'Contrast &amp; F-tests' tab, type PCC in the title, and click 'Done'. </p> <p>A blue and red design matrix will then be displayed. You can close it.</p> <p>Post-stats </p> <p>Nothing needs to be changed here.</p> <p>You are ready to run the first-level analysis. Click 'Go' to run. On BEAR, this should only take a few minutes. </p> <p>To actually examine the output, go to the BEAR Portal and at the menu bar select <code>Files \u279c /rds/projects/c/chechlmy-chbh-mricn/</code> </p> <p> </p> <p>  Then go into <code>SBC/sub1.feat</code>, select <code>report.html</code> and click 'View' (top left of the window). Navigate to the 'Post-stats' tab and examine the outputs. It should look like this: </p> <p> </p> <p>We can now run the second and third subjects. As we only have three subjects, we could manually run the other two by just changing three things: </p> <ol> <li>The fMRI data path</li> <li>The output directory</li> <li>The <code>sub_PCC.txt</code> path</li> </ol> <p>Whilst it would probably be quicker to do it manually in this case, it is not practical in other instances (e.g., more subjects, subjects with different number of scans etc.). So, instead we will be scripting the first level FEAT analyses for the other two subjects.</p> <p>The importance of scripting</p> <p>Scripting analyses may seem challenging at first, but it is an essential skill of modern neuroimaging research. It enables you to automate repetitive processing steps, dramatically reduces the chance of human error, and ensures your research is reproducible.</p> <p>To do this, go back into your terminal, you don't need to open a new terminal or close FEAT.</p> <p>The setup for each analysis is saved as a specific file, the <code>design.fsf</code> file within the FEAT output directory. We can see this by opening the <code>design.fsf</code> file for <code>sub1</code>:</p> <pre><code>pwd # make sure you are in your SBC directory e.g., blah/xxx/SBC\ncd sub1.feat\ncat design.fsf\n</code></pre> <p>FEAT acts as a large 'function' with its many variables corresponding to the options that we choose when setting up in the GUI. We just need to change three of these (the three mentioned above). In the <code>design.fsf</code> file this corresponds to:</p> <pre><code>set fmri(outputdir) \"/rds/projects/c/chechlmy-chbh-mricn/xxx/SBC/sub1\"\nset feat_files(1) \"/rds/projects/c/chechlmy-chbh-mricn/xxx/SBC/sub1/sub1/\"\nset fmri(custom1) \"/rds/projects/c/chechlmy-chbh-mricn/xxx/SBC/sub1/sub1_PCC.txt\"\n</code></pre> <p>To run the script, please copy the <code>run_feat.sh</code> script into your own <code>SBC</code> directory:</p> <pre><code>cd ..\npwd # make sure you are in your SBC directory\ncp /rds/projects/c/chechlmy-chbh-mricn/axs2210/SBC/run_feat.sh .\n</code></pre> <p>Viewing the script</p> <p>If you would like, you can have a look at the script yourself by typing <code>cat run_bash.sh</code></p> <p>The first line <code>#!/bin/bash</code> is always needed to run <code>bash</code> scripts. The rest of the code just replaces the 3 things we wanted to change for the defined subjects, <code>sub2</code> and <code>sub3</code>.</p> <p>Run the code (from your SBC directory) by typing <code>bash run_feat.sh</code>. (It will ask you for your University account name, this is your ADF username (axs2210 for me)).</p> <p>The script should take about 5-10 minutes to run on BEAR.</p> <p>After it has finished running, have a look at the <code>report.html</code> file for both directories, they should look like this:</p> <p>sub2</p> <p>sub3</p>"},{"location":"workshop8/functional-connectivity/#group-level-analysis","title":"Group-level analysisExamining the output","text":"<p>Ok, so now that we have our FEAT directories for all three subjects, we can run the group level analysis. Close FEAT and open a new FEAT by running <code>Feat &amp;</code> in your <code>SBC</code> directory. </p> <p>Here are instructions on how to setup the group-level FEAT:</p> <p>Data </p> <ol> <li>Change 'First-level analysis' to 'Higher-level analysis'</li> <li>Keep the default option for 'Inputs are lower-level FEAT directories'.</li> <li>Keep the 'Number of inputs' as 3.</li> <li>Click the 'Select FEAT directories'. Click the yellow folder on the right to select the FEAT folder that you had generated from each first-level analysis.</li> </ol> <p>Your window should look like this (before closing the 'Input' window):</p> <p> </p> <p></p> <p>\u00a0\u00a0\u00a0\u00a05. Keep 'Use lower-level COPEs' ticked.</p> <p>\u00a0\u00a0\u00a0\u00a06. In 'Output directory' stay in your current directory (SBC), and in the bottom bar, type in <code>PCC_group</code> at the end of the file path. </p> <p>Don't worry about it being empty, FSL will fill out the file path for us. </p> <p>If you click the folder again, it should look similar to this (with your ADF username instead of <code>axs2210</code>): </p> <p> </p> <p> Stats</p> <ol> <li>Leave the 'Mixed effects: FLAME 1' and click 'Full model setup'. </li> <li>In the 'General Linear Model' window, name the model 'PCC' and make sure the 'EVs' are all 1s. </li> </ol> <p>The interface should look like this:</p> <p> </p> <p>After that, click 'Done' and close the GLM design matrix that pops up (you don't need to change anything in the 'Contrasts and F-tests' tab).</p> <p>Post-stats</p> <ol> <li>Change the Z-threshold from 3.1 to 2.3.</li> </ol> <p>Lowering our statistical threshold</p> <p>Why do you think we are lowering this to 2.3 in our analysis instead of keeping it at 3.1? The reason is because we only have three subjects, we want to be relatively lenient with our threshold value, otherwise we might not see any activation at all!  For group-level analyses with more subjects, we would be more strict.</p> <p>Click 'Go' to run! </p> <p>This should only take about 2-3 minutes. </p> <p>While this is running, you can load the <code>report.html</code> through the file browser as you did for the individual subjects. </p> <p>Click on the 'Results' tab, and then on 'Lower-level contrast 1 (PCC)'. When the analysis has finished, your results should look like this: </p> <p> </p> <p>These are voxels demonstrating significant functional connectivity with the PCC at a group-level (Z &gt; 2.3).</p> <p>So, we have just ran our group-level analysis. Let's have a closer look at the outputted data. </p> <p>Close FEAT and your terminal, open a new terminal, go to your <code>SBC</code> directory and open FSLeyes: </p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/SBC\nmodule load FSL/6.0.5.1-foss-2021a\nmodule load FSLeyes/1.3.3-foss-2021a\nfsleyes &amp;\n</code></pre> <p>In FSLeyes, open up the standard brain (Navigate to the top menu and click on 'File \u279c Add standard' and select <code>MNI152_T1_2mm_brain.nii.gz</code>). </p> <p>Then add in our contrast image (File  \u279c Add from file, and then go into the <code>PCC_group.gfeat</code> and then into <code>cope1.feat</code> and open the file <code>thresh_zstat1.nii.gz</code>). </p> <p>When opened, change the colour to 'Red-Yellow' and the 'Minimum' up to 2.3 (The max should be around 3.12). If you set the voxel location to [42, 39, 52] your screen should look like this:</p> <p> </p> <p>This is the map that we saw in the <code>report.html</code> file. In fact we can double check this by changing the voxel co-ordinates to [45, 38, 46].</p> <p>Our thresholded image in fsleyes </p> <p> </p> <p> The FEAT output  Our image matches the one on the far right below:  </p> <p> </p>"},{"location":"workshop8/functional-connectivity/#bonus-identifying-regions-of-interest-with-atlases-and-neurosynth","title":"Bonus: Identifying regions of interest with atlases and Neurosynth","text":"<p>So we know which voxels demonstrate significant correlation with the PCC, but what region(s) of the brain are they located in? </p> <p>Let's go through two ways in which we can work this out. </p> <p>Firstly, as you have already done in the course, we can simply just overlap an atlas on the image and see which regions the activated voxels fall under. </p> <p>To do this:</p> <ol> <li>Navigate to the top menu and click on 'Settings \u279c Ortho View 1 \u279c Atlases'. </li> <li>Then at the bottom middle of the window, select the 'Harvard-Oxford Cortical Structural Atlas' and on the window directly next to it on the right, click 'Show/Hide'. </li> <li>The atlas should have loaded up but is blocking the voxels. Change the 'Opacity' to about a quarter. </li> </ol> <p> </p> <p> By having a look at the 'Location' window (bottom left) we can now see that significant voxels of activity are mainly found in the: </p> <p>Right superior lateral occipital cortex</p> <p>Posterior cingulate cortex (PCC) / precuneus</p> <p>Alternatively, we can also use Neurosynth, a website where you can get the resting-state functional connectivity of any voxel location or brain region. It does this by extracting data from studies and performing a meta-analysis on brain imaging studies that have results associated with your voxel/region of interest.</p> <p>About Neurosynth</p> <p>While Neurosynth has been superseded by Neurosynth Compose we will use the original Neurosynth in this tutorial.</p> <p>If you click the following link, you will see regions demonstrating significant connectivity with the posterior cingulate.</p> <p>If you type [46, -70, 32] as co-ordinates in Neurosynth, and then into the MNI co-ordinates section in FSLeyes, not into the voxel location, because Neurosynth works with MNI space, you can see that in both cases the right superior lateral occipital cortex is activated. </p> <p>Image orientation</p> <p>Note that the orientations of left and right are different between Neurosynth and FSLeyes!</p> <p>Neurosynth</p> <p>FSLeyes</p> <p> This is a great result given that we only have three subjects!</p> <p>Learning outcomes of this workshop</p> <p>In this workshop, you have:</p> <ul> <li>Created a seed region in the posterior cingulate cortex (PCC) using FSL's standard brain and atlases</li> <li>Extracted mean time-series data from the PCC for three subjects</li> <li>Run a single-subject functional connectivity analysis manually using FEAT</li> <li>Learned to automate analyses by scripting FEAT for multiple subjects using bash</li> <li>Conducted a group-level analysis to identify regions showing functional connectivity with the PCC</li> <li>Used two different methods to identify active brain regions:<ul> <li>FSL's Harvard-Oxford atlas for anatomical localization</li> <li>Neurosynth for validating findings against meta-analytic data</li> </ul> </li> <li>Successfully identified functional connectivity between the PCC and lateral occipital cortex, replicating known patterns of functional connectivity</li> </ul>"}]}