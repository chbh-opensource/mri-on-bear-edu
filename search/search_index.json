{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"MRI on BEAR","text":"<p>MRI on BEAR is a collection of educational resources created by members of the Centre for Human Brain Health (CHBH), University of Birmingham, to provide a basic introduction to fundamentals in magnetic resonance imaging (MRI) data analysis, using the computational resources available to the University of Birmingham research community.</p>"},{"location":"#about-this-website","title":"About this website","text":"<p>This website contains workshop materials created for the MSc module 'Magnetic Resonance Imaging in Cognitive Neuroscience' (MRICN) and its earlier version - Fundamentals in Brain Imaging taught by Dr Peter C. Hansen - at the School of Psychology, University of Birmingham. It is a ten-week course consisting of lectures and workshops introducing the main techniques of functional and structural brain mapping using MRI with a strong emphasis on - but not limited to - functional MRI (fMRI). Topics include the physics of MRI, experimental design for neuroimaging experiments and the analysis of fMRI, and other types of MRI data. This website includes only the workshop materials, which provide a basic training in analysis of brain imaging data and data visualization. </p> <p>Learning objectives</p> <p>At the end of the course you will be able to:</p> <ul> <li>Demonstrate an understanding of the basic concepts involved in MRI</li> <li>Show an understanding of how to design fMRI experiments</li> <li>Have the ability to work with BlueBEAR in a Linux environment and to use appropriate software to view and interpret MRI data</li> <li>Be able to analyse simple fMRI experiments and conduct basic tractography analysis</li> </ul> <p>For externals not on the course</p> <p>Whilst we have made these resources publicly available for anyone to use, please BEAR in mind that the course has been specifically designed to run on the computing resources at the University of Birmingham.</p>"},{"location":"#teaching-staff","title":"Teaching Staff","text":"Dr Magdalena Chechlacz <p>Role: Course Lead</p> <p>Magdalena Chechlacz is an Assistant Professor in Cognition and Ageing at the School of Psychology, University of Birmingham. She initially trained and carried out a doctorate in Cellular and Molecular Biology in 2002, and after working as a biologist at the University of California, San Diego, decided on a career change to a more human-oriented science and neuroimaging. She completed a second doctorate in psychology at the University of Birmingham under the supervision of Glyn Humphreys in 2012, and from 2013 to 2016, held a British Academy Postdoctoral Fellowship and EPA Cephalosporin Junior Research Fellowship at Linacre College, University of Oxford. In 2016, Dr Chechlacz returned to the School of Psychology, University of Birmingham as a Bridge Fellow.</p> <p></p> <p> </p> Aamir Sohail <p>Role: Teaching Assistant</p> <p>Aamir Sohail is an MRC Advanced Interdisciplinary Methods (AIM) DTP PhD student based at the Centre for Human Brain Health (CHBH), University of Birmingham, where he is supervised by Lei Zhang and Patricia Lockwood. He completed a BSc in Biomedical Science at Imperial College London, followed by an MSc in Brain Imaging at the University of Nottingham. He then worked as a Junior Research Fellow at the Centre for Integrative Neuroscience and Neurodynamics (CINN), University of Reading. Outside of research, he is also passionate about facilitating inclusivity and diversity in academia, as well as promoting open and reproducible science.</p>"},{"location":"#course-overview","title":"Course Overview","text":"Workshop Key Concepts/Tools Getting Started BEAR Portal, BEAR Storage, BlueBEAR Workshop 1 BlueBEAR GUI, Linux commands Workshop 2 MRI data formats, FSLeyes, MRI atlases Workshop 3 DTIfit, TBSS, BET Workshop 4 Probabilistic tractography, BEDPOSTX, PROBTRACKX Workshop 5 FEAT, First-level fMRI analysis Workshop 6 Bash scripting, Submitting jobs, Containers Workshop 7 Higher-level fMRI analysis, FEATquery Workshop 8 Resting-state fMRI, Functional connectivity, Neurosynth <p>Accessing additional course materials</p> <p>If you are a CHBH member and would like access to additional course materials (lecture recordings etc.), please contact one of the teaching staff members listed above.</p>"},{"location":"#license","title":"License","text":"<p>MRI on BEAR is hosted on GitHub. All content in this book (i.e., any files and content in the <code>docs/</code> folder) is licensed under the Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license. Please see the <code>LICENSE</code> file in the GitHub repository for more details.</p> <p> </p>"},{"location":"contributors/","title":"Contributors","text":"<p>Many thanks to our contributors for creating and maintaining these resources!</p> <sub>Andrew Quinn</sub>\ud83d\udea7 \ud83d\udd8b <sub>Aamir Sohail</sub>\ud83d\udea7 \ud83d\udd8b <sub>James Carpenter</sub>\ud83d\udd8b <sub>Magda Chechlacz</sub>\ud83d\udd8b <p>Acknowledgements</p> <p>Thank you to Charnjit Sidhu for their help and support in developing these materials!</p>"},{"location":"resources/","title":"Additional Resources","text":"<p>For those wanting to develop their learning beyond the scope of the module, here is a (non-exhaustive) list of links and pages for neuroscientists covering skills related to working with neuroimaging data, both with the concepts and practical application. </p> <p>Contributing to the list</p> <p>Feel free to suggest additional resources to the list by opening a thread on the GitHub page!</p> <p>FSL Wiki</p> <p>Most relevant to the course is the FSL Wiki, the comprehensive guide for FSL by the Wellcome Centre for Integrative Neuroimaging at the University of Oxford.</p>"},{"location":"resources/#existing-lists-of-resources","title":"Existing lists of resources","text":"<p>Here are some current 'meta-lists' which already cover a lot of resources themselves: </p> <ul> <li>Methods in Neuro Steven Weisberg's GitHub extensive list of resources covering the physics of MRI/fMRI, computational/programming, tools for the analysis of MRI/fMRI data, and online datasets, as part of his 'Methods in Neuroimaging' course at the University of Florida.</li> <li>Hitchhacker's guide to the brain A 'docs' style website with lists of resources for each stage of neuroimaging analysis including file organisation, planning. preregistration, data collection, pre-processing and analysis, and sharing data. By Remi Gau, McGill University and others.</li> <li>On-line neuroimaging resources A farily comprehensive list of 'softwares, databases, tutorials, blogs and other resources relevant to learn about neuroimaging or to help perform neuroimaging analysis'. Curated by Remi Gau, McGill University.</li> <li>Dartbrains A notebook style introduction to neuroimaging in Python. The materials cover how scanner generates data, how psychological states can be probed in the scanner, and how this data can be processed and analyzed. Created by Luke Chang, Dartmouth College.</li> <li>Awesome Magnetic Resonance Imaging (MRI) 'A curated list of delightful Magnetic Resonance (MR) courses, books, lectures, papers, blogs and free resources.' Created by Daniel Gomez, Harvard/MIT.</li> <li>Awesome Neuroscience 'A curated list of awesome neuroscience libraries, software and any content related to the domain.'  Created by Akash Tandon.</li> <li>fMRI-Resources A GitHub list not dissimilar to this one, providing information and resources on functional MRI. Created by John Pyles.</li> </ul>"},{"location":"resources/#neuroimaging","title":"Neuroimaging","text":"Conceptual understanding <p>Struggling to grasp the fundamentals of MRI/fMRI? Want to quickly refresh your mind on the physiological basis of the BOLD signal? Well, these resources are for you!</p> <ul> <li>Principles of fMRI The OG YouTube series for understanding the conceptual basis of MRI/fMRI. Created by Martin Lindquist and Tor Wager of Dartmouth College.</li> <li>Neuroimaging Research Methods Another YouTube channel for learning about MRI/fMRI including research methods. Created by Rasmus Birn, University of Wisconsin-Madison.</li> <li>Introduction to Principles of MRI A short book and associated simulation code for learning the principles of magnetic resonance imaging (MRI). Created by Peder Larson for students at UCSF.</li> <li>Questions and Answers in MRI Ever had a question about the basis of MRI/fMRI? Written from the perspective of a physicist, this website was specifically made to answer these questions. Created by Allen Elster, Washington University School of Medicine.</li> <li>fMRI Bootcamp A lecture series on fMRI, both conceptual and methodological by Rebecca Saxe, MIT.</li> <li>MIT 9.13 The Human Brain, Spring 2019 A course which 'surveys the core perceptual and cognitive abilities of the human mind and explores how they are implemented in the brain'. Delivered by Nancy Kanwisher, MIT.</li> </ul> Analysis of fMRI data <ul> <li>Introduction to Working with MRI Data in Python A Software Carpentries course covering MRI file types, organisational formats (e.g., BIDS) and working with open datasets.</li> <li>Andy's Brain Book The OG of neuroimaging tutorials. I don't know many trainee neuroimagers who haven't used Andy's amazing guides. Highly recommended to also check out his YouTube channel as well. Created by Andrew Jahn, University of Michigan.</li> <li>NI-edu A website covering two courses, \u201cfMRI-introduction\u201d (basic concepts and methodology of functional MRI (fMRI) research) and \u201cfMRI-pattern-analysis\u201d (machine-learning based \u2018decoding\u2019 and representational similarity analysis (RSA)), which are in a notebook format. Created by Lucas Snoek, University of Amsterdam.</li> <li>U of A: Neuroimaging Core Documentation Docs covering a range of neuroimaging tutorials including BIDS, ANTS, FSL, ITK-SNAP and more. Created by Dianne Paterson, University of Arizona.</li> <li>Data analysis for Neuroimaging (DAFNI) Denis Schluppeck's materials for the MSc Cognitive Neuroscience course at the University of Nottingham, covering SPM, git, FSL and MATLAB.</li> <li>Practice and theory of brain imaging A comprehensive course on neuroimaging in Python, with modules on reproducibility in programming/neuroimaging. Created by the Nipraxis team (Matthew Brett, Chris Markiewicz, Oscar Estaban, Zvi Baratz, Peter Rush).</li> <li>Psych 214 \u2013 functional MRI methods A 'hands-on course teaching the principles of functional MRI (fMRI) data analysis' created for students at UC Berkeley by Matthew Brett and JB Poline.</li> </ul>"},{"location":"resources/#programming","title":"Programming","text":"Unix/Linux <ul> <li>The Unix Shell Software Carpentries workshop on Unix.</li> <li>Ubuntu Linux Guide Nutanix's guide to Linux on Ubuntu.</li> </ul>"},{"location":"resources/#textbooks","title":"Textbooks","text":"<ul> <li> <p>An Introduction to Resting State fMRI Functional Connectivity (2017, Oxford University Press) by Janine Bijsterbosch, Stephen M. Smith, and Christian F. Beckmann</p> </li> <li> <p>Handbook of Functional MRI Data Analysis (2011, Cambridge University Press) by Russell A. Poldrack, Jeanette A. Mumford, and Thomas E. Nichols</p> </li> <li> <p>Introduction to Functional Magnetic Resonance Imaging (1998, Cambridge University Press) by Richard B. Buxton</p> </li> <li> <p>Introduction to Neuroimaging Analysis (2018, Oxford University Press) by Mark Jenkinson and Michael Chappell</p> </li> <li> <p>Short Introduction to Brain Anatomy for Neuroimaging (2018, Oxford University Press) by Mark Jenkinson and Michael Chappell</p> </li> <li> <p>Short Introduction to the General Linear Model (2018, Oxford University Press) by Mark Jenkinson and Michael Chappell</p> </li> <li> <p>Short Introduction to MRI Physics (2018, Oxford University Press) by Mark Jenkinson and Michael Chappell</p> </li> </ul>"},{"location":"setting-up/","title":"Accessing BlueBEAR and the BEAR Portal","text":"<p>Before you start with any workshop materials, you will need to familiarise yourself with the CHBH\u2019s primary computational resource, BlueBEAR. The following pages are aimed at helping you get started. </p> <p>To put these workshop materials into practical use you will be expected to understand what BlueBEAR is, what it is used for and to make sure you have access.</p> <p>Student Responsibility</p> <p>If you are an MSc student taking the MRICN module, please note that while there will be help available during all in person workshops, in case you have any problems with using the BEAR Portal, it is your responsibility to make sure that you have access, and that you are familiar with the information provided in pre-workshop materials. Failing to gain an understanding of BlueBEAR and using the BEAR Portal will prevent you from participating in the practical sessions and completing the module\u2019s main assessment (data analysis). </p>"},{"location":"setting-up/#what-are-bear-and-bluebear","title":"What are BEAR and BlueBEAR?Signing in to the BEAR Portal","text":"<p>BEAR stands for Birmingham Environment for Academic Research and is a collection of services provided specifically for researchers at the University of Birmingham. BEAR services are used by researchers at the Centre for Human Brain Health (CHBH) for various types of neuroimaging data analysis.</p> <p>BEAR services and basic resources - such as the ones we will be using for the purpose of the MRICN module - are freely available to the University of Birmingham research community. Extra resources which may be needed for some research projects can be purchased e.g., access to dedicated nodes and extra storage. This is something your PI/MSc/PhD project supervisor might be using and will give you access to.</p> <p> </p> <p>BlueBEAR refers to the Linux High Performance Computing (HPC) environment which:</p> <ol> <li>Enables researchers to run jobs simultaneously on many servers (thus providing fast and efficient processing capacity for data analysis)</li> <li>Gives easy access to multiple apps, software libraries (e.g., software we will be using in this module to analyse MRI data), as well as various software development tools</li> </ol> <p>As computing resources on BlueBEAR rely on Linux, in Workshop 1 you will learn some basic commands, which you will need to be familiar with to participate in subsequent practical sessions and to complete the module\u2019s main assessment (data analysis assessment). More Linux commands and basic principle of scriptings will be introduced in subsequent workshops.</p> <p>There are two steps to gaining access to BlueBEAR:</p> <ul> <li>Being a member on an active BEAR project </li> <li>Having a BEAR Linux account</li> </ul> <p>Gaining access to BEAR Projects</p> <p>Only a member of academic staff e.g., your project supervisor or module lead, can apply for a BEAR project. As a student you cannot apply for a BEAR project. If you are registered as a student on the MRICN module, you should have already been added as member to the project <code>chechlmy-chbh-mricn</code>. If not please contact one of the teaching staff.</p> <p>Even if you are already a member of a BEAR project giving you BlueBEAR access, you will still need to activate your BEAR Linux account via the self-service route or the service desk form. The information on how to do it and step-by-step instructions are available on the University Advanced Research Computing page, see the following link.</p> <p>Please follow these steps as above to make sure you have a BEAR Linux account before starting with workshop 1 materials. To do this you will need to be on campus or using the University Remote Access Service (VPN). </p> <p>After you have activated your BEAR Linux account, you can now sign-in to the BEAR Portal.</p> <ul> <li>In a web browser navigate to https://portal.bear.bham.ac.uk to access the BEAR Portal</li> <li>To log in, please use your university username and password </li> <li>First select the \u2018University of Birmingham\u2019 button as below and next log in via the University\u2019s Single Sign-On page </li> </ul> <p>BEAR Portal access requirements</p> <p>Remember that the BEAR Portal is only available on campus or using the VPN!</p> <p> </p> <p>If your log in is successful, you will be directed to the main BEAR Portal page as below. This means that you have successfully launched the BEAR Portal.</p> <p> </p> <p>If you get to this page, you are ready for Workshop 1. For now, you can log out. If you have any problems logging on to BEAR Portal, please email chbh-help@contacts.bham.ac.uk for help and advice. </p>"},{"location":"setting-up/#bear-storage","title":"BEAR Storage","text":"<p>The storage associated with each BEAR project is called the BEAR Research Data Store (RDS). Each BEAR project gets 3TB of storage space for free, but researchers (e.g., your MSc project supervisor) can pay for additional storage if needed. The RDS should be used for all data, job scripts and output on BlueBEAR.</p> <p> </p> <p>If you are registered as a student on the MRICN module, all the data and resources you will need to participate in the MRICN workshops and to complete the module\u2019s main assessment have been added to the MRICN module RDS, and you have been given access to the folder <code>/rds/projects/c/chechlmy-chbh-mricn</code>. When working on your MSc project using BEAR services, your supervisor will direct you to the relevant RDS project.</p> <p>External access to data</p> <p>If you are not registered on the module and would like access to the data, please contact one of the teaching staff members.</p>"},{"location":"setting-up/#finding-additional-information","title":"Finding additional information","text":"<p>There is extensive BEAR technical documentation provided by the University of Birmingham BEAR team (see links below). While for the purpose of this module, you are not expected to be familiar with all the provided there information, you might find it useful if you want to know more about computing resources available to researchers at CHBH via BEAR services, especially if you will be using BlueBEAR for other purposes (e.g., for your MSc project).</p> <p>You can find out more about BEAR, BlueBEAR and RDS on the dedicated BEAR webpages: </p> <ul> <li> <p>University of Birmingham BEAR Homepage</p> </li> <li> <p>More information on BlueBEAR</p> </li> <li> <p>More information on Research Data Storage</p> </li> </ul>"},{"location":"workshop1/intro-to-bluebear/","title":"Introduction to the BlueBEAR Portal","text":"<p>At this point you should know how to log in and access the main BEAR Portal page. </p> <p>Please navigate to https://portal.bear.bham.ac.uk, log in and launch the BEAR Portal; you should get to the page as below.</p> <p> </p> <p>BlueBEAR Portal is a web-based interface enabling access to various BEAR services and BEAR apps including:</p> <ul> <li>Files in RDS storage</li> <li>Data science apps and software</li> <li>BlueBEAR GUI </li> <li>Code Server Editor</li> <li>Submitting jobs to run on BlueBEAR cluster</li> <li>Information on currently running jobs and interactive sessions.</li> </ul> <p> BlueBEAR portal is basically a user friendly alternative to using the command line interface, your computer terminal.</p> <p> </p> <p>To view all files and data you have access to on BlueBEAR, click on 'Files' as illustrated above. You will see your home directory (your BEAR Linux home directory), and all RDS projects you are a member of. </p> <p>You should be able to see <code>/rds/projects/c/chechlmy-chbh-mricn</code> (MRICN module\u2019s RDS project). By selecting the 'Home Directory' or any 'RDS project' you will open a second browser tab, displaying the content. In the example below, you see the content of one of Magda's projects.</p> <p> </p> <p>Inside the module\u2019s RDS project, you will find that you have a folder labelled xxx, where xxx is your University of Birmingham ADF username. If you navigate to that folder <code>rds/projects/c/chechlmy-chbh-mricn/xxx</code>, you will be able to perform various file operations from there. However, for now, please do not move, download, or delete any files.</p> <p>Data confidentiality</p> <p>Please also note that the MRI data you will be given to work with should be used on BlueBEAR only and not downloaded on your personal desktop or laptop!</p>"},{"location":"workshop1/intro-to-bluebear/#launching-the-bluebear-gui","title":"Launching the BlueBEAR GUI","text":"<p>The BlueBEAR Portal options in the menu bar, 'Jobs', 'Clusters' and 'My Interactive Sessions' can be used to submit and edit jobs to run on the BlueBEAR cluster and to get information about your currently running jobs and interactive sessions. Some of these processes can be also executed using Code Server Editor (VS Code) accessible via Interactive Apps. We won\u2019t explore these options in detail now but some of these will be introduced later when needed. </p> <p> </p> <p>For example, from the 'Cluster' option you can jump directly on BlueBEAR terminal and by using this built-in terminal, submit data analysis jobs and/or employ own contained version of neuroimaging software rather than software already available on BlueBEAR. We will cover containers, scripting and submitting jobs in later workshops. For now, just click on this option and see what happens; you can subsequently exit/close the terminal page.</p> <p>Finally, from the BlueBEAR Portal menu bar you can select 'Interactive Apps' and from there access various GUI applications you wish to use, including JupyterLab, RStudio, MATLAB and most importantly the BlueBEAR GUI, which we will be using to analyse MRI data in the subsequent workshops. </p> <p> </p> <p>Please select 'BlueBEAR GUI'. This will bring up a page for you to specify options for your job to start the BlueBEAR GUI. You can leave some of these options as default. But please change \u201cNumber of Hours\u201d to 2 (our workshops will last 2 hours; for some other analysis tasks you might need more time) and make sure that the selected 'BEAR Project' is <code>chechlmy-chbh-mricn</code>. Next click on Launch.</p> <p> </p> <p>It will take few minutes for the job to start. Once it\u2019s ready you\u2019ll see an option to connect to the BlueBEAR GUI. Click on 'Launch BlueBEAR GUI'. </p> <p> </p> <p>Once you have launched the BlueBEAR GUI, you are now in a Linux environment, on a Linux Desktop. The following section will guide you on navigating and using this environment effectively.</p> <p>Re-launching the BlueBEAR GUI</p> <p>In the main window of the BlueBEAR portal you will be able to see that you have an Interactive session running (the information above will remain there). This is important as if you close the Linux Desktop by mistake, you can click on Launch BlueBEAR GUI again to open it.</p>"},{"location":"workshop1/intro-to-linux/","title":"Introduction to Linux","text":"<p>Linux is a computer Operating System (OS) similar to Microsoft Windows or Mac OS. Linux is very widely used in the academic world especially in the sciences. It is derived from one of the oldest and most stable and used OS platforms around, Unix. We use Linux on BlueBEAR. Many versions of Linux are freely available to download and install, including CentOS (Community ENTerprise Operating System) and Debian, which you might be familiar with. You can also use these operating systems with Microsoft Windows in Dual Boot Environment on your laptop or desktop computer. </p> <p>Linux and neuroimaging</p> <p>Linux is particularly suited for clustering computers together and for efficient batch processing of data. All major neuroimaging software runs on Linux. This includes FSL, SPM, AFNI, and many others. Linux, or some version of Unix, is used in almost all leading neuroimaging centres. Both MATLAB and Python also run well in a Linux environment.</p> <p>If you work in neuroimaging, it is to your advantage to become familiar with Linux. The more familiar you are, the more productive you will become. For some of you, this might be a challenge. The environment will present a new learning experience, one that will take time and effort to learn. But in the end, you should hopefully realize that the benefits of learning to work in this new computer environment are indeed worth the effort. </p> <p>Linux is not like the Windows or Mac OSX environments. It is best used by typing commands into a Terminal client and by writing small batch command programs. Frequently you may not even need to use the mouse. Using the Linux environment alone may take some getting used to, but will become more familar throughout the course, as we use them to navigate through our file system and to script our analyses. For now, we will simply explore using the Linux terminal and simple commands.</p>"},{"location":"workshop1/intro-to-linux/#using-the-linux-terminal","title":"Using the Linux Terminal","text":"<p>BlueBEAR GUI enables to load various apps and applications by using the Linux environment and a built-in Terminal client. Once you have launched the BlueBEAR GUI, you will see a new window and from there you can open the Terminal client. There are different ways to open Terminal in BlueBEAR GUI window as illustrated below.</p> <p>Either by selecting from the drop-down menu:</p> <p> </p> <p> Or by selecting the folder at the bottom of the screen:</p> <p> </p> <p> In either case you will load the terminal:</p> <p> </p> <p> Once you have started the terminal you, you will be able to load required applications (e.g., to start the FSL GUI). FSL (FMRIB Software Library) is a neuroimaging software package we will be using in our workshops for MRI data analysis. </p> <p>When using the BlueBEAR GUI Linux desktop, you can simultaneously work in four separate spaces/windows. For example, if you are planning on using multiple apps, rather than opening multiple terminals and apps in the same space, you can move to another space. You can do that by clicking on \u201cworkspace manager\u201d in Linux desktop window.</p> <p> </p> <p>Linux is fundamentally a command line-based operating system, so although you can use the GUI interface with many applications, it is essential you get used to issuing commands through the Terminal interface to improve your work efficiency. </p> <p>Make sure you have an open Terminal as per the instructions above. Note that a Terminal is a text-based interface, so generally the mouse is not much use. You need to get used to taking your hand off the mouse and letting go of it. Move it away, out of reach. You can then get used to using both hands to type into a Terminal client. </p> <p><code>[chechlmy@bear-pg0210u07a ~]$</code> as shown above in the Terminal Client is known as the system prompt. The prompt usually identifies the user and the system hostname. You can type commands at the system prompt (press the Enter key after each command to make it run). The system then returns output based on your command to the same Terminal. </p> <p>Try typing <code>ls</code> in the Terminal.</p> <p>This command tells Linux to print a list of the current directory contents. We will get back later to basic Linux commands, which you should learn to use BlueBEAR for neuroimaging data analysis. </p> <p>Why bother with Linux?</p> <p>You may wonder why you should invest the time to learn the names of the various commands needed to copy files, change directories and to do general things such as run image analysis programs via the command line. This may seem rather clunky. However, the commands you learn to run on the command line in a terminal can alternatively be written in a text file. This text file can then be converted to a batch script that can be run on data sets using the BlueBEAR cluster, potentially looping over hundreds or thousands of different analyses, taking many days to run. This is vastly more efficient and far less error prone than using equivalent graphical tools to do the same thing, one at a time.</p> <p>When you open a new terminal window it opens in a particular directory. By default, this will be your home directory: </p> <p><code>/rds/homes/x/xxx</code> </p> <p>or the Desktop folder in your home directory:</p> <p><code>/rds/homes/x/xxx/Desktop</code>(where x is the first letter of your last name and xxx is your University of Birmingham ADF username).</p> <p>On BlueBEAR files are stored in directories (folders) and arranged into a tree hierarchy. </p> <p>Examples of directories on BlueBEAR include:</p> <ul> <li><code>/rds/homes/x/xxx</code> (your home directory) </li> <li><code>/rds/projects/c/chechlmy-chbh-mricn</code> (our module RDS project directory) </li> </ul> <p>Directory separators on Linux and Windows</p> <p>/ (forward slash) is the Linux directory separator. Note that this is different from Windows (where the backward slash \\ is the directory separator).</p> <p>The current directory is always called <code>.</code> (i.e. a single dot).</p> <p>The directory above the current directory is always called <code>..</code> (i.e. dot dot) </p> <p>Your home directory can always be accessed using the shortcut <code>~</code> (the tilde symbol). Note that this is the same as <code>/rds/homes/x/xxx</code>.</p> <p>You need to remember this to use and understand basic Linux Commands.</p>"},{"location":"workshop1/intro-to-linux/#basic-linux-commands","title":"Basic Linux Commands","text":"<p>pwd (Print Working Directory) </p> <p>In a Terminal type <code>pwd</code> followed by the return (enter) key to find out the name of the directory where you are.  You are always in a directory and can (usually) move to directories above you or below to subdirectories. </p> <p>For example if you type <code>pwd</code> in your terminal you will see: <code>/rds/homes/x/xxx</code>    (e.g., <code>/rds/homes/c/chechlmy</code>)</p> <p>cd (Change Directory) </p> <p>In a Terminal window, type <code>cd</code> followed by the name of a directory to gain access to it. Keep in mind that you are always in a directory and normally are allowed access to any directories hierarchically above or below.</p> <p>Type in your terminal the examples below:</p> <p><code>cd /rds/projects</code></p> <p><code>cd /rds/homes/</code></p> <p><code>cd ..</code> (to change to the directory above using .. shortcut) </p> <p>To find out where you are now, type <code>pwd</code>:</p> <p>(answer: <code>/rds</code>)</p> <p>If the directory is not located above or below the current directory, then it is often less confusing to write out the complete path instead.  Try this in your terminal:</p> <p><code>cd /rds/homes/x/xxx/Desktop</code> (where x is the first letter of your last name and xxx is your ADF username)</p> <p>Changing directories with full paths</p> <p>Note that it does not matter what directory you are in when you execute this command, the directory will always be changed based on the full pathway you specified. </p> <p>Remember that the tilde symbol <code>~</code> is a shortcut for your home directory. Try this: </p> <pre><code>cd /rds/projects \ncd ~ \npwd\n</code></pre> <p>You should be now back in your home directory.</p> <p>ls (List Files) </p> <p>The <code>ls</code> command (lowercase L, S) allows you to see a summary list of the files and directories located in the current directory. Try this: </p> <pre><code>cd /rds/projects/c\nls\n</code></pre> <p>(you should now see a long list of various BEAR RDS projects)</p> <p>Before moving to the next section, please close your terminal by clicking on \u201cx\u201d in the top right of the Terminal.</p> <p>cp (Copy files/directories) </p> <p>The <code>cp</code> command will copy files and/or directories FROM a source TO a destination in the current working directory. This command will create the destination file if it doesn't exist. In some cases, to do that you might need to specify a complete path to a file location.</p> <p>Here are some examples (please do not type them, they are only examples):</p> Command Function <code>cp myfile yourfile</code> Basic file copy (in current directory) <code>cp data data_copy</code> Copy a directory (but not sub-directories) <code>cp -r ~fred/data .</code> Recursively copy <code>fred</code> dir to current dir <code>cp ~fred/fredsfile myfile</code> Copy remote file and rename it <code>cp ~fred/* .</code> Copy all files from <code>fred</code> dir to current dir <code>cp ~fred/test* .</code> Copy all files that begin with test e.g. <code>test</code>, <code>test1.txt</code> <p>In the subsequent workshops we will practice using the <code>cp</code> command. For now, looking at the examples above to understand its usage. There are also some exercises below to check your understanding.</p> <p>mv, rmdir and mkdir (Moving, removing and making files/directories) </p> <p>The <code>mv</code> command will move files FROM a source TO a destination. It works like copy, except the file is actually moved. If applied to a single file, this effectively changes the name of the file. (Note there is no separate renaming command in Linux). The command also works on directories. </p> <p>Here are some examples (again please do not type these in): </p> Command Function <code>mv myfile yourfile</code> renames file <code>mv ~/data/somefile somefile</code> moves file <code>mv ~/data/somefile yourfile</code> moves and renames <code>mv ~/data/* .</code> moves multiple files <p>There are also the <code>mkdir</code> and <code>rmdir</code> commands:</p> <ul> <li><code>mkdir</code> \u2013 to make a new directory e.g.        <code>mkdir testdir</code> </li> <li><code>rmdir</code> \u2013 to remove an empty directory e.g.   <code>rmdir testdir</code> </li> </ul> <p>You can try these two commands. Open a new Terminal and type:</p> <pre><code>mkdir testdir\nls\n</code></pre> <p>In your home directory you will see now a new directory <code>testdir</code>. Now type:</p> <pre><code>rmdir testdir\nls\n</code></pre> <p>You should notice that the <code>testdir</code> has been removed from your home directory.</p> <p>To remove a file you can use the <code>rm</code> command. Note that once files are deleted at the command line prompt in a terminal window, unlike in Microsoft Windows, you cannot get files back from the wastebin.</p> <p>e.g.    <code>rm junk.txt</code> (this is just an example, do not type it in your terminal)</p> <p>Clearing your terminal</p> <p>Often when running many commands, your terminal will be full and difficult to understand. To clear the terminal screen type <code>clear</code>. This is an especially helpful command when you have been typing lots of commands and need a clean terminal to help you focus.</p> Linux commands in general  <p>Note that most commands in Linux have a similar syntax:  <code>command name [modifiers/options] input output</code></p> <p>The syntax of the command is very important. There must be spaces in between the different parts of the command. You need to specify input and output. The modifiers (in brackets) are optional and may or may not be needed depending on what you want to achieve. </p> <p>For example, take the following command: </p> <p><code>cp -r /rds/projects/f/fred/data ~/tmp</code>  (This is an example, do not type this) </p> <p>In the above example <code>-r</code> is an option meaning 'recursive' often used with <code>cp</code> and other commands, used in this case to copy a directory including all its content from one directory to another directory.</p>"},{"location":"workshop1/intro-to-linux/#opening-fsl-on-the-bluebear-gui","title":"Opening FSL on the BlueBEAR GUI","text":"<p>FSL (FMRIB Software Library) is a software library containing multiple tools for processing, statistical analyses, and visualisation of magnetic resonance imaging (MRI) data. Subsequent workshops will cover usage of some of the FSL tools for structural, functional and diffusion MRI data. This workshop only covers how to start FSL app on BlueBEAR GUI Linux desktop, and some practical aspects of using FSL, specifically running it in the terminal either in the foreground or in the background. </p> <p>There are several different versions of FSL software available on BlueBEAR. You can search which versions of FSL are available on BlueBEAR as well as all other available software using the following link: https://bear-apps.bham.ac.uk</p> <p>From there you will also find information how to load different software. Below you will find an example of loading one of the available versions of FSL.</p> <p>To open FSL in terminal, you first need to load the FSL module. To do this, you need to type in the Terminal a specific command. </p> <p>First, either close the Terminal you have been previously using and open a new one, or simply clean it. Next, type:</p> <p><code>module load FSL/6.0.5.1-foss-2021a</code></p> <p>You will see various processes running the terminal. Once these have stopped and you see a system prompt in the terminal, type: </p> <p><code>fsl</code></p> <p>This <code>fsl</code> command will initiate the FSL GUI as shown below.</p> <p> </p> <p>Now try typing <code>ls</code> in the same terminal window and pressing return. </p> <p>Notice how nothing appears to happen (your keystrokes are shown as being typed in but no actual event seems to be actioned). Indeed, nothing you type is being processed and the commands are being ignored. That is because the <code>fsl</code> command is running in the foreground in the terminal window. Because of that it is blocking other commands from being run in the same terminal.</p> <ul> <li>Now close FSL by clicking on the 'Exit' button in the FSL GUI.  </li> </ul> <p>Notice now that control has been returned to the Terminal and how commands you type are now being acted on. Try typing <code>ls</code> again; it should now work in the Terminal.</p> <ul> <li> <p>Go back to the terminal window again, but this time type <code>fsl &amp;</code> at the system prompt and press return. Again, you should see the FSL GUI pop up. </p> </li> <li> <p>Now try typing <code>ls</code> in the same Terminal. </p> </li> </ul> <p>Notice that your new commands are now being processed. The <code>fsl</code> command is now running in the background in the Terminal allowing you to run other commands in parallel from the same Terminal.  Typing the <code>&amp;</code> after any command makes it run in the background and keeps the Terminal free for you to use. </p> <p>Sometimes you may forget to type <code>&amp;</code> after a command. </p> <ul> <li>Close all open windows, open a new terminal and type <code>fsl</code> (without the &amp;) so that it is running in the foreground. </li> <li>Now hold down the CTRL key and the z key together 'CTRL-z'. </li> </ul> <p>You should get a message like <code>\u201c[1]+ Stopped fsl\u201d</code>. You will notice that the FSL GUI is now unresponsive (try clicking on some of the buttons). The <code>fsl</code> process has been suspended. </p> <ul> <li>To make it run again in the background type <code>bg</code> in the terminal window (followed by pressing the return key). </li> </ul> <p>You should find the FSL GUI is now responsive again and input to the terminal now works once more. If you clicked the 'Exit' button when the FSL GUI was unresponsive, FSL might close now.</p> <p>Running and killing commands in the terminal</p> <p>If, for some reason, you want to make the command run in the foreground then rather than typing <code>bg</code> (for background) instead type <code>fg</code> (for foreground).  If you want to kill (rather than suspend) a command that was running in the foreground, press CTRL-c (CTRL key and c key).</p> <p>Linux: some final useful tips</p> <p>TIP 1:</p> <p>When typing a command - or the name of a directory or file - you never need to type everything out. The terminal will self-complete the command or file name if you type the TAB key as you go along. Try using TAB key when typing commands or complete path to specific directory.</p> <p>TIP 2:</p> <p>If you need help understanding what the options are, or how to use a command, try adding <code>--help</code> to the end of your command. For example, for better understanding of the <code>du</code> options, type: </p> <p><code>du --help [enter]</code></p> <p>TIP 3:</p> <p>There are many useful online lists of these various commands, for example: www.ss64.com/bash</p> <p>Exercise: Basic Linux commands</p> <p>Please complete the following exercises, you should hopefully know which Linux commands to use!</p> <ul> <li>clean up your Terminal </li> <li><code>cd</code> back to your home directory </li> <li>make sure you are in your home directory</li> <li>make a new directory called <code>test</code></li> <li>rename this directory to <code>test1</code> and make another directory called <code>test2</code></li> <li>move or copy directory <code>test1</code> to your folder on modules\u2019s RDS project (i.e., <code>rds/projects/c/chechlmy-chbh-mricn/xxx</code>)</li> <li>delete the <code>test1</code> and <code>test2</code> directories and confirm it</li> </ul> <p>If unsure, check your results with someone else or ask for help!</p> <p> The correct commands are provided below. (click to reveal)</p> Linux Commands Exercise (Answers) <ol> <li> <p><code>clear</code></p> </li> <li> <p><code>cd ~</code> or <code>cd /rds/homes/x/xxx</code></p> </li> <li> <p><code>pwd</code></p> </li> <li> <p><code>mkdir test</code></p> </li> <li> <p><code>mv test test1</code> <code>mkdir test2</code></p> </li> <li> <p><code>cp -r test1 /rds/projects/c/chechlmy-chbh-mricn/xxx/</code> or <code>mv test1 /rds/projects/c/chechlmy-chbh-mricn/xxx/</code></p> </li> <li> <p><code>rm -r test1 test2</code> <code>ls</code></p> </li> </ol> <p>Workshop 1: Further Reading and Reference Material</p> <p>Here are some additional resources that introduce users to Linux:</p> <ul> <li>A useful textbook on essential Linux commands is the Linux Pocket Guide by Daniel J. Barratt.</li> <li>The Carpentries (an organisation providing free training for various software engineering and data science skills) have an introduction to UNIX course.</li> <li>Iowa State University also have a course introducing users to UNIX.</li> </ul> <p>The copy of this workshop notes can be found on Canvas 39058 - LM Magnetic Resonance Imaging in Cognitive Neuroscience in Week 01 workshop materials.</p>"},{"location":"workshop1/workshop1-intro/","title":"Workshop 1 - Introduction to BlueBEAR and Linux","text":"<p>Welcome to the first workshop of the MRICN course!</p> <p>In this workshop we will introduce you to the environment where will run all of our analyses: the BlueBEAR Portal. Whilst you can run neuroimaging analyses on your own device, the various software and high computing resources needed necessitates a specifically-curated environment. At the CHBH we use BlueBEAR, which satisfies both of these needs. You will also be introduced to the Linux operating system (OS), which is the OS supported on the BlueBEAR Portal through Ubuntu.  Linux OS is similar to other operating systems such as Mac OS and Windows, and can similarly be navigated by using terminal commands. Learning how to do so is key when working with data on Linux, and - as you will see in future workshops - is particularly useful when creating and running scripts for more complex data analyses.</p> <p>Overview of Workshop 1</p> <p>Topics for this workshop include:</p> <ul> <li>Introduction to BlueBEAR portal</li> <li>Using the BlueBEAR Graphical User Interface (GUI) environment</li> <li>Files and Directories in BEAR Portal</li> <li>Introduction to Linux</li> <li>Using the Linux Terminal</li> <li>Basic Linux Commands</li> </ul> <p>Pre-requisites for the workshop</p> <p>Please ensure that you have completed the 'Setting Up' section of this course, as you will require access to the BEAR Portal for this workshop.</p> <p>The copy of this workshop notes can be found on Canvas 39058 - LM Magnetic Resonance Imaging in Cognitive Neuroscience in Week 01 workshop materials.</p>"},{"location":"workshop2/mri-data-formats/","title":"Working with MRI Data - Files and Formats","text":"MRI Image Fundamentals <p>When you acquire an MRI image of the brain, in most cases it is either a 3D image i.e., a volume acquired at one single timepoint (e.g., T1-weighted, FLAIR scans) or a 4D multi-volume image acquired as a timeseries (e.g., fMRI scans). Each 3D volume consists of multiple 2D slices, which are individual images.</p> <p>The volume consists of 3D voxels, with a typical size between 0.25 to 4mm, but not necessarily same in all three directions. For example, you can have voxel size [1mm x 1mm x 1mm] or [0.5mm x 0.5mm x 2mm]. The voxel size represents image resolution.</p> <p>The final important feature of an MRI image is field of view (FOV), a matrix of voxels represented as the voxel size multiplied by number of voxels. It provides information about the coverage of the brain in your MRI image. The FOV is sometime provided for the entire 3D volume or the individual 2D slice. Sometimes, the FOV is defined based on slice thickness and number of acquired slices.  </p> Image and standard space <p>When you acquire MRI images of the brain, you will find that these images will be different in terms of head position, image resolution and FOV, depending on the sequence and data type (e.g., T1 anatomical, diffusion MRI, fMRI). We often use term \u201cimage space\u201d to depict these differences i.e., structural (T1), diffusion or functional space. </p> <p>In addition, we also use term \"standard space\" to represent standard dimensions and coordinates of the template brain, which are used when reporting results of group analyses. Our brains differ in terms of size and shape and thus for the purpose of our analyses (both single-subject and group-level) we need to use standard space. The most common brain template is the MNI152 brain (an average of 152 healthy brains). </p> <p>The process of alignment between different image spaces is called registration or normalization, and its purpose is to make sure that voxel and anatomical locations correspond to the same parts of the brain for each image type and/or participant.</p>"},{"location":"workshop2/mri-data-formats/#mri-data-formats","title":"MRI Data Formats","text":"<p>MRI scanners collect MRI data in an internal format that is unique to the scanner manufacturer, e.g., Philips, Siemens or GE. The manufacturer then allows you to export the data into a more usable intermediate format. We often refer to this intermediate format as raw data as it is not directly usable and needs to be converted before being accessible to most neuroimaging software packages. </p> <p>The most common format used by various scanner manufacturers is the DICOM format. DICOM images corresponding to a single scan (e.g., a T1-weighted scan) might be one large file or multiple files (1 per each volume or one per each slice acquired). This will depend on the scanner and data server used to retrieve/export data from the scanner. There are other data formats e.g., PAR/REC that are specific to Philips scanners. The raw data needs to be converted into a format that the analysis packages can use. </p> <p>Retrieving MRI data at the CHBH</p> <p>At CHBH we have a Siemens 3T PRISMA scanner. When you acquire MRI scans at CHBH, data is pushed directly to a data server in the DICOM format. This should be automatic for all research scans. In addition, for most scans, this data is also directly converted to NIfTI format. So, at the CHBH you will likely retrieve MRI data from the scanner in NIfTI format.</p> <p> </p> <p>NIfTI (Neuroimaging Informatics Technology Initiative) is the most widely used format for MRI data, accessible by majority of the neuroimaging software packages e.g., FSL or SPM. Another older data format which is still sometimes used, is Analyze (with each image consisting of two files <code>.img</code> and <code>.hdr</code>).</p> <p>NIfTI format files have either the extension <code>.nii</code> or <code>.nii.gz</code> (compressed <code>.nii</code> file), where there is only one NIfTI image file per scan. DICOM files usually have a suffix of <code>.dcm</code>, although these files might be additionally compressed with <code>gzip</code> as <code>.dcm.gz</code> files.</p>"},{"location":"workshop2/mri-data-formats/#working-with-mri-data","title":"Working with MRI Data","text":"<p>We will now ourselves convert some DICOM images to NIfTI, using some data collected at the CHBH.</p> <p>Servers do not always provide MRI data as NIfTIs</p> <p>While at CHBH you can download the MRI data in NIfTI format, this might not be the case at some other neuroimaging centres. Thus, you should learn how to do it yourself. </p> <p>The data is located in <code>/rds/projects/c/chechlmy-chbh-mricn/module_data/CHBH</code>.</p> <p>First, log in into the BlueBEAR Portal and start a BlueBEAR GUI session (2 hours). Open a new terminal window and navigate to your MRICN project folder:</p> <p><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx</code>  [where XXX=your ADF username] </p> <p>Next copy the data from CHBH scanning sessions:</p> <pre><code>cp -r /rds/projects/c/chechlmy-chbh-mricn/module_data/CHBH .\npwd\n</code></pre> <p>After typing <code>pwd</code>, the terminal should show <code>/rds/projects/c/chechlmy-chbh-mricn/xxx</code> (i.e., you should be inside your MRICN project folder).</p> <p>Then type:</p> <pre><code>cd CHBH \nls\n</code></pre> <p>You should see data from 3 scanning sessions. Note that there are two files per scan session. One is labelled <code>XXX_dicom.zip</code>. This contains the DICOM files of all data from the scan session. The other file is labelled <code>XXX_nifti.zip</code>. This contains the NIFTI files of the same data, converted from DICOM. </p> <p>In general, both DICOM and NifTI data should be always copied from the server and saved by the researcher after each scan session. The DICOM file is needed in case there are problems with the automatic conversion to NIfTI. However, most of the time the only file you will need to work with is the <code>XXX_nifti.zip</code> file containing NIfTI versions of the data. </p> <p>We will now unpack some of the data to explore the data structure. In your terminal, type: </p> <pre><code>unzip 20191008#C4E7_nifti.zip\ncd 20191008#C4E7_nifti\nls\n</code></pre> <p>You should see six files listed as below, corresponding to 3 scans (two fMRI scans and one structural scan):</p> <pre><code>2.5mm_2000_fMRI_v1_6.json \n2.5mm_2000_fMRI_v1_6.nii.gz \n2.5mm_2000_fMRI_v1_7.json \n2.5mm_2000_fMRI_v1_7.nii.gz \nT1_vol_v1_5.json \nT1_vol_v1_5.nii.gz \n</code></pre> <p>JSON files</p> <p>You may have noticed that for each scan file (NifTI file, <code>.nii.gz</code>), there is also an autogenerated <code>.json file</code>. This is an information file (in an open standard format) that contains important information for our data analysis. For example, the <code>2.5mm_2000_fMRI_v1_6.json</code> file contains slice timing information about the exact point in time during the 2s TR (repetition time) when each slice is acquired, which can be used later in the fMRI pre-processing. We will come back to this later in the course.</p> <p>For now, let's look at another dataset. In your terminal type:</p> <pre><code>cd ..\nunzip 20221206#C547_nifti.zip\ncd 20221206#C547_nifti\nls\n</code></pre> <p>You should now see a list of 10 files, corresponding to 3 scans (two diffusion MRI scans and one structural scan). For each diffusion scan, in addition to the <code>.nii.gz</code> and <code>.json</code> files, there are two additional files, <code>.bval</code> and <code>.bvec</code> that contain important information about gradient strength and gradient directions (as mentioned in the MRI physics lecture). These two files are also needed for later analysis (of diffusion MRI data).</p> <p>We will now look at a method for converting data from the DICOM format to NIfTI.  </p> <pre><code>cd ..\nunzip 20191008#C4E7_dicom.zip\ncd 20191008#C4E7_dicom\nls\n</code></pre> <p>You should see a list of 7 sub-directories. Each top level DICOM directory contains sub-directories with each individual scan sequence. The structure of DICOM directories can vary depending on how it is stored/exported on different systems. The 7 sub-directories here contain data for four localizer scans/planning scans, two fMRI scans and one structural scan. Each sub-directory contains several <code>.dcm</code> files.</p> <p>There are several software packages which can be used to convert DICOM to NIfTI, but <code>dcm2niix</code> is the most widely used. It is available as standalone software, or part of MRIcroGL a popular tool for brain visualization similar to FSLeyes. <code>dcm2niix</code> is available on BlueBEAR, but to use it you need to load it first using the terminal.</p> <p>To do this, in the terminal type:</p> <p><code>module load bear-apps/2022b</code></p> <p>Wait for the apps to load and then type:</p> <p><code>module load dcm2niix/1.0.20230411-GCCcore-12.2.0</code></p> <p>To convert the <code>.dcm</code> files in one of the sub-directories to NIfTI using <code>dcm2niix</code> from terminal, type:</p> <p><code>dcm2niix T1_vol_v1_5</code></p> <p>If you now check the <code>T1_vol_v1_5</code> sub-directory, you should find there a single <code>.nii</code> file and a <code>.json</code> file.</p> <p>Converting more MRI data</p> <p>Now try to convert to NIfTI the <code>.dcm</code> files from the scanning session <code>20221206#C547</code> with 3 DICOM sub-directories, the two diffusion scans <code>diff_AP</code> and <code>diff_PA</code> and one structural scan MPRAGE. </p> <p>To do this, you will first need to change current directory, unzip, change directory again and then run the <code>dcm2niix</code> command as above.</p> <p>If you have done it correctly you will find <code>.nii</code> and <code>.json</code> files generated in the structural sub-directories, and in the diffusion sub-directories you will also find <code>.bval</code> and <code>.bvec</code> files.</p> <p>Now that we have our MRI data in the correct format, we will take a look at the brain images themselves using FSLeyes.</p>"},{"location":"workshop2/visualizing-mri-data/","title":"MRI data visualization with FSLeyes","text":"<p>FSL (FMRIB Software Library) is a comprehensive neuroimaging software library for the analysis of structural and functional MRI data. FSL is widely used, freely available, runs on both Linux and Mac OS as well as on Windows via a Virtual Machine. </p> <p>FSLeyes is the FSL viewer for 3D and 4D data. FSLeyes is available on BlueBEAR, but you need to load it first. You can just load FSLeyes as a standalone software, but as it is often used with other FSL tools, you often want to load both (FSL and FSLeyes). </p> <p>In this session we will only be loading FSLeyes by itself, and not with FSL.</p> <p>FSL Wiki</p> <p>Remember that the FSL Wiki is an important source for all things FSL!</p>"},{"location":"workshop2/visualizing-mri-data/#getting-started-with-fsleyes","title":"Getting started with FSLeyes","text":"<p>Assuming that you have started directly from the previous page, first close your previous terminal (to close <code>dcm2nii</code>). Then open a new terminal and to navigate to the correct folder, type in your terminal:</p> <p><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/CHBH</code></p> <p>To open FSLeyes, type:</p> <p><code>module load FSL/6.0.5.1-foss-2021a-fslpython</code></p> <p>There are different version of FSL on BlueBEAR, however this is the one which you need to use it together with FSLeyes.</p> <p>Wait for FSL to load and then type:</p> <p><code>module load FSLeyes/1.3.3-foss-2021a</code></p> <p>Again, wait for FSLeyes to load (it may take a few minutes). After this, to open FSLeyes, type in your terminal:</p> <p><code>fsleyes &amp;</code></p> <p>The importance of '&amp;'</p> <p>Why do we type <code>fsleyes &amp;</code> instead of <code>fsleyes</code>?</p> <p> You should then see the setup below, which is the default FSLeyes viewer without an image loaded.</p> <p> </p> <p>You can now load/open an image to view. Click 'File' \u2192 'Add from file' (and then select the file in your directory e.g., <code>rds/projects/c/chechlmy-chbh-mricn/xxx/CHBH/visualization/T1.nii</code>).</p> <p>You can also type directly in the terminal <code>fsleyes file.nii.gz</code> where you replace <code>file.nii.gz</code> with the name of the actual file you want to open.  However, you will need to include the full path to the file if you are not in the same directory when you open the terminal window e.g. <code>fsleyes rds/projects/c/chechlmy-chbh-mricn/xxx/CHBH/visualization/T1.nii</code></p> <p>You should now see a T1 scan loaded in ortho view with three canvases corresponding to the sagittal, coronal, and axial planes.</p> <p> </p> <p>Please now explore the various settings in the ortho view panel:</p> <p> </p> <p>Also notice the abbreviations on the three canvases:</p> <ul> <li>L/R: Left/Right</li> <li>S/I: Superior/Inferior</li> <li>A/P: Anterior/Posterior</li> </ul> <p>FSL comes with a collection of\u00a0NIFTI standard templates, which are used for image registration and normalisation (part of MRI data analysis). You can also load these templates in FSLeyes.</p> <p>To load a template, Click 'File' \u2192 'Add Standard' (for example select the file named <code>MNI152_T1_2mm.nii.gz</code>. If you still have the <code>T1.nii</code> image open, first close this image (by selecting 'Overlay' \u2192 'Remove') and then load the template.</p> <p>The image below depicts the various tools that you can use on FSLeyes, give them a go!</p> <p> </p> <p> We will now look at fMRI data. First close the previous image ('Overlay' \u2192 'Remove') and then load the fMRI image. To do this, click 'File' \u2192 'Add from file'  and then select the file <code>rds/projects/c/chechlmy-chbhmricn/xxx/CHBH/visualization2.5mm_2000_fMRI.nii.gz</code>.</p> <p>Your window should now look like this:</p> <p> </p> <p>Remember this fMRI data file is a 4D image \u2013 a set of 90-odd volumes representing a timeseries. To cycle through volumes, use the up/down buttons or type in a volume in the 'Volume' box to step through several volumes.</p> <p> </p> <p>Now try playing the 4D file in 'Movie' mode by clicking this button. You should see some slight head movement over time. Click the button again to stop the movie.</p> <p> </p> <p>As the fMRI data is 4D, this means that every voxel in the 3D-brain has a timecourse associated with it. Let's now have a look at this. </p> <p>Keeping the same dataset open (<code>2.5mm_2000_fMRI.nii.gz</code>) and now in the FSLeyes menu, select 'View' \u2192 'Time series'. </p> <p>FSLeyes should now look like the picture below. </p> <p> </p> <p>What exactly are we looking at?</p> <p>The functional image displayed here is the data straight from the scanner, i.e., raw, un-preprocessed data that has not been analyzed. In later workshops we will learn how to view analyzed data e.g., display statistical maps etc. </p> <p>You should see a timeseries shown at the bottom of the screen corresponding to the voxel that is selected in the main viewer. Move the mouse to select other voxels to investigate how variable the timecourse is. </p> <p>Within the timeseries window, hit the '+' button to show the 'Plot List' characteristics for this timeseries.</p> <p> </p> <p>Compare the timeseries in different parts of the brain, just outside the brain (skull and scalp), and in the airspace outside the skull. You should observe that these have very different mean intensities.</p> <p>The timeseries of multiple different voxels can be compared using the '+' button. Hit '+' and then select a new voxel. Characteristics of the timeseries such as plotting colour can also be changed using the buttons on the lower left of the interface.</p>"},{"location":"workshop2/visualizing-mri-data/#atlas-tools","title":"Atlas tools","text":"<p>FSL comes not only with a collection of\u00a0NIFTI standard templates but also with several built-in atlases, both probabilistic and histological (anatomical), comprising cortical, sub-cortical, and white matter parcellations.  You can explore the full list of included atlases here.</p> <p>We will now have a look at some of these atlases. </p> <p>Firstly, close all open files in FSLeyes (or close FSLeyes altogether and start it up again in your terminal by running <code>fsleyes &amp;</code>).</p> <p>In the FSLeyes menu, select 'File' \u2192 'Add Standard' and then choose the file called <code>MNI152_T1_2mm.nii.gz</code>  (this is a template brain in MNI space).</p> <p>The MNI152 atlas</p> <p>Remember that the MNI152 atlas is a standard brain template created by averaging 152 MRI scans of healthy adults widely used as a reference space in neuroimaging research.</p> <p>Now select from the menu 'Settings' \u2192 'Ortho View 1' and tick the box for 'Atlases' at the bottom.</p> <p> </p> <p>You should now see the 'Atlases' panel open as shown below.  </p> <p> </p> <p>The 'Atlases' panel is organized into three sections: </p> <ul> <li>Atlas information</li> <li>Atlas search</li> <li>Atlas management</li> </ul> <p>The 'Atlas information' tab provides information about the current display location, relative to one or more atlases selected in this tab. We will soon see how to use this information.</p> <p> </p> <p>The 'Atlas search' tab can be used to search for specific regions by browsing through the atlases. We will later look how to use this tab to create region-of-interest (ROI) masks. </p> <p> </p> <p>The 'Atlas management' tab can be used to add or delete atlases. This is an advanced feature, and we will not be using it during our workshops.</p> <p>We will now have a look at how to work with FSL atlases. First we need to choose some atlases to reference. In the 'Atlases' \u2192 'Atlas Information' window (bottom of screen in middle panel) make sure the following are ticked: </p> <ul> <li>Harvard-Oxford Cortical Structural Atlas</li> <li>Harvard-Oxford Subcortical Structural Atlas</li> <li>Juelich Histological Atlas</li> <li>Talairach Daemon Labels</li> </ul> <p>Now let's select a point in the standard brain. Move the cursor to the voxel position: [x=56, y=61, z=27] or enter the voxel location in the 'Location' window (2nd column). </p> <p>MNI Co-ordinate Equivalent</p> <p>Note that the equivalent MNI coordinates (shown in the 1st column/Location window) are [-22,-4,-18].</p> <p>It may not be immediately obvious what part of the brain you are looking at. Look at the 'Atlases' window. The report should say something like:</p> <pre><code>Harvard-Oxford Cortical Structural Atlas \nHarvard-Oxford Subcortical Structural Atlas \n98% Left Amygdala\n</code></pre> <p>Checking the brain region with other atlases</p> <p>What do the Juelich Histological Atlas &amp; Talairach Daemon Labels report?</p> <p>The Harvard-Oxford and Juelich are both probabilistic atlases. They report the percentage likelihood that the area named matches the point where the cursor is. </p> <p>The Talairach Atlas is a simpler labelling atlas. It is based on a single brain (of a 60-year-old French woman) and is an example of a deterministic atlas. it reports the name of the nearest label to the cursor coordinates. </p> <p>From the previous reports, particularly the Harvard-Oxford Subcortical Atlas and the Juelich Atlas, it should be obvious that we are most likely in the left amygdala. </p> <p>Now click the '(Show/Hide)' link after the Left Amygdala result (as shown below):</p> <p> </p> <p>This shows the (max) volume that the probabilistic Harvard-Oxford Subcortical Atlas has encoded for the Left Amygdala. The cursor is right in the middle of this volume.</p> <p> </p> <p>In the 'Overlay list' click and select the top amygdala overlay. You will note that the min/max ranges are set to 0 and 100. If it\u2019s not, change it to 0 and 100. These reflect the % likelihood of the labelling being correct.</p> <p> </p> <p>If you increase the min value from 0% to 50%, then you will see the size of the probability volume for the left amygdala will decrease. </p> <p>It now shows only the volume where there is a 50% or greater probability that this label is correct.</p> <p> </p> <p>Click the (Show/Hide) link after the Left Amygdala; the amygdala overlay will disappear.</p> <p>Exercise: Coordinate Localization</p> <p>Have a go at localizing exactly what the appropriate label is for these coordinates: </p> <ul> <li>Voxel coordinates [40, 51, 40] or MNI [10, -24, 8]</li> <li>Voxel coordinates [40, 51, 40] or MNI [10, -24, 8]</li> <li>Voxel coordinates [65, 29, 20] or MNI [-41, -68, -32]</li> </ul> <p>If unsure check your results with someone else, or ask for help! </p> <p>Make sure all overlays are closed (but keep the <code>MNI152_T1_2mm.nii.gz</code> open) before moving to the next section.</p>"},{"location":"workshop2/visualizing-mri-data/#using-atlas-tools-to-find-a-brain-structure","title":"Using atlas tools to find a brain structure","text":"<p>It is often helpful to locate where a specific structure is in the brain and to visually assess its size and extent.</p> <p>Let's suppose we want to visualize where Heschl's Gyrus is. In the bottom 'Atlases' window, click on the second tab ('Atlas search').</p> <p>In the Search box, start typing the word 'Heschl\u2026'. You should find that the system quickly locates an entry for Heschl's Gyrus in the Harvard-Oxford Cortical Atlas. Click on it to select.</p> <p>Now if you now the tick box immediately below next to the Heschl's Gyrus, an overlay will be added to the 'Overlay' list on the bottom (see below). Heschl's Gyrus should now be visible in the main image viewer. </p> <p>Now click on the '+' button next to the tick box. This will centre the viewing coordinates to be in the middle of the atlas volume (see below).</p> <p> </p> <p>Exercise: Atlas visualization</p> <p>Now try this for yourself: </p> <ul> <li>Remove the Heschl's Gyrus visualization. You can tick it off in the 'Atlases' window, or select Heschl's Gyrus in the 'Overlay list' window, and then either toggle its visibility off (click the eye icon) or remove it ('Menu' \u2192 'Overlay' \u2192 'Remove').</li> <li>Visualize the Lingual Gyrus and Left Hippocampus. To avoid confusion, change the colour of the Lingual Gyrus visualization from red/yellow to green and Left Hippocampus to blue.</li> </ul> <p>You can change the colour of the overlays by selecting the option below:</p> <p> </p> <p>Other options also exist to help you navigate the brain and recognize the different brain structures and their relative positions. </p> <p>Make sure you have firstly closed/removed all previous overlays. Now, select the 'Atlas Search' tab in the 'Atlases' window again.  This time, in the left panel listing different atlases, tick on the option for only one of the atlases, such as the Harvard-Oxford Cortical Structural Atlas, and make sure all others are unticked. </p> <p> </p> <p></p> <p>Now you should see all of the areas covered by the Harvard-Oxford cortical atlas shown on the standard brain. You can click around with the cursor, the labels for the different areas can be seen in the bottom right panel.</p> <p> </p> <p></p> <p>In addition to atlases covering various grey matter structures, there are also two white matter atlases: the JHU ICBM-DTI-81 white-matter labels atlas &amp; JHU white-matter tractography atlas.  If you tick (select) these atlases as per previous instructions (hint using the 'Atlas search' tab), you will see a list of all included white matter tracts (pathways) as shown below:</p> <p> </p>"},{"location":"workshop2/visualizing-mri-data/#using-atlas-tools-to-create-a-region-of-interest-mask","title":"Using atlas tools to create a region-of-interest mask","text":"<p>You can also use atlas tools in FSLeyes to not only locate specific brain structures but also to create masks for ROI (region-of-interest) analysis. We will now create ROI masks (one grey matter mask and one white matter) using FSL tools and built-in atlases.</p> <p>To start, please close 'FSLeyes' entirely, either by clicking 'x' in the right corner of the FSLeyes window or by selecting 'FSLeyes' \u2192 'Close'. Then close your current terminal and open a new terminal window. </p> <p>Then do the following:</p> <ul> <li>Navigate to your project directory and make a new directory called <code>ROImasks</code>. Navigate into this directory. </li> <li>Then load <code>fsl</code> and open FSLeyes in the background.</li> </ul> <p>Here are the commands to do this:</p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/\nmkdir ROImasks\ncd ROImasks\nmodule load FSL/6.0.5.1-foss-2021a-fslpython \nmodule load FSLeyes/1.3.3-foss-2021a\nfsleyes &amp; \n</code></pre> <p>Wait for FSLeyes to load, then:</p> <ul> <li>Load the MNI template by clicking 'File' \u2192 'Add Standard' \u2192 'MNI152_T1_2mm' and open the 'Atlases' panel. </li> <li>Go to the 'Atlas search' tab, select the 'Harvard Oxford Cortical Atlas' and tick the 'Middle Frontal Gyrus' (from the list below 'Search' box) to add overlay to the 'Overlay list'. </li> <li>Select the 'Middle Frontal Gyrus' (<code>harvardoxford-cortical_prob_Middle_Frontal_Gyrus</code>) from the 'Overlay' list and save it in your <code>ROImasks</code> directory as <code>MFG</code> (select 'Overlay' \u2192 'Save' \u2192 Name: MFG).</li> </ul> <p>You should now see the MFG overlay in the overlay list (as below) and have a <code>MFG.nii.gz</code> file in the <code>ROImasks</code> directory. You can check this by typing <code>ls</code> in the terminal.</p> <p> </p> <p>We will now create a white matter mask. Here are the steps:</p> <ul> <li>First, remove the MFG overlay ('Menu' \u2192 'Overlay' \u2192 'Remove').</li> <li>Then go to the 'Atlas search' tab, and select the 'JHU white-matter tractography' atlas and tick the 'Forceps minor' (from the list below 'Search' box) to add overlay to the 'Overlay' list. </li> <li>Finally, select the added 'Forceps minor' overlay from the 'Overlay' list and save it in your <code>ROImasks</code> directory as FM ('Overlay' \u2192 'Save' \u2192 Name: FM). </li> </ul> <p>You should now see the FM overlay in the overlay list (as below) and also have a <code>FM.nii.gz</code> file in the <code>ROImasks</code> directory. </p> <p> </p> <p>You now have two \u201cprobabilistic ROI masks\u201d. To use these masks for various analyses, you need to first binarize these images.</p> <p>Why binarize?</p> <p>Why do you think we need to binarize the mask first? There are several reasons, but primarily it creates clear boundaries between regions which simplifies our statistical analysis and reduces computation.</p> <p>To do this, first close FSLeyes. Make sure that you are in the <code>ROImasks</code> directory and check if you have the two masks.  If you type <code>pwd</code> in the terminal, you should get the output <code>rds/projects/c/chechlmy-chbh-mricn/xxx/ROImasks</code> (where XXX=your ADF username) and when you type <code>ls</code>, you should see <code>FM.nii.gz</code> and <code>MFG.nii.gz</code>.</p> <p>To binarize the masks, you can use one of the FSL tools for image manipulation, <code>fslmaths</code>. The basic structure of an <code>fslmaths</code> command is: </p> <p><code>fslmaths input image [modifiers/options] output</code></p> <p>Type in your terminal:</p> <pre><code>fslmaths FM.nii.gz -bin FM_binary\nfslmaths MFG.nii.gz -bin MFG_binary\n</code></pre> <p>This simply takes your ROI mask, binarizes it and saves the binarized mask with the <code>_binary</code> name.</p> <p>You should now have 4 files in the ROImasks directory.</p> <p>Now open FSLeyes and examine one of the binary masks you just created. First load a template (Click 'File' \u2192 'Add Standard' \u2192 'MNI152_T1_2mm') and add the binary mask (e.g., Click 'File' \u2192 'Add from file' \u2192 'FM_binary.nii.gz'). </p> <p>You can see the difference between the probabilistic and binarized ROI masks below:</p> <p>Probabilistic ROI mask</p> <p>Binary ROI mask</p> <p>To use ROI masks in your analysis, you might also need to threshold it i.e., to change/restrict the probability of the volume. We previously did this for the amygdala manually (e.g., from 0-100% to 50%-100%).  The choice of the threshold might depend on the type of analysis and the type of ROI mask you need to use. The instructions below explain how to threshold and binarize your ROI image in one single step using <code>fslmaths</code>.</p> <p>Open your terminal and make sure that you are in the <code>ROImasks</code> directory (<code>pwd</code>). To both threshold and binarize the MFG mask, type:</p> <p><code>fslmaths MFG.nii.gz -thr 25 -bin MFGthr_binary</code></p> <p>(option <code>-thr</code> is used to threshold the image below a specific number, in this case 25 corresponding to 25% probability)</p> <p>Now let's compare the thresholded and unthresholded MFG binarized masks.</p> <ul> <li>Go back to FSLeyes and add the unthresholded MFG binary mask (e.g., Click 'File' \u2192 'Add from file' \u2192 'MFG_binary.nii.gz'). </li> <li>Add the second, thresholded and binarized MFG mask (<code>MFGthr_binary.nii.gz</code>), and to avoid confusion, change the colour of the second mask to blue. You can either toggle its visibility on and off (click the eye icon) to compare mask or use the 'Opacity' button. </li> </ul> <p>You can see the difference in size between the two below:</p> <p>Binarized MFG mask</p> <p>Binarized and thresholded MFG mask</p> <p>Exercise: Atlases and masks</p> <p>Have a go at the following exercises:</p> <ul> <li>Explore different atlases to localize various cortical, subcortical or white matter structures (take inspiration from your MSc project, recent papers or seminars)</li> <li>Using atlas tools, create binary (un-thresholded) masks for the left and right Superior longitudinal fasciculus (hint: use JHU white-matter tractography atlas)</li> <li>Using atlas tools, create binary and thresholded (at different levels of probability 5, 25 and 75%) masks for the right thalamus (hint: use Harvard-Oxford Subcortical Structural Atlas)</li> </ul> <p>If unsure, check your results with someone else or ask for help!</p> <p>Workshop 2: Further Reading and Reference Material</p> <p>FSLeyes is not the only MRI visualization tool available. Here are some others:</p> <ul> <li>fslview (older version of FSL viewer) </li> <li>MRIcroGL (for high quality images)</li> <li>AFNI - Analysis of Functional NeuroImages</li> <li>Mango/Papaya</li> </ul> <p>More details of what is available on BEAR at the CHBH can be found at the BEAR Technical Docs website.</p>"},{"location":"workshop2/workshop2-intro/","title":"Workshop 2 - MRI data formats, data visualization and atlas tools","text":"<p>Welcome to the second workshop of the MRICN course! Prior lectures introduced you to the basics of the physics and technology behind MRI data acquisition.  In this workshop we will explore, MRI image fundamentals, MRI data formats, data visualization and atlas tools. </p> <p>Overview of Workshop 2</p> <p>Topics for this workshop include:</p> <ul> <li>The fundamentals of MRI data, including file types and formats</li> <li>Converting between different MRI data files (e.g., DICOM to NIFTI)</li> <li>Introduction to FSLeyes and basic navigation</li> <li>Loading atlases and creating regions-of-interest (ROIs)</li> <li>Binarizing and thresholding ROIs</li> </ul> <p>You will need this information before you can analyse data, regardless if using structural or functional MRI data.</p> <p>For the purpose of the module we will be using BlueBEAR. You should remember from Workshop 1, how to access the BlueBEAR Portal and use the BlueBEAR GUI. </p> <p> </p> <p>You have already been given access to the RDS project, <code>rds/projects/c/chechlmy-chbh-mricn</code>. Inside the module\u2019s RDS project, you will find that you have a folder labelled <code>xxx</code> (<code>xxx</code> = University of Birmingham ADF username). </p> <p>If you navigate to that folder <code>(rds/projects/c/chechlmy-chbh-mricn/xxx)</code>, you will be able to perform the various file operations from there during workshops.</p> <p>The copy of this workshop notes can be found on Canvas 39058 - LM Magnetic Resonance Imaging in Cognitive Neuroscience in Week 02 workshop materials.</p>"},{"location":"workshop3/diffusion-intro/","title":"Diffusion MRI basics - visualization and preprocessing","text":"<p>In this workshop and the workshop next week, we will follow some basic steps in the diffusion MRI analysis pipeline below.  The instructions here are specific to tools available in FSL, however other neuroimaging software packages can be used to perform similar analyses.  You might also recall from lectures that models other than diffusion tensor and methods other than probabilistic tractography are also often used. </p> FSL diffusion MRI analysis pipeline <p> </p> <p></p> <p>First, if you have not already, log in into the BlueBEAR Portal and start a BlueBEAR GUI session (2 hours). You should know how to do it from the previous workshops.  Open a new terminal window and navigate to your MRICN project folder:</p> <p><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx</code> [where XXX=your ADF username] </p> <p>Please check your directory by typing <code>pwd</code>. This should return: <code>/rds/projects/c/chechlmy-chbh-mricn/xxx</code>.</p> <p>Where has all my data gone?</p> <p>Before this workshop, any old directories and files from previous workshops have been removed (you will not need it for subsequent workshops and storing unnecessary data would result in exceeding allocated quota).  Your XXX directory should therefore be empty. </p> <p>Next you need to copy over the data for this workshop.</p> <p><code>cp -r /rds/projects/c/chechlmy-chbh-mricn/module_data/diffusionMRI/ .</code>    (make sure you do not omit spaces and .)</p> <p>This might take a while, but once it has completed, change into that downloaded directory:</p> <p><code>cd diffusionMRI</code> (your <code>XXX</code> subdirectory you should now have the folder <code>diffusionMRI</code>)</p> <p>Type <code>ls</code>. You should now see three subdirectories/folders (<code>DTIfit</code>, <code>TBSS</code> and <code>tractography</code>). Change into the <code>DTIfit</code> folder:</p> <p><code>cd DTIfit</code></p>"},{"location":"workshop3/diffusion-intro/#viewing-diffusion-data-using-fsleyes","title":"Viewing diffusion data using FSLeyes","text":"<p>We will first look at what diffusion images look like and explore text files which contain information about gradient strength and gradient directions.</p> <p>In your terminal type <code>ls</code>. This should return:</p> <pre><code>p01/\np02/\n</code></pre> <p>So, the folder <code>DTIfit</code> contains data from two participants contained within the <code>p01</code> and <code>p02</code> folders. </p> <p>Inside each folder (<code>p01</code> and <code>p02</code>) you will find a T1 scan, uncorrected diffusion data (<code>blip_up.nii.gz</code>, <code>blip_down.nii.gz</code>) acquired with two opposing PE-directions (<code>AP/blip_up</code> and <code>PA/blip_down</code>) and corresponding <code>bvals</code> (e.g., <code>blip_up.bval</code>) and <code>bvecs</code> (e.g., <code>blip_up.bvec</code>) files. </p> <ul> <li>The <code>bvals</code> files contain b-values (scalar values for each applied gradient). </li> <li>The <code>bvecs</code> files contain a list of gradient directions (diffusion encoding directions), including a [3x1] vector for each gradient. </li> </ul> <p>The number of entries in <code>bvals</code> and <code>bvecs</code> files equals the number of volumes in the diffusion data files. </p> <p>Finally, inside <code>p01</code> and <code>p02</code> there is also subdirectory data with distortion-corrected diffusion images.</p> <p>We will start with viewing the uncorrected data. Please navigate inside the <code>p01</code> folder, open FSLeyes and then load one of the uncorrected diffusion images:</p> <pre><code>cd p01\nmodule load FSL/6.0.5.1-foss-2021a-fslpython\nmodule load FSLeyes/1.3.3-foss-2021a\nfsleyes &amp;\n</code></pre> <p>The image you have loaded is 4D and consists of 64 volumes acquired with different diffusion encoding directions. Some of the volumes are non-diffusion images (b-value = 0), while most are diffusion weighted images.  The first volume, which you can see after loading the file, is a non-diffusion weighted image as demonstrated below. </p> <p> </p> <p>Viewing separate volumes</p> <p>You can view the separate volumes by changing the number in the Volume box or playing movie mode. Note that the volume count starts from 0.  You should also note that there are significant differences in the image intensity between different volumes. </p> <p>Now go back to volume 0 and - if needed - stop movie mode. In the non-diffusion weighted image, the ventricles containing CSF are bright and the rest of the image is relatively dark.  Now change the volume number to 2, which is a diffusion weighted image (with a b-value of approximately 1500). </p> <p>The intensity of this volume is different. To see anything, please change max. intensity to 400. Now the ventricles are dark and you can see some contrast between different voxels. </p> <p> </p> <p>Let's view the content of the <code>bvals</code> and <code>bvecs</code> files by using the <code>cat</code> command. In your terminal type:</p> <p><code>cat blip_down.bval</code></p> <p> </p> <p>The first number is 0. This indicates that indeed the first volume (volume 0) is a non-diffusion weighted image and the third volume (volume 2) is diffusion weighted volume with b=1500.  Based on the content of this <code>bval</code> file, you should be able to tell how many diffusion-weighted volumes were acquired and how many without any diffusion weighting (b0 volumes). </p> <p>Comparing diffusion-weighted volumes</p> <p>Please compare this with the file you loaded into FSLeyes. </p> <p>Now type:</p> <p><code>cat blip_down.bvec</code></p> <p> </p> <p>You should now see 3 separate rows of numbers representing the diffusion encoding directions (3x1 vector for each acquired volume; x,y,z directions) and that for volume 2 the diffusion encoding is represented by the vector [0.578, 0.671, 0.464]. </p> Distortion correction <p>As explained in the lectures, diffusion imaging suffers from various distortions (susceptibility, eddy-currents and movement induced distortions). These need to be corrected before further analysis.  The most most noticeable geometric distortions are susceptibility-induced distortions caused by field inhomogeneities, and so we will have a closer look at these.</p> <p>All types of distortions need correction during pre-processing steps in diffusion imaging analysis. FSL includes two tools used for distortion correction, topup and eddy.  The processing with these two tools is time and computing intensive. Therefore we will not run the distortion correction steps in the workshop but instead explore some principles behind it. </p> <p> </p> <p>For this, you are given distortion corrected data to conduct further analysis, diffusion tensor fitting and probabilistic tractography.  </p> <p>First close the current image in FSLeyes ('Overlay' \u2192 'Remove') and load both uncorrected images (<code>blip_up.nii.gz</code>, <code>blip_down.nii.gz</code>) acquired with two opposing PE-directions (PE=phase encoding). </p> <p>Compare the first volumes in each file. To do that you can either toggle the visibility on and off (click the eye icon) or use the 'Opacity' button (you should remember from the previous workshop how to do this). </p> <p> </p> <p>The circled area indicates the differences in susceptibility-induced distortions between the two images acquired with two opposing PE-directions. </p> <p>Now change the max. intensity to 400 and compare the third volumes in each file. Again, the circled area indicate the differences in distortions between the two images acquired with the two opposing PE-directions. </p> <p> </p> <p>Finally, we will look at distortion corrected data. First close the current image ('Overlay' \u2192 'Remove').</p> <p>Now in FSLeyes load <code>data.nii.gz</code> (the distortion-corrected diffusion image located inside the data subdirectory) and have a look at one of the the non-diffusion weighted and diffusion-weighted volumes. </p> <p> </p> <p>Comparing corrected to uncorrected diffusion-weighted volumes</p> <p>Can you tell the difference in the corrected compared to the uncorrected diffusion images?</p> <p>Further examining the difference between uncorrected and corrected diffusion data</p> <p>In your own time (outside of this workshop as part of independent study), load both the corrected and uncorrected data for <code>p01</code> and compare using the 'Volume' box or 'Movie' mode. Also explore the data in <code>p02</code> folder using the instructions above. </p>"},{"location":"workshop3/diffusion-intro/#creating-a-binary-mask-using-fsls-brain-extraction-tool","title":"Creating a binary mask using FSL's Brain Extraction Tool","text":"<p>In the next part of the workshop, we will look FSL's Brain Extraction Tool (BET).</p> <p>Brain extraction is a necessary pre-processing step, which removes non-brain tissue from the image. It is applied to structural images prior to tissue segmentation and is needed to prepare anatomical scans for registration of functional MRI or diffusion scans to MNI space. BET can be also used to create binary brain masks (e.g., brain masks needed to run diffusion tensor fitting, DTIfit). </p> <p>In this workshop we will look at only at creating a binary brain mask as required for DTIfit. In subsequent workshops we will look at using BET for removing non-brain tissues from diffusion and T1 scans (\u201cskull-stripping\u201d) in preparation for registration.</p> <p>First close FSLeyes and to make sure you do not have any processes running in the background, close your current terminal.</p> <p>Open a new terminal window, navigate to the <code>p02</code> subdirectory, and load FSL and FSLeyes again:</p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/diffusionMRI/DTIfit/p02\nmodule load FSL/6.0.5.1-foss-2021a-fslpython\nmodule load FSLeyes/1.3.3-foss-2021a \n</code></pre> <p>Now check the content of the <code>p02</code> subdirectory by typing <code>ls</code>. You should get the response <code>bvals</code>, <code>bvecs</code> and <code>data.nii.gz</code>.</p> <p>From the <code>data.nii.gz</code> (distortion corrected diffusion 4D image) we will extract a single volume without diffusion weighting (e.g. the first volume). You can extract it using one of FSL's utility commands, <code>fslroi</code>. </p> <p>What is <code>fslroi</code> used for?</p> <p><code>fslroi</code>is used to extract a region of interest (ROI) or subset of data from a larger 3D or 4D image file.</p> <p>In the terminal, type:</p> <p><code>fslroi data.nii.gz nodif 0 1</code></p> <p>where: </p> <ul> <li><code>data.nii.gz</code> is your input image, </li> <li><code>nodif</code> is your output image (3D non-diffusion weighted volume), </li> <li>0 and 1 indicate that you are extracting volume 0 and the you only want to extract one (1) volume</li> </ul> <p>You should have a new file <code>nodif.nii.gz</code> (type <code>ls</code> to confirm) and can now create a binary brain mask using BET.</p> <p>To do this, first open BET in terminal. You can open the BET GUI directly in a terminal window by typing:</p> <p><code>Bet &amp;</code></p> <p>Or by runnning FSL in a terminal window and accessing BET from the FSL GUI. To do it this way, type:</p> <p><code>fsl &amp;</code></p> <p>and then open the 'BET brain extraction tool' by clicking on it in the GUI.</p> <p>In either case, once BET is opened, click on advanced options and make sure the first two outputs are selected ('brain extracted image' and 'binary brain mask') as below. Select as the 'Input' image the previously created <code>nodif.nii.gz</code> and change 'Fractional Intensity Threshold' to 0.4. Then click the 'Go' button. </p> <p> </p> <p> </p> <p>Completing BET in the terminal</p> <p>After running BET you may need to hit return to get a visible prompt back after seeing \"Finished\u201d in the terminal!</p> <p>You will see 'Finished' in the terminal when you are ready to inspect the results. Close BET and open FSLeyes and load three files (<code>nodif.nii.gz</code>, <code>nodif_brain.nii.gz</code> and <code>nodif_brain_mask</code>). Compare the files. To do that you can either toggle the visibility on and off (click the eye icon) or use 'Opacity button' (you should remember from previous workshop how to do it).</p> <p>The <code>nodif_brain_mask</code> is a single binarized image with ones inside the brain and zeroes outside the brain. You need this image both for DTIfit and tractography.</p> <p>Comparing between BET and normal images</p> <p>Can you tell the difference between <code>nodif.nii.gz</code> and <code>nodif_brain.nii.gz</code>? It might be easier to compare these images if you change max intensity to 1500 and <code>nodif_brain</code> colour to green.</p>"},{"location":"workshop3/diffusion-mri-analysis/","title":"Diffusion tensor fitting and Tract-Based Spatial Statistics","text":""},{"location":"workshop3/diffusion-mri-analysis/#diffusion-tensor-fitting-dtifit","title":"Diffusion tensor fitting (DTIfit)","text":"<p>The next thing we will do is to look at how to run and examine the results of diffusion tensor fitting. </p> <p>First close FSLeyes, and to make sure you do not have any processes running in the background, close the current terminal.</p> <p>Open a new terminal window, navigate to the <code>p01</code> subdirectory, load FSL and FSLeyes again, and finally open FSL (with &amp; to background it):</p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/diffusionMRI/DTIfit/p01\nmodule load FSL/6.0.5.1-foss-2021a-fslpython\nmodule load FSLeyes/1.3.3-foss-2021a\nfsl &amp; \n</code></pre> <p>To run the diffusion tensor fit, you need 4 files as specified below:</p> <ol> <li>Distortion corrected diffusion data: <code>data.nii.gz</code></li> <li>Binary brain mask: <code>nodif_brain_mask.nii.gz</code></li> <li>Gradient directions: <code>bvecs</code> (test file with gradient directions)</li> <li>b-values: <code>bvals</code> (text file with list of b-values)</li> </ol> <p>All these files are included inside the data subdirectory <code>p01/data</code>. You will later learn how to create a binary brain mask but first we will run DTIfit.</p> <p>In the FSL GUI, first click on 'FDT diffusion', and in the FDT window, select 'DTIFIT Reconstruct diffusion tensors'. Now choose as 'Input directory' the <code>data</code> subdirectory located inside <code>p01</code> and click 'Go'. </p> <p> </p> <p>You should see something happening in the terminal and once you see 'Done!' you are ready to view the results.</p> <p> </p> <p>Click 'OK' when the message appears.</p> Different ways of running DTIfit <p>Instead of running DTIfit by choosing the 'Input' directory, you can also run it by specifying the input file manually. If you click it now, the files would be auto-filled but otherwise you would need to provide inputs as below.       </p> <p> </p> <p></p> <p>Running DTIfit in your own time</p> <p>Please do NOT run it now, but instead try it in your own time with data in the <code>p02</code> folder.</p> <p>Finally, you can also run DTIfit directly from the terminal. To do this, you would need to type <code>dtifit</code> in the terminal and choose the <code>dtifit</code> compulsory arguments:</p> Argument Description -k, --data dti data file -o, --out Output basename -m, --mask Bet binary mask file -r, --bvecs b vectors file -b, --bvals b values file <p>To run DTIfit from the terminal, you would need to navigate inside the <code>subdirectory/folder</code> with all the data and type the full <code>dtifit</code> command, specifying compulsory arguments as below:</p> <p><code>dtifit --data=data --mask=nodif_brain_mask --bvecs=bvecs --bvals=bvals --out=dti</code></p> <p>This command only works when running it from inside a folder where all the data is located, otherwise you will need to specify the full path with the data location.  This would be useful if you want to write a script; we will look at it in the later workshops.</p> <p>Running DTIfit from the terminal in your own time</p> <p>Again, please do NOT run it now but try it in your own time with data in the <code>p02</code> folder.</p> <p>The results of running DTIfit are several output files as specified below. We will look closer at the highlighted files in bold.  All of these files should be located in the <code>data</code> subdirectory, i.e. within <code>/rds/projects/c/chechlmy-chbh-mricn/xxx/diffusionMRI/DTIfit/p01/data/</code>.</p> Output File Description dti_V1 (V2, V3) 1st, 2nd, 3rd eigenvectors dti_L1 (L2, L3) 1st, 2nd, 3rd eigenvalues dti_FA Fractional Anisotropy map dti_MD Mean Diffusivity map dti_MO Mode of anisotropy (linear versus planar) dti_SO Raw T2 signal with no diffusion weighting <p>To do this, firstly close the FSL GUI, open FSLeyes and load the FA map ('File' \u2192 'Add from file' \u2192 <code>dti_FA</code>)</p> <p> </p> <p>Next add the principal eigenvector map (<code>dti_V1</code>) to your display ('File' \u2192 'Add from file' \u2192 <code>dti_V1</code>).</p> <p>FSLeyes will open the image <code>dti_V1</code> as a 3-direction vector image (RGB) with diffusion direction coded by colour.  To display the standard PDD colour coded orientation map (as below), you need to modulate the colour intensity with the FA map so that the anisotropic voxels appear bright. </p> <p>In the display panel (click on 'Settings' (the Cog icon)) and change 'Modulate' by setting it to <code>dti_FA</code>.</p> <p> </p> <p>Finally, compare the FA and MD maps (<code>dti_FA</code> and <code>dti_MD</code>). To do this, load the FA map and add the MD map.  By contrast to the FA map, the MD map appears uniform in both gray and white matter, plus higher intensities are in the CSF-filled ventricles and indicate higher diffusivity. This is opposed to dark ventricles in the FA map. </p> <p>Differences between the FA and MD maps</p> <p>Why are there such differences?</p>"},{"location":"workshop3/diffusion-mri-analysis/#tract-based-spatial-statistics-tbss","title":"Tract-Based Spatial Statistics (TBSS)Tract-Based Spatial Statistics analysis pipeline","text":"<p>In the next part of the workshop, we will look at running TBSS, Tract-Based Spatial Statistics.</p> <p>TBSS is used for a whole brain \u201cvoxelwise\u201d cross-subject analysis of diffusion-derived measures, usually FA (fractional anisotropy).</p> <p>We will look at an example TBSS analysis of a small dataset consisting of FA maps from ten younger (y1-y10) and five older (o1-o5) participants.  Specifically, you will learn how to run the second stage of TBSS analysis, \u201cvoxelwise\u201d statistics, and learn how to display results using FSLeyes.  The statistical analysis that you will run aims to examine where on the tract skeleton younger versus older (two groups) participants have significantly different FA values.</p> <p>Before that, let's shortly recap TBSS as it was covered in the lecture. </p> <p>The steps for Tract-Based Spatial Statistics are:</p> <ol> <li>Fitting the diffusion tensor (DTIfit)</li> <li>Alignment of  all study participants\u2019 FA maps to standard space using non-linear registration</li> <li>Merging all participants\u2019 nonlinearly aligned FA maps into a single 4D image file and creating the mean FA image </li> <li>FA \u201cskeletonization\u201d (the mean FA skeleton representing the centres of major tracts specific to all participants is created)</li> <li>Each participant\u2019s aligned FA map is then projected back onto the skeleton prior to statistical analysis </li> <li>Hypothesis testing (voxelwise statistics)</li> </ol> <p>To save time, some of the pre-processing stages including generating FA maps (tensor fitting), preparing data for analysis, registration of FA maps and skeletonization have been run for you and all outputs are included in the <code>data</code> folder you have copied at the start of this workshop. </p> <p> </p> <p>You will only run the TBSS statistical analysis to explore group differences in FA values based upon age (younger versus older participants).</p> <p>First close FSLeyes (if you still have it open) and make sure that you do not have any processes running in the background by closing your current terminal.</p> <p>Then open a new terminal window, navigate to the subdirectory where pre-processed data are located and load both FSL and FSLeyes:</p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/diffusionMRI/TBSS/TBSS_analysis_p2/\nmodule load FSL/6.0.5.1-foss-2021a-fslpython\nmodule load FSLeyes/1.3.3-foss-2021a \n</code></pre> <p>Once you have loaded all the required software, we will start with exploring the pre-processed data. If you correctly followed the previous steps, you should be inside the subdirectory <code>TBSS_analysis_p2</code>.  Confirm that, and then check the content of that subdirectory by typing:</p> <p><code>pwd</code> (answer <code>/rds/projects/c/chechlmy-chbh-mricn/xxx/diffusionMRI/TBSS/TBSS_analysis_p2/</code>)</p> <p><code>ls</code> (you should see 3 data folders listed: <code>FA</code>, <code>origdata</code>, <code>stats</code>)</p> <p>We need to firstly check if all the pre-processing steps have been run correctly and that we have all the required files. </p> <p>Navigate inside the <code>stats</code> folder and check the files inside by typing in your terminal:</p> <pre><code>cd stats\nls\n</code></pre> <p>You should find inside the files listed below. </p> <ul> <li><code>all_FA</code> (4D image file with all participants\u2019 FA maps registered into standard space)</li> <li><code>mean_FA</code> (3D image file mean of all participants FA maps)</li> <li><code>all_FA_skeletonised</code> (4D image file with all participants skeletonised FA data)</li> <li><code>mean_FA_skeleton</code> (3D image file mean FA skeleton)</li> </ul> <p>Exploring the data</p> <p>If this is the case, open FSLeyes and explore these files one by one to make sure you understand what each represents. You might need to change the colour to visualise some image files. </p> <p>Remember to ask for help!</p> <p>If you are unsure about something, or need help, please ask!</p> <p>Once you have finished taking a look, close FSLeyes.</p> <p>Before using the General Linear Model (GLM) GUI to set up the statistical model, you need to determine the order in which participants\u2019 files have been entered into the single 4D skeletonized file (i.e., the data order in the <code>all_FA_skeletonised</code> file).  The easiest way to determine the alphabetical order of participants in the the final 4D file (<code>all_FA_skeletonised</code>), is to check in which order FSL lists the pre-processed FA maps inside the FA folder. You can do this in the terminal with the commands below</p> <pre><code>cd .. \ncd FA \nimglob *_FA.*\n</code></pre> <p>You should see data from the 5 older (o1-o5) followed by data fromthe  10 (y1-y10) younger participants.</p> <p>Next navigate back to the <code>stats</code> folder and open FSL:</p> <pre><code>cd ..\ncd stats\nfsl &amp;\n</code></pre> <p>Click on 'Miscellaneous tools' and select 'GLM Setup' to open the GLM GUI. </p> <p> </p> <p>In the workshop we will set up a simple group analysis (a two sample unpaired t-test).</p> <p>How to set up more complex models</p> <p>To find information re how to set up more complex models, using GUI, click on this link: https://fsl.fmrib.ox.ac.uk/fsl/docs/#/statistics/glm</p> <p>In the 'GLM Setup' window, change 'Timeseries design' to 'Higher-level/non-timeseries design' and '# inputs' to 15.</p> <p>Then click on 'Wizard' and select 'two groups, unpaired' and set 'Number of subjects in first group' to 5. Then click 'Process'.</p> <p> </p> <p>In the 'EVs' tab, name 'EV1' and 'EV2' as per your groups (old, young).</p> <p>In the contrast window set number of contrasts to 2 and re-name them accordingly to the image below: </p> <p>(C1: old &gt; young, [1 -1]) (C2: young &gt; old, [-1 1])</p> <p> </p> <p>Click 'View Design', close the image and then go back to the GLM set window and save your design with the filename <code>design</code>. Click 'Exit' and close FSL.</p> <p>To run the TBSS statistical analysis FSL's <code>randomise</code> tool is used.</p> <p>FSL's randomise</p> <p>Randomise is FSL's tool for nonparametric permutation inference on various types of neuroimaging data (statistical analysis tool). For more information click on this link: https://fsl.fmrib.ox.ac.uk/fsl/docs/#/statistics/randomise</p> <p>The basic command line to use this tool is:</p> <p><code>randomise -i &lt;input&gt; -o &lt;input&gt; -d &lt;design.mat&gt; -t &lt;design.con&gt; [options]</code></p> <p>You can explore options and the set up by typing <code>randomise</code> in your terminal.</p> <p>The basic command line to use randomise for TBSS is below:</p> <p><code>randomise -i all_FA_skeletonised -o tbss -m mean_FA_skeleton_mask -d design.mat -t design.con -n 500 --T2</code></p> <p>Check if you are inside the <code>stats</code> folder and run the command above in terminal to run your TBSS group analysis:</p> <p>The elements of this command are explained below:</p> Argument Description -i input image -o output image basename -m mask -d design matrix -t design contrast -n number of permutations --T2 TFCE <p>Why so few permutations?</p> <p>To save time we only run 500 permutations; this number will vary depending on the type of analysis, but usually it is between 5,000 to 10,000 or higher.</p> <p>The output from <code>randomise</code> will include two raw (unthresholded) tstat images, <code>tbss_tstat1</code> and <code>tbss_tstat2</code>. </p> <p>The TFCE p-value images (fully corrected for multiple comparisons across space) will be <code>tbss_tfce_corrp_tstat1</code> and <code>tbss_tfce_corrp_tstat2</code>.</p> <p>Based on the set up of your design, contrast 1 gives the older &gt; young test and contrast 2 gives the young &gt; older test; the contrast which will likely give significant results is the 2nd contrast i.e., we are expecting higher FA in younger participants (due to the age related decline in FA).</p> <p>To check that, use FSLeyes to view results of your TBSS analysis. Open FSLeyes, load <code>mean_FA</code> plus the <code>mean_FA_skeleton</code> template and add your display TFCE corrected stats-2 image:</p> <ol> <li>'File' -&gt; 'Add from file' -&gt; <code>mean_FA.nii.gz</code></li> <li>File -&gt; 'Add from file' -&gt; <code>mean_FA_skeleton.nii.gz</code> (change greyscale to green)</li> <li>File -&gt; 'Add from file' -&gt; <code>tbss_tfce_corrp_tstat2.nii.gz</code> (change greyscale to red-yellow and set up Max to 1, and Min to 0.95 or 0.99)</li> </ol> <p>Please note that TFCE-corrected images, are actually 1-p for convenience of display, so thresholding at 0.95 gives significant clusters at p corrected &lt; 0.05, and 0.99 gives significant clusters at p corrected &lt; 0.01.</p> <p>You should see the same results as below:</p> <p> </p> <p>Interpreting the results</p> <p>Are the results as expected? Why/why not?</p> <p>Reviewing the tstat1 image</p> <p>Next review the <code>tbss_tfce_corrp_tstat1.nii.gz</code> </p> <p>Further information on TBSS</p> <p>More information on TBSS, can be found on the 'TBSS' section of the FSL Wiki: https://fsl.fmrib.ox.ac.uk/fsl/docs/#/diffusion/tbss</p>"},{"location":"workshop3/workshop3-intro/","title":"Workshop 3 - Basic diffusion MRI analysis","text":"<p>Welcome to the third workshop of the MRICN course! Prior lectures in the module introduced you to basics of the diffusion MRI and its applications, including data acquisition, the theory behind diffusion tensor imaging and using tractography to study structural connectivity. The aim of the next two workshops is to introduce you to some of the core FSL tools used for diffusion MRI analysis. </p> <p>Specifically, we will explore different elements of the FMRIB's Diffusion Toolbox (FDT) to walk you through basic steps in diffusion MRI analysis. We will also cover the use of Brain Extraction Tool (BET). </p> <p>By the end of the two workshops, you should be able to understand the principles of correcting for distortions in diffusion MRI data, how to run and explore results of a diffusion tensor fit, and how to run a whole brain group analysis and probabilistic tractography. </p> <p>Overview of Workshop 3</p> <p>Topics for this workshop include:</p> <ul> <li>Visualizing diffusion data using FSLeyes (before and after distortion correction)</li> <li>Using FSL's Brain Extraction Tool (BET) to create a brain mask</li> <li>Understand and perform diffusion tensor fitting (DTIfit) to generate key diffusion metrics like FA (Fractional Anisotropy) and MD (Mean Diffusivity)</li> <li>Learn to conduct Tract-Based Spatial Statistics (TBSS) for group-level comparisons of diffusion data</li> </ul> <p>We will be working with various previously acquired datasets (similar to the data acquired during the CHBH MRI Demonstration/Site visit). We will not go into details as to why and how specific sequence parameters and specific values of the default settings have been chosen. Some values should be clear to you from the lectures or assigned on Canvas readings, please check there, or if you are still unclear, feel free to ask. </p> <p>Note that for your own projects, you are very likely to want to change some of these settings/parameters depending on your study aims and design. </p> <p>The copy of this workshop notes can be found on Canvas 39058 - LM Magnetic Resonance Imaging in Cognitive Neuroscience in Week 03 workshop materials.</p>"},{"location":"workshop4/probabilistic-tractography/","title":"Probabilistic Tractography","text":"<p>In the first part of the workshop, we will look again at BET, FSL's Brain Extraction Tool.</p> <p>Brain extraction is a necessary pre-processing step which allows us to remove non-brain tissue from the image.  It is applied to structural images prior to tissue segmentation and is needed to prepare anatomical scans for the registration of functional MRI or diffusion scans to MNI space.  BET can be also used to create binary brain masks (e.g., brain masks needed to run diffusion tensor fitting, DTIfit). </p>"},{"location":"workshop4/probabilistic-tractography/#skull-stripping-our-data-using-bet","title":"Skull-stripping our data using BET","text":"<p>In this workshop we will first look at a very simple example of removing non-brain tissues from diffusion and T1 scans (\u201cskull-stripping\u201d) in preparation for the registration of diffusion data to MNI space.</p> <p>Log into the BlueBEAR Portal and start a BlueBEAR GUI session (2 hours). </p> <p>In your session, open a new terminal window and navigate to the <code>diffusionMRI</code> data in your <code>MRICN</code> folder:</p> <p><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/diffusionMRI</code>  [where XXX=your ADF username]</p> <p>In case you missed the previous workshop</p> <p>You were instructed to copy the <code>diffusionMRI</code> data in the previous workshop. If you have not completed last week's workshop, you either need to find details on how to copy the data in the 'Workshop 3: Basic diffusion MRI analysis' materials or work with someone who has completed the previous workshop.</p> <p>Then load FSL and FSLeyes:</p> <pre><code>module load FSL/6.0.5.1-foss-2021a-fslpython\nmodule load FSLeyes/1.3.3-foss-2021a\n</code></pre> <p>We will now look at how to \u201dskull-strip\u201d the T1 image (remove skull and non-brain areas); this step is needed for the registration step in both fMRI and diffusion MRI analysis pipelines.  </p> <p>We will do this using BET on the command line. The basic command-line version of BET is:</p> <p><code>bet &lt;input&gt; &lt;output&gt; [options]</code></p> <ul> <li>input = the input image (e.g., T1 scan)</li> <li>output = the filename of BET output (e.g., T1_brain)</li> <li>options = enable to control how to run BET</li> </ul> <p>In this workshop we will look at a simple brain extraction i.e., performed without changing any default options.</p> <p>To do this, navigate inside the <code>p01</code> folder:</p> <p><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/diffusionMRI/DTIfit/p01</code></p> <p>Then in your terminal type:</p> <p><code>bet T1.nii.gz T1_brain</code></p> <p>Once BET has completed (should only take a few seconds at most), open FSLeyes (with &amp; to background it).  Then in FSLeyes: </p> <ul> <li>Load up <code>T1.nii.gz</code> and add the <code>T1_brain</code> image</li> <li>Change the colour for the <code>T1_brain</code> to 'Red' or 'Green'</li> </ul> <p>This will likely show that in this case the default brain extraction was good. The reason behind such a good brain extraction with default options is a small FOV and data from a young healthy adult. This is not always the case e.g., when we have a large FOV or data from older participants. </p> <p>More brain extraction to come? You BET!</p> <p>In the next workshop (Workshop 5) we will explore different BET [options] and how to troubleshoot brain extraction.</p>"},{"location":"workshop4/probabilistic-tractography/#preparing-our-data-with-bedpostx","title":"Preparing our data with BEDPOSTX","text":"<p>BEDPOSTX is an FSL tool used for a step in the diffusion MRI analysis pipeline, which prepares the data for probabilistic tractography. BEDPOSTX (Bayesian Estimation of Diffusion Parameters Obtained using Sampling Techniques, X = modelling Crossing Fibres) estimates fibre orientation in each voxel within the brain. BEDPOSTX employs Markov Chain Monte Carlo (MCMC) sampling to reconstruct distributions of diffusion parameters at each voxel.</p> <p>We will not run\u00a0it\u00a0during this workshop as it takes a long time. The data has been processed for you, and you copied it at the start of the previous workshop.</p> <p>To run it, you would need to open FSL GUI, click on FDT diffusion and from drop down menu select 'BEDPOSTX'. The input directory must contain the distortion corrected diffusion file (<code>data.nii.gz</code>), binary brain mask (<code>nodif_brain_mask.nii.gz</code>) and two text files with the b-values (<code>bvals</code>) and gradient orientations (<code>bvecs</code>). </p> <p> In case of the data being used for this workshop with a single b-value, we need to specify the single-shell model. </p> <p>After the workshop, in your own time, you could run it using the provided data (see Tractography Exercises section at the end of workshop notes).</p> <p>BEDPOSTX outputs a directory at the same level as the input directory called <code>[inputdir].bedpostX</code> (e.g. <code>data.bedpostX</code>).  It contains various files (including mean fibre orientation and diffusion parameter distributions) needed to run probabilistic tractography.</p> <p>As we will look at tractography in different spaces, we also need the output from registration. The concept of different image spaces has been introduced in Workshop 2.  The registration step can be run from the FDT diffusion toolbox after BEDPOSTX has been run.  Typically, registration will be run between three spaces: </p> <ol> <li>Diffusion space (<code>nodif_brain</code> image) </li> <li>Structural space (T1 image for the same participant)</li> <li>Standard space (the MNI152 template)</li> </ol> <p>This step has been again run for you. To run it, you would need to open FSL GUI, click on 'FDT diffusion' and from the drop down menu select 'Registration'.  The main structural image would be your \u201dskull-stripped\u201d T1 (<code>T1_brain</code>) and non-betted structural image would be T1. Plus you need to select <code>data.bedpostX</code> as the 'BEDPOSTX directory'.</p> <p></p> <p> </p> <p>After the workshop, you can try running it in your own time (see Tractography Exercises section at the end of workshop notes).</p> <p>Registration output directory</p> <p>The outputs from registration needed for probabilistic tractography are stored in the <code>xfms</code> subdirectory. </p>"},{"location":"workshop4/probabilistic-tractography/#probabilistic-tractography-using-probtrackx","title":"Probabilistic tractography using PROBTRACKX","text":"<p>PROBTRACKX (probabilistic tracking with crossing fibres) is an FSL tool used for probabilistic tractography. To run it, you need to open FSL GUI, click on FDT diffusion and from the drop down menu select PROBTRACKX (it should default to it). </p> <p>PROBTRACKX can be used to run tractography either in diffusion or non-diffusion space (e.g., standard or structural).  If running it in non-diffusion space you will need to provide a reference image. You can also run tractography from a single seed (voxel), single mask (ROI) or from multiple masks which can be specified in either diffusion or non-diffusion space. </p> <p>We will look at some examples of different ways of running tractography.</p> <p>First close any processes still running and open a new terminal. Next navigate inside where all the files to run tractography have been prepared for you:</p> <p><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/diffusionMRI/tractography/p01</code></p> <p>As you may recall, on BlueBEAR there are different versions of FSL available. These correspond to different FSL software releases and have been compiled in a different way. The different versions of FSL are suitable for different purposes i.e., used for different MRI data analyses. </p> <p>To run BEDPOSTX and PROBTRACKX, you need to use a specific version of FSL (FSL 6.0.7.6), which you can load by typing in your terminal:</p> <pre><code>module load bear-apps/2022b\nmodule load FSL/6.0.7.6\nsource $FSLDIR/etc/fslconf/fsl.sh\n</code></pre> <p>Once you have loaded FSL using these commands, open the FDT toolbox from either the FSL GUI or directly typing in your terminal:</p> <p><code>Fdt &amp;</code></p> <p>We will start with tractography from a single voxel in diffusion space. Specifically, we will run it from a voxel with coordinates [47, 37, 29] located within the forceps major of the corpus callosum, a white matter fibre bundle which connects the occipital lobes. </p> <p>Running tractography on another voxel</p> <p>Later, you can use the FA map (<code>dti_FA</code> inside the <code>p01/data</code> folder) loaded to FSLeyes to check the location of the selected voxel, choose another voxel within a different white matter pathway, and run the tractography again. </p> <p>You should have the FDT Toolbox window open as below:</p> <p> </p> <p>From here do the following: </p> <ol> <li>Select <code>data.bedpostX</code> as the 'BEDPOSTX directory'</li> <li>Enter voxel coordinates [47, 37, 29] (X, Y, Z)</li> <li>Enter output file name 'corpus' - this we be the name of directory that contains the output files</li> <li>Press Go (you will see something happening in the terminal, once you see window Done!/OK, you are ready to view results. Before proceeding click OK)</li> </ol> <p>After the tractography has finished, check the contents of subdirectory <code>/corpus</code> with the tractography output files. It should contain:</p> <ul> <li><code>probtrackx.log</code> with the <code>probtrackx</code> command that was run </li> <li><code>fdt_coordinates.text</code> with used coordinates  </li> <li><code>corpus_47_37_29.nii.gz</code> (general structure <code>outputname_X_Y_Z.nii.gz</code>; where\u00a0outputname\u00a0= name of the subdirectory and\u00a0X,\u00a0Y, and\u00a0Z = the seed voxel coordinates). This file contains for each voxel a count of how many of the streamlines intersected with that voxel. </li> </ul> <p>We will explore the results later. First, you will learn how to run tractography in the standard (MNI) space.</p> <p>Close FDT toolbox and then open it again from the terminal to make sure you don\u2019t have anything running in the background.</p> <p>We will now run tractography using a combination of masks (ROIs) in standard space to reconstruct tracts connecting the right motor thalamus (portion of the thalamus involved in motor function) with the right primary motor cortex. The ROI masks have been prepared for you and put inside the mask subdirectory <code>~/diffusionMRI/tractography/masks</code>. The ROIs have been created using FSL's ATLAS tools (you\u2019ve learnt in a previous workshop how to do this) and are in standard/MNI space, thus we will run tractography in MNI (standard) space and not in diffusion space.</p> <p>This is the most typical design of tractography studies.</p> <p>In the FDT Toolbox window - before you select your input in the 'Data' tab - go to the 'Options' tab (as below) and reduce the number of samples to 500 under 'Options'. You would normally run 5000 (default) but reducing this number will speed up processing and is useful for exploratory analyses.</p> <p> </p> <p>Now going back to the 'Data' tab (as below) do the following:</p> <p> </p> <ol> <li>Select <code>data.bedpostX</code> as 'BEDPOSTX directory'</li> <li>In 'Seed Space', change 'Single voxel' to 'Single mask'</li> <li>As 'Seed Image' choose <code>Thalamus_motor_RH.nii.gz</code> from the <code>masks</code> subdirectory</li> <li>Tick both 'Seed space is not diffusion' and 'nonlinear'</li> <li>You have to use the warp fields between standard space and diffusion space created during registration, which are inside the <code>data.bedpost/xfms</code> directory. Select <code>standard2diff_warp</code> as 'Seed to diff transform' and <code>diff2standard_warp</code> as 'diff to Seed transform'. These files are generated during registration.</li> <li>As a waypoint mask choose <code>cortex_M1_right.nii.gz</code> from the <code>masks</code> subdirectory to isolate only those tracts that reach from the motor thalamus. Use this mask also as a termination mask to avoid tracking to other parts of the brain.</li> <li>Enter output file name <code>MotorThalamusM1</code></li> <li>Press Go!</li> </ol> <p>Specifying masks</p> <p>Without selecting the waypoint and termination masks, you would also get other tracts passing via motor thalamus, including random offshoots with low probability (noise). This is expected for probabilistic tractography, as the random sampling without specifying direction can result in spurious offshoots into nearby tracts and give low probability noise. </p> <p>It will take significantly longer this time to run the tractography in standard space. However, once it has finished, you will see the window 'Done!/OK'. Before proceeding, click 'OK'. </p> <p>A new subdirectory will be created with the chosen output name <code>MotorThalamusM1</code>. Check the contents of this subdirectory. It contains slightly different files compared to the previous tractography output. The main output, the streamline density map is called <code>fdt_paths.nii.gz</code>. There is also a file called <code>waytotal</code> that contains the total number of valid streamlines runs.</p> <p>We will now explore the results from both tractography runs. First close FDT and your terminal as we need FSLeyes, which cannot be loaded together with the current version of FSL.</p> <p>Next navigate inside where all the tractography results have been generated and load/open FSLeyes:</p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/diffusionMRI/tractography/p01\nmodule load FSLeyes/1.3.3-foss-2021a\nfsleyes &amp;\n</code></pre> <p>We will start with our results from tractography in seed space. In FSLeyes, do the following:</p> <ol> <li>Load the FA map (<code>~/diffusionMRI/tractography/p01/data/dti_FA.nii.gz</code>) and tractography output file (<code>~/corpus/corpus_47_37_29.nii.gz</code>)</li> <li>Change the colour of tractography output to\u00a0'Red-Yellow'</li> <li>Change the 'Min' display thresholds to 50 to remove voxels with low probability of being in the tract. The displayed values denote a number of streamlines running through a voxel. If we use default settings, 5000 samples are generated, thus 50 represents a 1% probability, meaning that the voxels that are not shown when \"Min\" is set to 50 are voxels with a probability (of being part of the tract) of less than 1%. </li> </ol> <p>Your window should look like this:</p> <p> </p> <p>Once you have finished reviewing results of tractography in see space, close the results ('Overlay \u2192 Remove all').</p> <p>We will now explore the results from our tractography ran in MNI space, but to do so we need a standard template. Assuming you have closed all previous images:</p> <ol> <li>Load in the MNI template (<code>~/diffusionMRI/tractography/MNI152T1_brain.nii.gz</code>) and tractography output file (<code>/MotorThalamusM1/fdt_paths.nii.gz.</code>)</li> <li>Change the colour of tractography output to\u00a0'Red-Yellow'</li> <li>You might want to add/load the ROI masks ('motor thalamus' and 'M1')</li> <li>Adjust the min and max display thresholds to explore the reconstructed tract. Change the Min display thresholds to 50 to remove voxels with low probability of being in the tract. There is no gold standard for thresholding tractography outputs. It will depend on study design, parameter set up and further analysis.</li> </ol> <p>Tractography exercises</p> <p>In your own time, you should try the exercises below to consolidate your tractography skills. If you have any problems completing or any further questions, you can ask for help during one of the upcoming workshops.  </p> <ul> <li>Exercise 1: Run tractography using only a single mask i.e., as during the workshop but without waypoint and termination masks (use only <code>Thalamus_motor_RH.nii.gz</code> mask as seed image). Compare the results to the output from our tractography we ran during the workshop.</li> <li>Exercise 2: Run tractography with <code>cortex_M1_right.nii.gz</code> mask as the seed image and without <code>Thalamus_motor_RH.nii.gz</code> as waypoint and termination masks. Compare these results to previous outputs (from thw tractography we ran during the workshop). Are the results the same? Why not? </li> <li>Exercise 3: In the <code>mask</code> subdirectory, you will find two other masks: <code>LGN_left.nii.gz</code> and <code>V1_left.nii.gz</code>. You can use a combination of these two masks to reconstruct portion of the left hemispheric optic radiation connecting the left lateral geniculate nucleus (LGN) and left primary visual cortex (V1). Hint: use LGN as the seed image and the V1 mask as waypoint and termination masks. </li> <li>Exercise 4: Use FSL's ATLAS tools to create your own mask(s) and use them for tractography. </li> <li>Exercise 5: During the workshop there was not enough time to run BEDPOSTX. As suggested earlier try running it using the provided data for participant <code>p02</code> (<code>~/diffusionMRI/tractography/p02/</code>). It might take ~60-90 minutes to run.</li> <li>Exercise 6: During the workshop there was also not enough time to run the registration needed to perform tractography in non-diffusion space. As suggested earlier try running it using the provided instructions and data for participant <code>p02</code> (<code>~/diffusionMRI/tractography/p02/</code>). To run it you first need to complete Exercise 5. It will take ~15min to complete registration.  </li> </ul> <p>Help and further information</p> <p>As always, more information on diffusion analyses in FSL, can be found on the 'diffusion' section of the FSL Wiki and this practical course ran by FMRIB (the creators of FSL).</p>"},{"location":"workshop4/workshop4-intro/","title":"Workshop 4 - Advanced diffusion MRI analysis","text":"<p>Welcome to the fourth workshop of the MRICN course! </p> <p>In the previous workshop we started exploring different elements of the FMRIB's Diffusion Toolbox (FDT). This week we will continue with the different applications of the FDT toolbox and the use of Brain Extraction Tool (BET).</p> <p>Overview of Workshop 4</p> <p>Topics for this workshop include:</p> <ul> <li>Preparing MRI images for diffusion analysis by skull-stripping using BET</li> <li>Using BEDPOSTX to estimate fiber orientations in each brain voxel</li> <li>Running probabilistic tractography with PROBTRACKX to trace white matter pathways between brain regions</li> <li>Visualizing and evaluating tractography results using FSLeyes</li> </ul> <p>We will be working with various previously acquired datasets (similar to the data acquired during the CHBH MRI Demonstration/Site visit).  We will not go into details as to why and how specific sequence parameters and specific values of the default settings have been chosen.  Some values should be clear to you from the lectures or assigned on Canvas readings, please check there, or if you are still unclear, feel free to ask. </p> <p>Note that for your own projects, you are very likely to want to change some of these settings/parameters depending on your study aims and design. </p> <p>In this workshop we will follow basic steps in the diffusion MRI analysis pipeline, specifically with running tractography.  The instructions here are specific to tools available in FSL. Other neuroimaging software packages can be used to perform similar analyses.</p> Example of Diffusion MRI analysis pipeline <p> </p> <p>The copy of this workshop notes can be found on Canvas 39058 - LM Magnetic Resonance Imaging in Cognitive Neuroscience in Week 04 workshop materials.</p>"},{"location":"workshop5/first-level-analysis/","title":"Running the first-level fMRI analysis","text":"<p>We are now ready to proceed with running our fMRI analysis. We will start with the first dataset (first participant <code>/p01</code>) and our first step will be to skull-strip the data using BET. You should now be able by now to not only run BET but also to troubleshoot poor BET i.e., use different methods to run BET.</p> <p>The <code>p01</code> T1 scan was acquired with a large field-of-view (FOV) (you can check this using FSLeyes; it is generally a good practice to explore the data before the start of any analysis, especially if you were not the person who acquired the data). Therefore, we will apply an appropriate method using BET as per the example we explored earlier. This will be likely the right method to be applied to all datasets in the <code>/recon</code> folder but please check.</p> <p>Open a terminal and use the commands below to skull-strip the T1:</p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/recon/p01\nmodule load FSL/6.0.5.1-foss-2021a\nmodule load FSLeyes/1.3.3-foss-2021a \nimmv T1 T1neck \nrobustfov -i T1neck -r T1 \nbet T1.nii.gz T1_brain -R \n</code></pre> <p>Remember that:</p> <ul> <li>The <code>immv</code> command renames the T1 image, and automatically takes care of the filename extensions</li> <li>The <code>robustfov</code> command crops the image and names it back to <code>T1.nii.gz</code></li> <li>The <code>bet -R</code> command runs BET recursively</li> </ul> <p>It is very important that after running BET that you examine, using FSLeyes, the quality of the brain extraction process performed on each and every T1.</p> <p>A poor brain extraction will affect the registration of the functional data into MNI space giving a poorer quality of registered image. This in turn will mean that the higher-level analyses (where functional data are combined in MNI space) will be less than optimal. It will then be harder to detect small BOLD signal changes in the group. </p> <p>Re-doing inaccurate BETs</p> <p>Whenever the BET process is unsatisfactory you will need to go back and redo the individual BET extraction by hand, by tweaking the \u201cFractional intensity threshold\u201d and/or the advanced option parameters for the centre coordinates\" and/or the \u201cThreshold gradient\u201d.</p> <p>You should be still inside the <code>/p01</code> folder; please rename the fMRI scan by typing:</p> <p><code>immv fs005a001 fmri1</code></p>"},{"location":"workshop5/first-level-analysis/#setting-up-and-running-the-first-level-fmri-analysis-using-feat","title":"Setting up and running the first-level fMRI analysis using FEAT","text":"<p>We are now ready to proceed with our fMRI data analysis. To do that we will need a different version of FSL installed on BlueBEAR. Close your terminal and again navigate inside the <code>p01</code> folder:</p> <p><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/recon/p01</code></p> <p>Now load FSL using the commands below:</p> <pre><code>module load bear-apps/2022b\nmodule load FSL/6.0.7.6\nsource $FSLDIR/etc/fslconf/fsl.sh\n</code></pre> <p>Finally, open FEAT (from the FSL GUI or by typing <code>Feat &amp;</code> in a terminal window).</p> <p>On the menus, make sure 'First-level analysis' and 'Full analysis' are selected. Now work through the tabs, setting and changing the values for each parameter as described below.  Try to understand how these settings relate to the description of the experiment as provided at the start.</p> <p> </p> Misc Tab <p>Accept all the defaults.</p> Data Tab <p>Input file</p> <p>The input file is the 4D fMRI data (the functional data for participant 1 should be called something like <code>fmri1.nii.gz</code> if you have renamed it as above). Select this using the 'Select 4D data' button. Note that when you have selected the input, 'Total volumes' should jump from 0.</p> <p>Total volumes troubleshooting</p> <p>If \u201cTotal volumes\u201d is still set to 0, or jumps to 1, you have done something wrong. If you get this, stop and fix the error at this point. DO NOT CARRY ON. If \u201cTotal volumes\u201d is still set to 0, that means you have not yet selected any data. Try again. If \u201cTotal volumes\u201d is set to 1, that means you have most likely selected the T1 image, not the fMRI data. Try again, but selecting the correct file.</p> <p>Check carefully at this point that the total number of volumes is correct (93 volumes were collected on participants 1-2, 94 volumes on participants 3-15). </p> <p>Output directory</p> <p>Enter a directory name in the output directory. This needs to be something systematic that you can use for all the participants and which is comprehensible.  It needs to make sense to you when you look at it again in a year or more in the future. It is important here to use full path names.  It is also very important that you do not use shortened or partial path names and that you do not put any spaces in the filenames you use.  If you do, these may cause some programs to crash with errors that may not seem to make much sense.</p> <p>For example, use an output directory name like:</p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1/p01_s1</code></p> <p>where:</p> <ul> <li><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat</code> is the top level directory where you intend to put all of your upcoming FEAT analyses for the experiment</li> <li><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1</code> is the sub-directory where you intend to put specifically only the 1st level (per session) FEAT analyses (and not the 2nd or higher level analyses). <code>p01</code> refers to participant 1 and <code>s1</code> refers to session/scan 1</li> </ul> <p>Note that when FEAT is eventually run this will automatically create a new directory called <code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1/p01_s1.feat</code> for you containing the output of this particular analysis. If the directory structure does not exist, FEAT will try and make it. You do not need to make it yourself in advance.</p> <p>Repetition Time (TR)</p> <p>For this experiment make sure that the TR is set to 2.0s. If FEAT can read the TR from the header information it will try and set it automatically. If not you will need to set it manually.</p> <p>High pass filter cutoff</p> <p>Set 'High pass filter cutoff' to 60sec (i.e. 50% greater than OFF+ON length of time).</p> Pre-stats <p> </p> <p>Set the following:</p> <ul> <li>Alternative reference image checkbox: OFF</li> <li>Motion Correction: Select McFLIRT</li> <li>B0 unwarping checkbox: OFF </li> <li>Slice timing: Select \u201cRegular up\u201d (Some researchers advise against slice timing correction, however we will use it here to illustrate the process).</li> <li>Bet brain extraction checkbox: ON</li> <li>Spatial smoothing: 5mm</li> <li>Intensity normalization: OFF</li> <li>Temporal filtering \u2013 Perfusion Subtraction checkbox: OFF</li> <li>Highpass checkbox: ON </li> <li>Melodic ICA data exploration checkbox: OFF</li> </ul> Stats <p> </p> <p>Set the following:</p> <ul> <li>Use FILM prewhitening checkbox: ON</li> <li>Motion parameters: Select the option \u201cStandard Motion Parameters\u201d</li> <li>Voxelwise confound List: Leave empty</li> <li>Apply external script: Leave empty</li> <li>Add additional confound EVs checkbox: OFF </li> </ul> <p>Select the 'Full model setup' option (depicted on the blue arrow above); and then on the 'EVs' tab:</p> <ul> <li>1 EV (Explanatory variable) name: vision</li> <li>Basic shape: square</li> <li>Skip: 0s</li> <li>OFF: 20s </li> <li>ON: 20s </li> <li>phase: 0 </li> <li>stop: 180</li> <li>Convolution: select defaults (Gamma: 0,3,6)</li> </ul> <p>On the Contrasts Tab: </p> <ul> <li>We have 1 contrast, name it 'vision', and then click done</li> </ul> <p>Check the plot of the design that will be generated and then click on the image to dismiss it.</p> Post-stats <p>Change the 'Thresholding' pull down option to be of type 'Uncorrected' and leave the P threshold value at p&lt;0.05. </p> <p>Thresholding and processing time</p> <p>Note this is not the correct thresholding that you will want at the final (third stage) of processing (where you will probably want 'Cluster thresholding') but for the convenience of the workshop, at this stage it will speed up the processing per run. </p> Registration <p>Set the following:</p> <ul> <li>Click the checkbox for 'Main structural image' and choose the BET\u2019ed anatomical (i.e. participant 1's <code>T1_brain.nii.gz</code>) as the main structural image with 'Linear Options: Normal search, BBR'</li> <li>Accept the default as the standard brain with 'Linear Options: Normal search, 12 DOF'</li> <li>Make sure the 'Nonlinear' checkbox is set to 'OFF'</li> </ul> <p>The model should now be set up with all the correct details and be ready to be analyzed.</p> <p>Hit the GO button!</p> <p>Running FSL on BlueBEAR</p> <p>FSL jobs are now submitted in an automated way to a back-end high performance computing cluster on BlueBEAR for execution. Processing time for this analysis will vary but will probably be about 5 mins per run.</p>"},{"location":"workshop5/first-level-analysis/#monitoring-and-viewing-the-results","title":"Monitoring and viewing the resultsSeeing the effect of other parametersAnalysing other participants' data","text":"<p>FEAT has a built-in progress watcher, the 'FEAT Report', which you can open in a web browser. </p> <p>To do that, you need to navigate inside the <code>p01_s1.feat</code> folder from the BlueBEAR Portal as below and from there select the <code>report.html</code> file, and either open it in a new tab or in a new window. </p> <p> </p> <p>Watch the webpage for progress. Refresh the page to update and click the links (the tabs near the top of the page) to see the results when available (the 'STILL RUNNING' message will disappear when the analysis has finished).</p> <p> </p> <p>Example FEAT Reports for processes that are still running, and which have completed.</p> <p>After it has completed, first look at the webpage, click on the various links and try to understand what each part means.</p> <p> </p> <p>Now let's use FSLeyes to look at the output in more detail. To do that you will need to open a separate terminal and load FSLeyes:</p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/recon/p01\nmodule load FSL/6.0.5.1-foss-2021a-fslpython\nmodule load FSLeyes/1.3.3-foss-2021a\nfsleyes &amp;\n</code></pre> <p>Open the <code>p01_s1.feat</code> folder and select the <code>filtered_func_data</code> (this is the fMRI data after it has been preprocessed by motion correction etc). </p> <p>Put FSLeyes into movie mode and see if you can identify areas that change in activity.</p> <p>Now, add the <code>thresh_zstat1</code> image and try to identify the time course of the stimulation in some of the most highly activated voxels. You should remember how to complete the above tasks from previous workshops. You can also use the \u201ccamera\u201d icon to take a snapshot of the results.</p> <p> </p> <p>Let's have a look and see the effects that other parameters have on the data. To do this, do the following steps:</p> <ul> <li>Open FEAT (either through the GUI or using the terminal by typing <code>Feat &amp;</code>)</li> <li>Press the 'Load' button and open the <code>design.fsf</code> file in the <code>p01_s1.feat</code> directory for the first participant</li> <li>Change any one of the parameters \u2013 some make very little difference but ones that should have some difference are: <ul> <li>Motion parameters: change from 'Standard Motion Parameters' to 'Don't Add Motion Parameters'</li> <li>Spatial smoothing (previously set to 5mm): try increasing to 10mm</li> <li>High pass filter: set to 30sec (i.e. 50% less than OFF+ON time period). </li> </ul> </li> <li>Hit 'Go'</li> </ul> <p>Note that each time you rerun Feat, it creates a new folder with a '+' sign in the name. So you will have folders rather messily named 'p01_s1.feat', 'p01_s1+.feat', 'p01_s1++.feat', and so on. This is rather wasteful of of your precious quota space, so you should delete unnecessary ones after looking at them. </p> <p>For example, if you wanted to remove all files and directories that end with '+' for participant 1:</p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1/ \nrm -rf *+\n</code></pre> <p>You might want also to change the previous output directory name to have a more meaningful name in order to make it more obvious what parameter has changed, e.g. <code>p01_s1_motion_off.feat</code>.</p> <p>For participant 2, you will need to repeat the main steps above:</p> <ul> <li>Rename the files to be consistent with your naming scheme for participant <code>p01</code> </li> <li>Skull strip the reoriented T1 scan (and check it is done properly)</li> <li>Run FEAT</li> </ul> <p>To rerun a FEAT analysis, rather than re-entering all the model details:</p> <ul> <li>Open FEAT, press the 'Load' button and open the 'design.fsf' file in the FEAT directory from participant <code>p01</code></li> </ul> <p>Now change the input 4D file, the output directory name, and the registration details (the BET'ed reoriented T1 for participant 2), and hit 'Go'.</p> <p>Design files</p> <p>You can also save the design files (<code>design.fsf</code>) using the 'Save' button on the FEAT GUI. You can then edit this in a text editor, which is useful when running large group studies. You can also run FEAT from the command line, by giving it a design file to use e.g., <code>feat my_saved_design.fsf</code>. We will take a look at modifying <code>design.fsf</code> files directly in the Functional Connectivity workshop.</p> <p>Running a first-level analysis on the remaining participants</p> <p>In your own time, you should analyse the remaining participants as above. </p> <p>Remember:</p> <ul> <li>Participants 1-2 have only one fMRI run each (i.e. 2 people x 1 run each = 2 runs)</li> <li>Participant 5 has 3 fMRI runs (i.e. 1 person x 3 runs = 3 runs)</li> <li>Participants 3-4 and 6-15 have 2 fMRI runs each (i.e. 12 people x 2 runs = 24 runs)</li> </ul> <p>There are therefore 29 separate analyses that need to be done.</p> <ul> <li>Analyze each of these 29 fMRI runs independently and put the output of each one into a separate, clearly labelled directory as suggested above. </li> <li>Try and get all these done before the next fMRI workshop in week 10 on higher level fMRI analysis as you will need this processed data for that workshop. You have two weeks to complete this task. </li> </ul> <p>Scripting your analysis</p> <p>It will seem laborious to re-write and re-run 29 separate FEAT analyses; a much quicker way is by scripting our analyses using <code>bash</code>. If you would like, try scripting your analyses! Contact one of the course TA's or convenors if you are stuck!</p> <p>As always, help and further information is also available on the relevant section of the FSL Wiki.</p>"},{"location":"workshop5/preprocessing/","title":"Troubleshooting brain extraction with BET","text":"<p>In the first part of the workshop, you will learn the proper skull-stripping of T1 scans using FSL's Brain Extraction Tool (BET), including troubleshooting techniques for problematic cases, as well as organizing neuroimaging data files through proper naming conventions. </p> Background and set-up <p>The data that we will be using are data collected from 15 participants scanned on the same experimental protocol on the Phillips 3T scanner (our old scanner).</p> <p>The stimulus protocol was a visual checkerboard reversing at 2Hz (i.e., 500ms between each reversal) and was presented alternately (20s active \u201con\u201d checkerboard, 20s grey screen \u201coff\u201d),  starting and finishing with \u201coff\u201d and including 4 blocks of \u201con\u201d (i.e., off, on, off, on, off, on, off, on, off) = 180 sec. </p> <p>A few extra seconds of \u201coff\u201d (6-8s) were later added at the very end of the run to match the number of volumes acquired by the scan protocol.</p> <p> </p> <p>Normally in any experiment it is very important to keep all the protocol parameters fixed when acquiring the neuroimaging data.  However, in this case we can see different parameters being used which reflect slightly different \u201cbest choices\u201d made by different operators over the yearly demonstration sessions:</p> <ul> <li>The repetition time and voxel size were the same for all scans: (TR = 2000 ms, voxel size 2.5 x 2.5 x 2.5mm).</li> <li>However, 93 volumes were collected on participants 1-2, 94 volumes on all later participants.</li> <li>The first 11 participants were scanned with an 8-channel head coil, acquiring 32 slices in ascending order, whilst participants 12-15 were scanned with a 32-channel head coil, acquiring 30 slices in ascending order. </li> <li>Each participant always also had a planning scan (i.e., so called anatomical localizer, this was always scan 1 and can be ignored), a T1 anatomical scan and one or more fMRI scans. </li> <li>Participants 1-2  performed only 1 functional sequence, participants 3-4 and 6-15 performed 2 functional sequences, and participant 5 performed 3 functional sequences. </li> </ul> <p>Sequence order</p> <p>Note that sometimes the T1 was the first scan acquired after the planning scan, sometimes it was the very last scan acquired.</p> <p>Now that we know what the data is, let's start our analyses. Log in into the BlueBEAR Portal and start a BlueBEAR GUI session (2 hours). You should know how to do it by now from previous workshops. </p> <p>Open a new terminal window and navigate to your MRICN project folder:</p> <p><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx</code>  [where XXX=your ADF username] </p> <p>Please check that you are in the correct directory by typing <code>pwd</code>. This should return: </p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/xxx</code> (where XXX = your login username)</p> <p>You now need to create a copy of the reconstructed fMRI data to be analysed during the workshop but in your own MRICN folder. To do this, in your terminal type:</p> <p><code>cp -r /rds/projects/c/chechlmy-chbh-mricn/module_data/recon/ .</code></p> <p>Be patient as this might take few minutes to copy over. In the meantime, we will revisit BET and learn how to troubleshoot the often problematic process of 'skull-stripping'.</p>"},{"location":"workshop5/preprocessing/#skull-stripping-t1-scans-using-bet-on-the-command-line","title":"Skull-stripping T1 scans using BET on the command-line","text":"<p>We will now look at how to \u201dskull-strip\u201d the T1 image (remove the skull and non-brain areas), as this step is needed as part of the registration step in the fMRI analysis pipeline.  We will do this using FSL's BET on the command line. As you should know from previous workshops the basic command-line version of BET is: </p> <p>(do not type this command, this is just a reminder)</p> <p><code>bet &lt;input&gt; &lt;output&gt; [options]</code></p> <p>where:</p> <ul> <li>input = you need to specify input image (e.g., <code>T1_scan</code>)</li> <li>output = filename of BET output (e.g., <code>T1_brain</code>)</li> <li>options = controls how to run BET</li> </ul> <p>We will firstly explore the different options and how to troubleshoot brain extraction.</p> <p>If the fMRI data has finished copying over, you can use the same terminal which you have previously opened.  If not, keep that terminal open and instead open a new terminal, navigating inside your MRICN project folder (i.e., <code>/rds/projects/c/chechlmy-chbh-mricn/xxx</code>)</p> <p>Next you need to copy the data for this part of the workshop. As there is only 1 file, it will not take a long time. Type:</p> <p><code>cp -r /rds/projects/c/chechlmy-chbh-mricn/module_data/BET/ .</code></p> <p>And then load FSL and FSLeyes by typing:</p> <pre><code>module load FSL/6.0.5.1-foss-2021a\nmodule load FSLeyes/1.3.3-foss-2021a\n</code></pre> <p>After this, navigate inside the copied BET folder and type:</p> <p><code>bet T1.nii.gz T1_brain1</code></p> <p>Open FSLeyes (<code>fsleyes &amp;</code>), and when this is open, load up the T1 image, and add the <code>T1_brain1</code> image. Change the colour for the <code>T1_brain1</code> to Red. </p> <p>This will likely show that the default brain extraction was not very good and included non-brain matter. It may also have cut into the brain and thus some part of the cortex is missing. The reason behind the poor brain extraction is a large field-of-view (FOV) (resulting in the head plus a large amount of neck present). </p> <p>There are different ways to fix a poor BET output i.e., problematic \u201dskull-stripping\u201d.</p> <p>First of all, you can use the <code>-R</code> option. </p> <p>This option is used to run BET in an iterative fashion which allows it to better determine the centre of the brain itself. </p> <p>In your terminal type:</p> <p><code>bet T1.nii.gz T1_brain2 -R</code></p> <p>Running BET recursively from the BET GUI</p> <p>Instead of using the <code>bet -R</code> command from the terminal, you can also use the BET GUI. To run it this way, you would need to select the processing option \u201cRobust brain centre estimation (iterates bet2 several times)\u201d from the pull down menu.</p> <p>You will find that running BET with <code>-R</code> option takes longer than before because of the extra iterations. Reload the newly extracted brain (<code>T1_brain2</code>) into FSLeyes and check that the extraction now looks improved. </p> <p>In the case of T1 images with a large FOV, you can first crop the image (to remove portion of the neck) and run BET again. To do that you need to use command <code>robustfov</code> before applying BET. But first rename the original image. </p> <p>Type in your terminal:</p> <pre><code>immv T1 T1neck` \nrobustfov -i T1neck -r T1 \nbet T1.nii.gz T1_brain3 -R \n</code></pre> <ul> <li>The first command renames the T1 image, and automatically takes care of the filename extensions.</li> <li>The second command crops the image and names it back to <code>T1.nii.gz</code></li> <li>The third command runs BET again with the recursive <code>-R</code> option.</li> </ul> <p>Reload the newly extracted brain (<code>T1_brain3</code>) into FSLeyes and compare it to <code>T1_brain1</code> and to check that the extraction looks improved. Also compare the cropped T1 image to the original one with a large FOV (<code>T1neck</code>). </p> <p>Another option is to leave the large FOV and to manually set the initial centre by hand via the <code>-c</code> option on the command line.  To do that you need to firstly examine the T1 scan in FSLeyes to get a rough estimation (in voxels) of where the centre of the brain is. </p> <p>There is another BET option which can improve \u201dskull stripping\u201d: the fractional intensity threshold, which by default is set to 0.5.  You can change it from any value between 0-1. Smaller values give larger brain outline estimates (and vice versa). Thus, you can make it smaller if you think that too much brain tissue has been removed. </p> <p>To use it, you would need to use the <code>-f</code> option (e.g., <code>bet T1.nii.gz T1_brain -f 0.3</code>). </p> <p>Changing the fractional intensity</p> <p>In your own time (after the workshop) you can check the effect of changing the fractional intensity threshold to 0.1 and 0.9 (however make sure you name the outputs accordingly, so you know which one is which).</p> <p>It is very important that after running BET you always examine (using FSLeyes) the quality of the brain extraction process performed on each and every T1. </p> <p>The strategy you might need to use could be different for participants in the same study. You might need to try different options. The general recommendation is to combine the cropping (if needed) and the <code>-R</code> option.  However, it may not work for all T1 scans, some types of T1 scans work better with one strategy than with another. Therefore, it is good to always try a range of options.</p> <p>Now you should be able to \u201cskull-strip\u201d T1 scans as needed for fMRI analyses!</p>"},{"location":"workshop5/preprocessing/#exploring-the-data-and-renaming-the-mri-scan-files","title":"Exploring the data and renaming the MRI scan files","text":"<p>By now you should have a copy of the reconstructed fMRI data in your own folder. As described above, the <code>/recon</code> version of the directory contains the MRI data from 15 participants acquired over several years from various site visits. </p> <p>The datasets have been reconstructed into the NIFTI format. The T1 images in each directory are named <code>T1.nii.gz</code>. The first (planning) scan sequences (localisers) have been removed in each directory as these will not be needed for any subsequent analysis we are doing.</p> <p>Navigate inside the <code>recon</code> folder and list the contents of these directories (using the <code>ls</code> command) to make sure they actually contain imaging files. Note that all the imaging data here should be in NIFTI format. </p> <p>You should see the set of  participant directories labelled <code>p01</code>, <code>p02</code> etc., all the way up to the final directory,<code>p15</code>.</p> <p>The directory structure should look like this:</p> <pre><code>~/chechlmy-chbh-mricn/xxx/recon/\n                              \u251c\u2500\u2500 p01/\n                              \u251c\u2500\u2500 p02/\n                              \u251c\u2500\u2500 p03/\n                              \u251c\u2500\u2500 p04/\n                              \u251c\u2500\u2500 p05/\n                              \u251c\u2500\u2500 ...\n                              \u251c\u2500\u2500 p13/\n                              \u251c\u2500\u2500 p14/\n                              \u2514\u2500\u2500 p15/\n</code></pre> <p>Verifying the data structure</p> <p>Please verify that you have this directory structure before proceeding!</p> <p>Explore what\u2019s inside each participant folder. Please note that each participant folder only contains reconstructed data. It\u2019s a good idea to store raw and reconstructed data separately. At this point you should have access to reconstructed participants <code>p01</code> to <code>p15</code>. The reconstructed data should be in folders named <code>~/chechlmy-chbh-mricn/xxx/recon/p01</code> etc. </p> <p>However, apart from the T1 images that have been already renamed for you, the other reconstructed files in this directory will have unusual names, created automatically by the <code>dcm2nii</code> conversion program. </p> <p>You can see this by typing into your terminal:</p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/recon/p03 \nls\n</code></pre> <p>Which should list:</p> <pre><code>fs004a001.nii.gz\nfs005a001.nii.gz\nT1.nii.gz\n</code></pre> <p>It is poor practice to keep with these names as they do not reflect the actual experiment and will likely be a source of confusion later on. We should therefore rename the files to be something meaningful.  For this participant (<code>p03</code>) the first fMRI scan is file 1 (<code>fs004001.nii.gz</code>) and the second fMRI scan is file 2 (<code>fs005a001.nii.gz</code>). Rename the files as follows (to do that you need to be inside folder <code>p03</code>):</p> <pre><code>immv fs004a001 fmri1 \nimmv fs005a001 fmri2\n</code></pre> <p>Renaming files</p> <p>Notes:</p> <ul> <li>The 'immv' command is a special FSL Linux command that works just like the standard Linux <code>mv</code> command except that it automatically takes care of the filename extensions. It saves from having to write out: <code>mv fs004a001.nii.gz fmri1.nii.gz</code> which would be the standard Linux command to rename a file.</li> <li>You can of course name these files to anything you want. In principle, you could call the fMRI scan <code>run1</code> or <code>fmri_run1</code> or <code>epi1</code> or whatever. The important thing is that you need to be extremely consistent in the naming of files for the different participants.</li> </ul> <p>For this workshop we will use the naming convention above and call the files <code>fmri1.nii.gz</code> and <code>fmri2.nii.gz</code>.</p> <p>As the experimenter you would normally be familiar with the order of acquisition of the different sequences and therefore the order of the resulting files created, including which one is the T1 image. You would write these down in your research log book whilst acquiring the MRI data. But sometimes, as here, if data is given to you later it may not be clear which particular file is the T1 image. </p> <p>There are several ways to figure this out:</p> <ol> <li>The very first file will always (unless it has been deleted before it got to you) be a planning scan (localizer). This can be ignored. In general, the T1 image is very likely to be either the second file or the very last file.</li> <li>If you look at the list of file sizes (using <code>ls -al</code>) you should be able to see that the T1 image is smaller than most typical EPI fMRI images. Also, if there are more than one fMRI sequences (as here with <code>p03</code> onwards) you will also see that several files have the same file size and the odd one out is the T1.</li> <li>If you load the images into FSLeyes and look at them individually it should be very obvious which image is the T1. Remember the T1 image is a single volume, in high spatial resolution. It will also likely have a much larger field of view (showing all of the skull and part of the spine). The fMRI images will consist of many volumes (click through several volumes to check), be of lower spatial resolution (it will look coarser) and have a more limited field of view.</li> </ol> <p>If you have access to the NIFTI format files (<code>.nii.gz</code> as we have here) then you can use one of the FSL command line tools (in a terminal window) called <code>fslinfo</code> to examine the protocol information on the file.  This will show you the number of volumes in the acquisition (remember this is 1 volume for a T1 image) as well as other information about the number of voxels and the voxel size. </p> <p>Together this information is sufficient to work out which file is the T1 and which are the fMRI sequence(s). </p> <p>For example if you type the following in your terminal:</p> <pre><code>cd ..\ncd p08 \nfslinfo fs005a001.nii.gz\n</code></pre> <p>You should see something like the image below:</p> <p> </p> <p>Before proceeding to the next section on running a first-level fMRI analysis, close your terminal.</p>"},{"location":"workshop5/workshop5-intro/","title":"Workshop 5 - First-level fMRI analysis","text":"<p>Welcome to the fifth workshop of the MRICN course! </p> <p>The module lectures provide a basic introduction to fMRI concepts and the theory behind fMRI analysis, including the physiological basis of the BOLD response, fMRI paradigm design, pre-processing and single subject model-based analysis. </p> <p>In this workshop you will learn how to analyse fMRI data for individual subjects (i.e., at the first level). This includes running all pre-processing stages and the first level fMRI analysis itself.  The aim of this workshop is to introduce you to some of the core FSL tools used in the analysis of fMRI data and to gain practical experience with analyzing real fMRI data.</p> <p>Specifically,  we will explore FEAT (FMRI Expert Analysis Tool, part of FSL) to walk you through basic steps in first level fMRI analysis.  We will also revisit the use of Brain Extraction Tool (BET), and learn how to troubleshoot problematic \u201dskull-stripping\u201d for certain cases.</p> <p>Overview of Workshop 5</p> <p>Topics for this workshop include:</p> <ul> <li>Troubleshooting problematic skull-stripping by using BET options (recursive BET, <code>robustfov</code>)</li> <li>Renaming fMRI data and understanding the importance of having suitable file names</li> <li>Running a first-level fMRI analysis on single subjects using FEAT</li> <li>Examining the processed fMRI data using the FEAT Report and FSLeyes</li> </ul> <p>We will not go into details as to why and how specific values of the default settings have been chosen. Some values should be clear to you from the lectures or resource list readings, please check there or if you are still unclear feel free to ask.  We will explore some general examples. Note that for your own projects you are very likely to want to change some of these settings/parameters depending on your study aims and design. </p> <p>The copy of this workshop notes can be found on Canvas 39058 - LM Magnetic Resonance Imaging in Cognitive Neuroscience in Week 05 workshop materials.</p>"},{"location":"workshop6/running-containers/","title":"Scripting analyses, submitting jobs on the cluster, and running containers","text":""},{"location":"workshop6/running-containers/#introduction-to-scripting","title":"Introduction to scripting","text":"<p>Script files are text files that contain Linux command-line instructions that are equivalent to typing a series of commands in the terminal or the equivalent using the software GUI (e.g., FSL GUI). By scripting, it is possible to automate most of the FSL processing, including both diffusion MRI and fMRI analysis. In the previous workshop you were learning how to set up a first level fMRI model for a single experimental run for one participant. Subsequently, in your own time, you were asked to repeat that process for other participants (15 participants in total), some participants with 2 or 3 experimental runs. </p> <p>The notes below provide a basic introduction to Linux (bash) scripting as well as some guidelines and examples on how to automate the first-level analysis as you might want to do when completing the Data Analysis assignment.</p> <p>To do this, we have provided some basic info and script examples. You can use the examples when analysing the remaining participants' data (the task you were given to complete at the end of the previous workshop) if you have not done it already. If you have already done so, you can either repeat that process by scripting, or apply it to your assessment. But you can also complete all these tasks without scripting. </p> <p>All the example scripts shown below are in the folder:</p> <pre><code>/rds/projects/c/chechlmy-chbh-mricn/module_data/scripts\n</code></pre> <p> </p> <p>To start please copy the entire folder into your module directory. Please note that to run some of the scripts as per examples below, you will need to load FSL, something you should know how to do.</p> <p>A script can be very simple, containing just commands that you already know how to use, with each command put on a separate line. To create a script for automating FSL analysis, the most widely used language is <code>bash</code> (<code>shell</code>). To write a <code>bash</code> script you need a plain text editor (e.g., vim, nano). If you are not familiar with using a text editor in Linux terminal, there is a simple way of creating and/or editing scripts using the BlueBEAR portal.</p> <p>You can start a new script by clicking on \u201cNew File\u201d and naming it for example \u201c<code>my_script.sh</code>\u201d and next clicking on \u201cEdit\u201d to start typing commands you want to use. You can also use \u201cEdit\u201d to edit existing scripts. </p> <p> </p> <p>The shebang</p> <p>The first line of every <code>bash</code> script should be <code>#!/bin/bash</code>. This is the 'shebang' or 'hashbang' line. It tells the system which interpreter should be used to execute the script.</p> <p>Suppose we want to create a very simple script repeatedly using one of the FSL command line tools. Let's say that we want to find out what is the volume for each of the T1 brains in our experiment. The FSL command that will tell us this is <code>fslstats</code>, together with the <code>-V</code> option which lists the number and volume of non-empty voxels. </p> <p>To create this script, you would type the text below as in the provided <code>brainvols.sh</code> script example. To view it, select this script and click on 'Edit'. Alternatively, you can start a new file and copy the commands as shown below. (In the actual script you would need to replace <code>xxx</code> with the name of your own directory).</p> <pre><code>#!/bin/bash\n\ncd rds/projects/c/chechlmy-chbh-mricn/xxx/recon\nfslstats p01/T1_brain -V\nfslstats p02/T1_brain -V\nfslstats p03/T1_brain -V\nfslstats p04/T1_brain -V\nfslstats p05/T1_brain -V\nfslstats p06/T1_brain -V\nfslstats p07/T1_brain -V\nfslstats p08/T1_brain -V\nfslstats p09/T1_brain -V\nfslstats p10/T1_brain -V\nfslstats p11/T1_brain -V\nfslstats p12/T1_brain -V\nfslstats p13/T1_brain -V\nfslstats p14/T1_brain -V\nfslstats p15/T1_brain -V\n</code></pre> <p>Whether you are editing or creating a new script, you need to save it. After saving, exit the editor. </p> <p>Next you need to make the script executable (as below) and remember the script will run in the current directory (<code>pwd</code>). You also need to make the script executable if you copied a script from someone else.</p> <p>To make your script executable type in terminal: <code>chmod a+x brainvols.sh</code></p> <p>Running the script without permissions</p> <p>If you try to run the script without making it executable, you will get a permission error. </p> <p>To run the script, type in your terminal <code>./brainvols.sh</code></p> <p>You can now tell which participant has the biggest brain.</p> <p>The previous script hopefully worked. But it is not very elegant and is not much of an improvement over typing the commands one at a time in a terminal window. However, the <code>bash</code> scripting language that we are using provides an extra layer of simple program commands that we can use in combination with the FSL commands we want to run. In this way we can make our scripts more efficient:</p> <ul> <li> <p>In the <code>brainvols.sh</code> script above it would be better to replace the above <code>fslstats</code> lines of code \u2013 which basically do the same thing but with a different participant \u2013 with a loop of some kind. </p> </li> <li> <p>Also, it would be helpful to print out some text so that we know which line of output on the screen relates to which participant. </p> </li> </ul> <p>Bash has a <code>for \u2026 do ... done</code> construct to do the former and an <code>echo</code> command to do the latter. So, let's use these to create an improved script with a loop. This is illustrated in the example <code>brainvols2.sh</code>:</p> <pre><code>#!/bin/bash\n\ncd rds/projects/c/chechlmy-chbh-mricn/xxx/recon\nfor p in p01 p02 p03 p04 p05 p06 p07 p08 p09 p10 p11 p12 p13 p14 p15\ndo\n    echo -n \"Participant ${p}: \"\n    fslstats ${p}/T1_brain -V\ndone\n</code></pre> <p>Both examples above assume that you have already run BET (brain extraction) on T1 scans. But of course, you could also automate the process of brain extraction and complete both tasks, i.e., running bet and calculate volume, using a single script. This is illustrated in the example <code>bet_brainvols.sh</code>:</p> <pre><code>#!/bin/bash\n\n# navigate to the folder containing the T1 scans\ncd rds/projects/c/chechlmy-chbh-mricn/xxx/recon\n\n# a for loop over participant data files\nfor participant_num in p01 p02 p03 p04 p05 p06 p07 p08 p09 p10 p11 p12 p13 p14 p15\n\n# do the following ...\ndo\n    # show the participant number \n    printf \"Participant ${participant_num}: \\n\"\n\n    # delete the old extracted brain image (we can see this\n    # happen in real time in the file explorer)\n    rm ${participant_num}/T1_brain.nii.gz\n\n    # extract the brain from the T1 image\n    bet ${participant_num}/T1.nii.gz ${participant_num}/T1_brain\n\n    # list the number of non-empty brain voxels\n    fslstats ${participant_num}/T1_brain -V\n# end the loop\ndone\n\n# ============================================================\n# END OF SCRIPT\n</code></pre> <p>Some of the most powerful scripting comes when manipulating FEAT model files. When you create a design for a first level fMRI analysis in the FEAT GUI and press the 'Go' button, FEAT writes out the model analysis file into the output directory. The name for this saved file is <code>design.fsf</code>. Once you have created one of these files, you can load it back into FEAT and modify only the parts that are different between the different analyses and then resave it e.g., change parameters or change it for another participant (see workshop materials covering first level of fMRI analysis).</p> <p>Alternatively, since <code>design.fsf</code> is a text file it can also be opened (and edited) in a text editor. Because the experiment \u2013 and therefore the model design \u2013 is almost the same for all participants, there is very little difference in the <code>design.fsf</code> files between the level one analyses for different participants. In fact, if following the directory structure naming convention suggested in the workshop, the only thing that changes for a particular run is the identifier of the participant. </p> <p>So, if we copy the design file for <code>p01</code>'s first scan (i.e. the file <code>feat/1/p01_s1.feat/design.fsf</code>), open it up in a text editor, search and replace every instance of <code>p01</code> with <code>p02</code> and then save it, we should have the model file for <code>p02</code>'s first scan. The only differences should be in:</p> <ul> <li> <p>The 4d input EPI file: <code>/recon/p01/fmri1</code> \u2192 /recon/p02/fmri1</p> </li> <li> <p>The output directory: <code>/feat/1/p02_s1</code> \u2192 <code>/feat/1/p02_s1</code></p> </li> <li> <p>The registration: <code>/recon/p01/T1_brain</code> \u2192 <code>/recon/p02/T1_brain</code></p> </li> </ul> <p>In general, all the model files will only differ by the participant identifiers (<code>p01-p15</code>) the identifiers we've used for the particular scan number (<code>s1</code>, <code>s2</code> and <code>s3</code> for output directories, and <code>fmri1</code>, <code>fmri2</code> and <code>fmri3</code> in the input EPI file names). </p> <p>The special cases are the scans for the first two participants. These scans were only 93 volumes long, whereas all the rest of the scans following this are 94 volumes long. However, given the above information and the model file for participant 1, scan 1, we can now create a script that will generate all other model files. </p> <p>Firstly, let's create a new directory (<code>models</code>) where you will keep your model files. Navigate to your folder (<code>/rds/projects/c/chechlmy-chbh-mricn/xxx/</code>) and type:</p> <p><code>mkdir models</code></p> <p>Now copy the script <code>create_alldesignmodels.sh</code> into that folder. The script contains the following code:</p> <pre><code>#!/bin/bash\n\n# Copy over the saved model file for p01 scan 1\ncp /rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1/p01_s1.feat/design.fsf p01_s1.fsf\n\n# Create model file for p02 scan1\ncp p01_s1.fsf p02_s1.fsf\nperl -i -p -e 's/p01/p02/' p02_s1.fsf\n\n# Create model files for p03-p15 scan 1\nfor p in p03 p04 p05 p06 p07 p08 p09 p10 p11 p12 p13 p14 p15\ndo\n    cp p01_s1.fsf ${p}_s1.fsf\n    perl -i -p -e \"s/p01/${p}/\" ${p}_s1.fsf\n    perl -i -p -e 's/93/94/' ${p}_s1.fsf\ndone\n\n# Create model files for p03-p15 scan 2\nfor p in p03 p04 p05 p06 p07 p08 p09 p10 p11 p12 p13 p14 p15\ndo\n    cp ${p}_s1.fsf ${p}_s2.fsf\n    perl -i -p -e 's/_s1/_s2/' ${p}_s2.fsf\n    perl -i -p -e 's/fmri1/fmri2/' ${p}_s2.fsf\ndone\n\n# Create model files for p05 scan 3\ncp p05_s1.fsf p05_s3.fsf\nperl -i -p -e 's/_s1/_s3/' p05_s3.fsf\nperl -i -p -e 's/fmri1/fmri3/' p05_s3.fsf\n</code></pre> <p>Edit (hint: replace the <code>xxx</code>), and save the file, make it executable, and then run it. </p> <p>If it has worked you should now have a directory full of model files. Each of them can be run from the command line with a command such as <code>feat p01_s1.fsf</code> or with a script (you should be able to create such script using the earlier example).</p> <p>FSL's scripting tutorial</p> <p>You can find more information and other examples on FSL's scripting tutorial webpage.</p>"},{"location":"workshop6/running-containers/#submitting-jobs-to-the-cluster","title":"Submitting jobs to the cluster","text":"<p>The first part of this workshop introduced you to running <code>bash</code> scripts using the terminal in the BlueBEAR GUI. However, in addition to running <code>bash</code> scripts in this way, you can also create scripts and run analysis jobs directly on the cluster with Slurm (BlueBEAR's high performance computing (HPC) scheduling system). </p> <p>What is Slurm?</p> <p>Understanding how Slurm works is beyond the scope of this course, and is not strictly necessary, but you can find out more by reading the official Slurm documentation.</p> <p>In previous workshops we were using BlueBEAR Portal to launch BlueBEAR GUI Linux desktop and from there the built-in terminal. As mentioned in workshop 1, you can also use BlueBEAR Portal to jump directly on BlueBEAR terminal, to access one of the available login nodes and from there, run analysis jobs. </p> <p> </p> <p>While the BEAR Portal provides a convenient web-based access to a range of BlueBEAR services, you don\u2019t have to go via this portal, but can instead use the command line to access BlueBEAR through one of the multiple login nodes, available from the address bluebear.bham.ac.uk.</p> <p>Exactly how you do that will depend on the type of the operating system your computer uses; you can find detailed information about Accessing BlueBEAR using the command line from this link.</p> <p>The process of submitting and running jobs on the cluster is exactly the same whether using the BlueBEAR terminal via the \u201cClusters\u201d tab on the BlueBEAR portal or using the command line. To run a job with Slurm (BlueBEAR HPC scheduling system) you first need to prepare a job script and then submit using the command <code>sbatch</code>.</p> <p>In the first part of this workshop you have learned how to create <code>bash</code> scripts to automate FSL analyses; in order to turn these scripts into the job script for Slurm, you need to add a few additional command lines. This is illustrated in the example below: <code>bet_brainvols_job.sh</code></p> <pre><code>#!/bin/bash\n\n#SBATCH --qos=bbdefault\n#SBATCH --time=60\n#SBATCH --ntasks=5\n\nmodule purge; module load bluebear\nmodule load FSL/6.0.5.1-foss-2021a\n\nset -e\n\n# navigate to the folder containing the T1 scans\ncd rds/projects/c/chechlmy-chbh-mricn/xxx/recon\n\n# a for loop over participant data files\nfor participant_num in p01 p02 p03 p04 p05 p06 p07 p08 p09 p10 p11 p12 p13 p14 p15\ndo\n    # do the following...\n\n    # show the participant number\n    printf \"Participant ${participant_num}: \\n\"\n\n    # delete the old extracted brain image (we can see this\n    # happen in real time in the file explorer)\n    rm ${participant_num}/T1_brain.nii.gz\n\n    # extract the brain from the T1 image\n    bet ${participant_num}/T1.nii.gz ${participant_num}/T1_brain\n\n    # list the number of non-empty brain voxels\n    fslstats ${participant_num}/T1_brain -V\n\n# end the loop\ndone\n\n# ============================================================\n# END OF SCRIPT\n</code></pre> <p>This is a modified version of the <code>bet_brainvols.sh script</code>. If you compare the two, you will notice that after the <code>#!/bin/bash</code> line and before the start of the script loop, a few new lines have been added. These define the BlueBEAR resources required to complete the analysis job. </p> <p>These are explained below:</p> <ul> <li><code>#SBATCH \u2013qos=bbdefault</code> defines set of resources required, here default option)</li> <li><code>#SBATCH \u2013time=60</code> sets the run time</li> <li><code>#SBATCH \u2013ntasks=5</code> runs the job with five cores; by default, the amount of memory (RAM) that a job is allocated is (the <code>--ntasks</code> value) x 4096MB.</li> <li><code>module purge; module load bluebear</code> resets the environment to ensure that the script hasn\u2019t inherited anything from where it was submitted. This line is required and slurm will reject the script if it isn\u2019t present \u2013 it must be included before any other <code>module load</code> statements.</li> <li><code>module load FSL/6.0.5.1-foss-2021a</code> loads FSL</li> <li><code>set -e</code> - makes the script fail on first error </li> </ul> <p>The task of setting up resources is not always straightforward and will often require several test and error trials as if you do not request sufficient resources and time, your job might fail and if you request too many resources, it might be either rejected or be put in a long queue till require resources become available.</p> <p>You can find detailed guidelines re specifying required resources in the BlueBEAR documentation.</p> <p>The script above can be run on the cluster using the BlueBEAR terminal or the command line. To do the latter, you need to use the <code>sbatch</code> command which submits your job to the BlueBEAR scheduling system based on the requested resources. Once submitted it will run on the first available node(s) providing the resources you requested in your script. </p> <p>For example, to submit your BET job as in the example script above, in the BlueBEAR terminal you would type:</p> <p><code>sbatch bet_brainvols_job.sh</code></p> <p>The system will return a job number, for example:</p> <p><code>Submitted batch job XXXXXX</code></p> <p>You need this number to monitor or cancel your job.</p> <p>To monitor your job, you can use the <code>squeue</code> command by typing in the terminal:</p> <p><code>squeue -j XXXXXX</code></p> <p>This is a command for viewing the status of your jobs. It will display information including the job\u2019s ID and name, the user that submitted the job, time elapsed and the number of nodes being used.</p> <p>To cancel a queued or running job, you can use the <code>scancel</code> command by typing in the terminal:</p> <p><code>scancel XXXXXX</code></p>"},{"location":"workshop6/running-containers/#containers","title":"Containers","text":"<p>In previous workshops we have been using different pre-installed versions of FSL through different modules available on BEAR apps. Sometimes however, you might need a different (older or newer) version of FSL or a differently pre-compiled FSL. While you can request an up-to-date version of FSL - following the new release of the software it is added to BEAR apps (although it might take a while) - you cannot request to change how FSL is compiled on BEAR apps, as it would affect other BlueBEAR users or might not even be possible due to the BlueBEAR set up. </p> <p>Instead, you can install FSL within a controlled container and use this contained version instead of what\u2019s available on BEAR apps. </p> <p>BlueBEAR supports containerisation using Apptainer. Each BlueBEAR node has Apptainer installed, which means that the <code>apptainer</code> command is available without needing to first load a module. Apptainer can download images from any openly available container repository, for example Docker Hub or Neurodesk. Such sites will provide information re available software, software version and how to download a specific container.  </p> <p>Downloading containers</p> <p>Please do try to download any containers in this workshop!</p> <p>In the folder <code>scripts</code>, which you copied at the start of this workshop, you will find the subdirectory <code>containers</code> with two FSL containers (two different versions of FSL) downloaded from Neurodesk:</p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/module_data/scripts/containers</code></p> <p>The simplest way to use a container would be to load it and then use specific commands to run various FSL tools e.g., <code>bet</code>.</p> <p>For example, you would type in your terminal:  </p> <pre><code>apptainer shell fsl_6.0.7.4_20231005.sing\nbet T1.nii.gz T1_brain.nii.gz\n</code></pre> <p>You could also use a container in your job script to replace the BEAR apps version of FSL with the FSL container. To do that, you would need to add the line below to your script:</p> <p><code>apptainer exec [name of the container]</code></p> <p>Below is a very simple example of such a script, <code>example_job_fslcontainer.sh</code> which you can find inside the subdirectory <code>containers</code>:</p> <pre><code>#!/bin/bash\n#SBATCH --qos=bbdefault\n#SBATCH --time=30\n#SBATCH --ntasks=5\n\nmodule purge; module load bluebear\n\nset -e\n\napptainer exec fsl_6.0.7.4_20231005.sing bet T1.nii.gz T1_brain.nii.gz\n</code></pre>"},{"location":"workshop6/workshop6-intro/","title":"Workshop 6 - Scripts, containers and running analyses on the academic computing cluster","text":"<p>Welcome to the sixth workshop of the MRICN course! </p> <p>Prior workshops introduced you to running MRI analysis with various FSL tools by either using the FSL GUI or typing a simple command in the terminal. In this workshop we will look at how to automate FSL analyses by creating scripts. Subsequently, we will explore how to run FSL scripts more efficiently by submitting jobs to the cluster. The final part of this workshop will introduce how to use FSL containers rather than pre-installed versions of FSL using different modules available on BEAR apps.  </p> <p>Overview of Workshop 6</p> <p>Topics for this workshop include:</p> <ul> <li>Knowing how to automate neuroimaging analyses by <code>bash</code> scripting</li> <li>Understanding how to submit and managing analysis jobs on the BlueBEAR cluster using Slurm</li> <li>Working with FSL software containers using Apptainer as an alternative to pre-installed versions</li> </ul> <p>More information</p> <p>The BEAR Technical Docs provides guidance on submitting jobs to the cluster.</p> <p>The copy of this workshop notes can be found on Canvas 39058 - LM Magnetic Resonance Imaging in Cognitive Neuroscience in Week 06 workshop materials.</p>"},{"location":"workshop7/advanced-fmri-tools/","title":"Advanced fMRI analysis tools","text":"<p>The materials included in this worksheet follows on from the previous two FSL-specific fMRI workshops: in the first workshop you ran first level fMRI analysis, whilst in the second workshop you combined the data across scans within each participant (second level analysis) and across the group (third level analysis) in a couple of different ways. </p> <p>The information included in this page covers:</p> <ul> <li>The effect of changing threshold and correction methods</li> <li>T-tests vs F-tests</li> <li>Dealing with large motion artefacts (<code>fsl_motion_outliers</code>) </li> <li>How to extract ROI statistics from specific regions of interest (using FEATquery)</li> </ul> <p>As in the other materials we will not discuss in detail why you might choose certain parameters. The aim is to familiarise you with some of the available analysis tools. This worksheet will take you, step by step, through these analyses using the FSL tools. You are encouraged to read the pop-up help throughout (hold your mouse arrow over the FSL gui buttons and menus), refer to your lecture notes lectures or resource list readings. You can also find more information on the FSL website.</p> <p>If you have correctly followed the instructions from the previous workshops, you should by now have 29 first-level FEAT directories for the analysis of each scan of each participant:</p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1/</code> </p> <p>e.g. <code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1/p01_s1.feat</code></p> <p>(1 feat analysis directory for participant 1, 1 for participant 2, 3 for participant 5, and 2 for everyone else). </p> <p>At the second-level you should have 13 simple within-subjects fixed-effects directories:</p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/2/</code> </p> <p>e.g. <code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/ 2/p03.gfeat</code></p> <p>(one each for participants 3-15) </p> <p>You should also have a directory:</p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/2/level2all.gfeat</code> (for the all-participants-all-runs second level model)</p> <p>Finally, you should have 2 third-level group analysis folders:</p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/3/GroupAnalysis1.gfeat</code></p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/3/GroupAnalysis2.gfeat</code></p> <p>(containing the third level group analyses corresponding to the two different ways of combining data at third level)</p>"},{"location":"workshop7/advanced-fmri-tools/#testing-the-effects-of-different-thresholds","title":"Testing the effects of different thresholds","text":"<p>Try as many of the suggestions below as you have time for. I suggest trying 1 or 2 at the start of the session and returning to these later if you have time, or outside the workshop. (You can load up an existing model by running FEAT and using the 'Load' button. The model file will be called <code>design.fsf</code> and can be found in the <code>.feat</code> or <code>.gfeat</code> directory of an existing analysis folder.)</p> <ol> <li> <p>Ordinary Least Squares (OLS) vs FLAME: Repeat the third level group analyses from FSL Workshop 5, but on the 'Stats' tab select 'Mixed Effects: Simple OLS'</p> </li> <li> <p>Different correction for multiple comparisons: Repeat the third level group analyses from FSL Workshop 5, but on the 'Post-Stats' tab for 'Thresholding', use the pull down menu to select 'Voxel correction'. </p> </li> <li> <p>Different thresholds and correction for multiple comparisons: Repeat the third level group analyses from FSL Workshop 5, but on the 'Post-Stats' tab use 'Cluster Thresholding' but choose a different z-threshold.</p> </li> </ol> <p>Examining the results</p> <p>Look at the results from each method of correction. Use both the webpage output and FSLeyes to look at your data. Find the regions of significant activation. Try looking at a time series (see the 'Data Visualisation' workshop notes for help).</p>"},{"location":"workshop7/advanced-fmri-tools/#t-test-vs-f-test","title":"T-test vs F-test","text":"<p>Start up a FEAT GUI, by opening a new terminal window and typing <code>fsl &amp;</code>. Click on the 'Feat' button [or type <code>Feat &amp;</code>  in the terminal window to directly open the FEAT GUI].</p> <p>Load up one of the third level analyses run in the last workshop (e.g., 'GroupAnalysis1' or 'GroupAnalysis2').</p> <p>Now follow the instructions below:</p> <p>Data Tab</p> <ul> <li>Change the 'Output' directory name to be something different, for example 'GroupAnalysis1_Ftest'</li> </ul> <p> </p> <p>Stats Tab</p> <ul> <li> <p>Click on the 'Full model setup' button and select the 'Contrasts and F-tests' tab</p> </li> <li> <p>Change the number of 'F-tests' from 0 to 1. When you do this, the interface will update and a check box will appear on the right hand side of the group mean contrast. Select the check box (toggle it from off to on).</p> </li> <li> <p>Click the 'Done' button, check and dismiss the pop-up window and press the 'Go' button</p> </li> </ul> <p> </p> <p>Differences between t-test and F-test images</p> <p>Once it has run inspect the resulting output in a browser. How does the rendered stats image for the F-test (the <code>zfstat</code> image) differ from the t-stat image? Why are they different?</p>"},{"location":"workshop7/advanced-fmri-tools/#extracting-information-from-regions-of-interest-rois-using-featquery","title":"Extracting information from Regions of Interest (ROIs) using FEATquery","text":"<p>FEATquery is an FSL tool which allows you to explore FEAT results by extracting information from regions of interests within specific (MNI) coordinates or using a mask. </p> <p>In the examples below we will get basic stats from two pre-prepared Regions of Interest (ROIs) using the first level models that you have run. You can also make your own ROIs using FSLeyes (you should remember how to create ROI masks from previous workshops).</p> <p>To start FEATquery, you need to load FSL (see previous workshops), and either - in a terminal - type <code>Featquery &amp;</code> or on the FSL GUI click the button on the bottom right labelled 'Misc' and select the menu option 'Featquery'.</p> <p>In any case, when FEATQuery is open, following the instructions below:</p> <p> </p> <p>Input FEAT directories</p> <ul> <li>Keep the 'Number of Feat Directories' set to 1. </li> <li>Click the 'Select' button next to it and choose one of your first-level analysis <code>.feat</code> directories (e.g. <code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1/p01_s1.feat</code>) and then click the 'OK' button </li> </ul> <p>Stats images of interest</p> <p>Once you have done that the GUI interface will update and you will see a list of possible statistics. </p> <ul> <li>Click the checkbox next to <code>stats/cope 1</code> and <code>stats/zstat 1</code> (and any others you would like to see summarised).</li> </ul> <p>Input ROI selection</p> <p>For the 'Mask image' entry select either one of the prepared masks.</p> <p>Either:</p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/module_data/masks/V1.nii.gz</code></p> <p>or</p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/module_data/masks/Parietal.nii.gz</code></p> <p>Output options</p> <ul> <li>Click the checkbox next to 'Convert PE/Cope values to %'</li> <li>Make sure the check box next to 'Popup results in web browser' is checked on</li> <li>Change the 'Featquery output directory name' from the default to something meaningful and related to the mask name e.g. 'V1'</li> </ul> <p>When ready, click the 'Go' button!</p> <p>Examining the FEATquery output</p> <p>Inspect the results by opening <code>report.html</code> inside the 'V1' folder. Do they make sense?</p>"},{"location":"workshop7/higher-level-analysis/","title":"Running the higher-level fMRI analysis","text":"<p>If you have correctly followed the instructions from the previous workshop, you should now have 29 FEAT directories arising from each fMRI scan of each participant, e.g.,</p> <pre><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1/p01_s1.feat  \u2190 Participant 1 scan 1\n/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1/p02_s1.feat  \u2190 Participant 2 scan 1\n/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1/p03_s1.feat  \u2190 Participant 3 scan 1\n/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1/p03_s2.feat  \u2190 Participant 3 scan 2\n(\u2026)\n/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1/p15_s2.feat  \u2190 Participant 15 scan 2\n</code></pre> <p>(where XXX = your particular login (ADF) username).</p> <p>For participants 1 and 2 you should have only one FEAT directory. For participants 3-4 and 6-15 you should have 2 FEAT directories. For participant 5 you should have 3 FEAT directories. You should therefore have 29 complete first level feat directories.</p> <p>If you haven\u2019t done so already, please check that the output of each and all of these first level analyses looks ok either through the FEAT Report or through FSLeyes. If you would like to use the FEAT Report, select the report (called <code>report.html</code>) from within each FEAT directory from your analysis, e.g.,: </p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1/p03_s1.feat/report.html</code></p> <p>and either open it in a new tab or in a new window. </p> <p> </p> <p>Selecting a participant's first-level FEAT output (left) and examining the FEAT Report (right).</p> <p>Check all of the reports and click on each of the sub-pages in turn:</p> <ol> <li>Check that the analysis ran ok by checking the 'Log' page. </li> <li>Check that the 'Registration' looks ok. </li> <li>Check how much they moved on the 'Pre-Stats' page. </li> <li>Have a quick look at the 'Post-Stats' page. </li> </ol> <p>Motion correction</p> <p>Participant 5 was scanned three times. In one of these scans they moved suddenly. Use the motion correction results to decide which scan this was. We will ignore this scan for the rest of this workshop. If you have not completed this stage of the analysis, you should do this now before continuing on. Refer to the worksheet from previous workshops.</p>"},{"location":"workshop7/higher-level-analysis/#second-level-analysis-averaging-across-runs-within-participants","title":"Second-level analysis - averaging across runs within participants","text":"<p>Most of our participants did the experiment twice \u2013 a repeated measurement. How do we model this data? There are different ways we can do this. </p> <p>The simplest way is to combine the data within participant before generating group statistics across the entire group of participants. This corresponds with what you might do if you are analysing data as you go along.  Here, we will average their data over the two fMRI runs so that we can benefit from the extra power. Participant 5 did the experiment 3 times.  For the moment, for this participant, we will choose only the two scans where they moved less for further analysis. </p> <p>Choose one of the participants that did the experiment twice (not participant 1 or 2), such as participant 3. Open a terminal and load FSL:</p> <pre><code>module load bear-apps/2022b\nmodule load FSL/6.0.7.6\nsource $FSLDIR/etc/fslconf/fsl.sh\n</code></pre> <p>Then navigate to the folder where your first-level directories are located and open FEAT:</p> <pre><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1\nFeat &amp;\n</code></pre> <p>At the top left of the GUI you will see a pull down menu labelled 'First-level analysis'. Click here to pull down this menu and choose 'Higher-level analysis' The options available will change.</p> <p>Now fill out the tabs as below:</p> Stats <p>Choose 'Fixed effects' from the pull down menu at the top.</p> <p>It is necessary to select this now in order to reduce the number of inputs on the 'Data' tab to be only 2 (the default for all other higher level model types is a minimum no of 3 inputs).  Note that choosing 'Fixed effects' will ignore cross scan variance, which is fine to do here because these are scans from the same person at the same time. </p> Data <p> </p> <ul> <li>On the data tab select 'Inputs are lower-level FEAT directories\u2019. </li> <li>Make sure the 'Number of inputs' is set as 2. </li> <li>Click on the 'Select FEAT directories' button. </li> <li>In the form that appears fill in the top line with the <code>.feat</code> directory from the first scan and the second line with the <code>.feat</code> directory from the second scan. (Hint: you can use the little folder icon to browse)</li> </ul> <p> </p> <p></p> <p>Once you have selected the FEAT directories for input, the GUI will update to show what COPE files (contrasts) are available for analysis. </p> <ul> <li>There is only one COPE (contrast) in this data so leave the 'Use lower level copes' line as it is (with the single COPE 1 selected).</li> <li>You should now enter a name for the output analysis directory. Use the Output directory option to choose a directory to put the results in. </li> </ul> <p>The naming scheme here (as with your raw data directories, reconstruction directories and your first level analysis directories) needs to be clear and logical. </p> <p>It is therefore sensible to use a directory structure like:</p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/2</code> (directory for all FEAT 2nd level analyses)</p> <p>For example, you can then put the second level analysis for participant 3 in the subdirectory and so on.</p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/2/p03</code></p> Stats (again) <ul> <li>Click the Full model setup button</li> <li>On the EVs tab put 1 in all rows of the EV1 column (see picture below) [this averages both inputs together]</li> <li>On the Contrasts and F-tests tab, check that there is one contrast, titled 'group mean', weighted 1 for EV1 (see picture below)</li> </ul> <p> </p> <p>Click the 'Done' button. This will produce a schematic of your design. Check it makes sense and close its window.</p> Post-Stats <p>As with the first-level analysis, select the 'Thresholding' pull down option and type 'Uncorrected' and leave the P-threshold value at p&lt;0.05. </p> <p>Thresholding and processing time</p> <p>Note this is not the correct thresholding that you will want at the final (third stage) of processing (where you will probably want Cluster thresholding) but at the first and second level stages it will speed up the processing per person. </p> <p>Click the 'Go' button. Repeat for the other participants. </p> <p>In the web browser look at the results in the FEAT report (i.e., by opening the <code>report.html</code>). Note that the output folder is called <code>p03.gfeat</code>. You will have to click on the link marked 'Results' and then on the link labelled 'Lower-level contrast 1 (vision)' on that page and then on the 'Post-stats' link. </p> <p>Comparing second-level and first-level results</p> <p>Are the results better than for just one scan from <code>p03</code>? </p>"},{"location":"workshop7/higher-level-analysis/#third-level-analysis-combining-data-across-participants-from-the-second-level","title":"Third-level analysis - combining data across participants from the second level","text":"<p>In FSL, the procedure for setting up an analysis across participants is very similar to averaging within a participant. The main difference is that we specifically need to set up the analysis to model the inter-subject variance. This allows us to generalise beyond our specific group of participants and to interpret the results as being typical of the wider population.</p> <p>In this demonstration experiment, 12 participants did the scan twice, 1 was scanned three times, and 2 did the scan only once. (Note that it should be rather obvious that this is not an ideal design for a real experiment). In our case, we have averaged within participants and now we will combine these second level analyses with the first level analyses from those participants who were only scanned once.</p> <p>Close FEAT if you still have it open. Then open it again by typing <code>Feat &amp;</code></p> <p>Don't close the terminal if you don't have to!</p> <p>Please note that if you close the terminal here you will first need to load FSL again and navigate back to your folder!</p> <p>At the top left of the GUI select the pull down menu labelled 'First-level analysis\u2019 and choose 'Higher-level analysis'.</p> <p>Now complete each of the tabs as described below:</p> Data <p>We have 13 participants who did the experiment at least twice (who we combined via a second-level analysis) and 2 who did the experiment only once (who we only analysed at first-level).  Therefore, for this next (third level) analysis we will need to combine over first and second level analyses.</p> <ul> <li>On the top button check that 'Inputs are lower-level FEAT directories' is selected.</li> <li>Change 'Number of inputs' to be 15</li> <li>Press the button marked 'Select Feat Directories'</li> </ul> <p>In the dialogue that appears you need to add in the path to the <code>.feat</code> directory for each person. For the first-level analyses this is something like:</p> <pre><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1/p01_s1.feat\n/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/1/p02_s1.feat\n</code></pre> <p>For the participants where you did a second level analysis this will be:</p> <pre><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/2/p03.gfeat/cope1.feat\n/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/2/p04.gfeat/cope1.feat\n/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/2/p05.gfeat/cope1.feat\n...\n/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/2/p15.gfeat/cope1.feat\n</code></pre> <p>Subdirectory location</p> <p>Note that the actual feat subdirectories of interest for the second-level analyses are hidden inside the <code>.gfeat</code> directories. </p> <p>You should now enter a name for the output analysis directory. Use the 'Output' directory entry to choose a directory to put the results in.  As with your second-level analyses, the naming scheme here needs to be clear and logical.</p> <p>It is therefore sensible to use a structure like:</p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/3</code> (directory for all FEAT 3rd level analyses)</p> <p>For example, you can then put this current 3rd level output in the subdirectory:</p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/3/GroupAnalysis1</code></p> Stats <ul> <li>On the Top Pull down menu, Select 'Mixed Effects: Flame 1'. (Remember, mixed effects means that we can generalise beyond our participants)</li> <li>Click on the 'Full model setup' button</li> <li>For all the inputs, set 'EV1' to 1 (see the left picture below) and name it 'vision\u2019</li> <li>On the Contrasts and F-tests tab set EV1 to 1 and name the contrast 'group mean' (as on the right image below)</li> <li>Click the Done button. Check and dismiss the pop-up window</li> </ul> <p> </p> Post-Stats <p>Accept the defaults.   </p> <p>Now click the 'Go' button!</p> <p>When the group analysis has finished, check through the output (using the FEAT Report) and try to work out what each page means. For example, the 'Registration Summary' page will highlight areas where the registrations are not aligned between scans/participants.  A few missing voxels are ok, but any more than that is a problem as you won't get results from areas where there are missing data.</p>"},{"location":"workshop7/higher-level-analysis/#second-level-analysis-the-all-in-one-method","title":"Second-level analysis - the 'all-in-one' method","text":"<p>A more complicated modelling approach at the second-level is to use one single second-level model (instead of separate models per participant) which incorporates all of the information available about participants and runs.</p> <p>This corresponds with what you might do if you are analysing all the data after it has been collected, all in one go.  Depending on the design of the particular experiment, this has the potential to be an improved approach as it allows a better estimate of both the between-run and the between-subject variance.</p> <p>Close FEAT if you still have it open. Then open it again by clicking on the FEAT button in the FSL GUI (or type <code>Feat &amp;</code> in the terminal window to directly open the FEAT GUI).</p> <ul> <li>At the top left of the GUI select the pull down menu labelled 'First-level analysis'.</li> <li>Click here to pull down this menu and choose 'Higher-level analysis'</li> </ul> <p>Now complete the tabs following the instructions below:</p> Data <ul> <li>On the 'Data' tab select 'Inputs are lower-level FEAT directories'</li> <li>Change \u201cNumber of inputs\u201d to be 28</li> </ul> <p>(Note that there are only 28 inputs here as we are going to use all of the first level feat dirs as inputs except for the worst run of the three that participant 5 did).</p> <ul> <li> <p>Click on Select FEAT directories button. </p> </li> <li> <p>In the form that appears fill in the lines with the <code>.feat</code> directory from the first level analysis from the first participant (scan 1), followed by the second participant (scan 1), followed by the third participant (scan 1, and then scan 2) and so on down to participant 15, scan 2. It is important that you enter these in strict logical order, starting with <code>p01</code> as in this example below:</p> </li> </ul> <p> </p> <ul> <li>When this is done, hit the OK button. </li> </ul> <p>As always you should now enter a name for the output analysis directory. Use the 'Output' directory entry to choose a directory to put the results in. </p> <p>As this is a second-level model it should go under the directory:</p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/2</code> (directory for all FEAT 2nd level analyses)</p> <p>and should be meaningfully named. For example, you could call it:</p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/2level2all</code></p> Stats <ul> <li>Choose 'Fixed Effects' from the pull down menu at the top.</li> <li>Click the 'Full model setup' button.</li> <li>On the 'EVs' tab set the \u201cNumber of main EVs\u201d to be 15 (This corresponds to the 15 participants in the experiment).</li> <li>Going across the columns, label each EV as participant <code>p01</code> to <code>p15</code> as per below.</li> <li>Going down the columns, for each participant, put a 1 in any row of the EV column for which the participant has a matching scan. Each EV should pick out the scans that correspond to a particular participant. There are 28 rows, corresponding to the 28 first level feat inputs. There should therefore be 28 ones in the design matrix overall (as below).</li> </ul> <p> </p> <p>On the 'Contrasts and F-tests' tab, we also need 15 contrasts to represent the 15 subject means. Fill in the tab as below: </p> <p> </p> <p>Check boxes in FSL</p> <p>In the older versions of FSL after selecting an option you will see a yellow checkbox, however in the newer versions of FSL such as the one we are using, the checkbox is yellow to start with, and after selecting option you will see a tick \u2714\ufe0f inside the yellow checkbox.</p> <p>Click the Done button. This will produce a schematic of your new second level design. </p> <p> </p> <p>Check it makes sense, and that you understand what it is showing, then close its window.</p> Post-Stats <p>Accept the defaults.</p> <p>Now click the Go button!</p> <p>Wait for the analysis to complete and then look at the results. Note that the output folder is called <code>level2all.gfeat</code> if you named it as above. You will have to click on the link marked 'Results' and then on the link labelled 'Lower-level contrast 1 (vision)' on that page and then on the 'Post-stats' link.  This will then show you a contrast (rendered stats image) for each of the participants.</p> <p>Comparing the second-level results</p> <p>Are the results from this bigger model better than the simple fixed effects model for the same participant? For example, with participant <code>p09</code>? </p>"},{"location":"workshop7/higher-level-analysis/#third-level-analysis-combining-participant-data-from-the-all-in-one-second-level","title":"Third-level analysis - combining participant data from the 'all-in-one' second-level","text":"<p>We can now estimate the mean group effect by combining across participants from the better second-level analysis we have just calculated above.</p> <ul> <li>First, close FEAT if you still have it open, then open it again by clicking on the FEAT button in the FSL GUI (or typing <code>Feat &amp;</code> in the terminal window to directly open the FEAT GUI)</li> <li>Then at the top left of the GUI select the pull down menu labelled 'First-level analysis' and choose 'Higher-level analysis\u2019</li> </ul> Data <p>In the second-level analysis we just performed we combined data over both participants and runs, effectively collapsing across runs, and the output analyses were then the summary data for each of the 15 participants. Therefore, for this next (third-level) analysis we will again need to combine over the 15 participants.</p> <p>To do this, follow the steps below:</p> <ul> <li>On the top button change the default setting from 'Inputs are lower-level FEAT directories' and select 'Inputs are 3D cope images from FEAT directories'.</li> <li>Change 'Number of inputs' to be 15 (because we have 15 participant data sets we want to combine)</li> <li>Press the button marked 'Select cope images'</li> <li>In the dialogue that appears you need to add in the path to the COPE image for each of the participants in the second level analysis we have just performed. </li> </ul> <p>If you have used the correct naming convention above, this is:</p> <pre><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/2/level2all.gfeat/cope1.feat/stats/cope1.nii.gz \n/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/2/level2all.gfeat/cope1.feat/stats/cope2.nii.gz \n...\n/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/2/level2all.gfeat/cope1.feat/stats/cope15.nii.gz\n</code></pre> <p>As before, you should now enter a name for the output analysis directory. As this is a third level model it should go under the directory:</p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/3</code> (for all FEAT 3rd level analyses)</p> <p>and should be meaningfully named. For example, you can then put this current 3rd level output in the subdirectory:</p> <p><code>/rds/projects/c/chechlmy-chbh-mricn/xxx/feat/3/GroupAnalysis2</code></p> Stats <ul> <li>On the top pull down menu, Select 'Mixed Effects: Flame 1'</li> <li>Click on the 'Full model setup' button (Remember, mixed effects means that we can generalise beyond our participants)</li> <li>For all the inputs, set EV1 to 1 (see picture below) and name it 'vision'.</li> </ul> <p> </p> <ul> <li>On the 'Contrasts and F-tests' tab set EV1 to 1 and name the contrast 'group mean'</li> <li>Click the 'Done' button</li> <li>Check and dismiss the pop-up window</li> </ul> Post-Stats <ul> <li>Accept the defaults</li> </ul> <p>Now click the 'Go' button!</p> <p>Comparing the third-level results</p> <p>Check through the output when the group analysis has finished. Is the result better than the simple third level analysis above?</p> <p>If you followed the instructions in workshop materials, you should be able to replicate and see the results as above in the <code>report.html</code> file inside the respective third level analysis folders ('GroupAnalysis1' and 'GroupAnalysis2').</p> <p> </p> <p>As always, help and further information is also available on the relevant section of the FSL Wiki.</p>"},{"location":"workshop7/workshop7-intro/","title":"Workshop 7 - Higher-level fMRI analysis","text":"<p>Welcome to the seventh workshop of the MRICN course! </p> <p>Prior lectures introduced you to the basic concepts and theory behind higher-level fMRI analysis, including multi-session analysis and the general linear model (GLM). In this workshop you will be learning practical skills in how to run higher level fMRI analysis using FSL tools.</p> <p>This workshop follows on from the workshop on first-level fMRI analysis. In that workshop you analysed the first level data for 2 participants and at the end of the workshop you were asked to analyse the rest of the scans in the data set. Participants 1-2 had one fMRI experiment run each, participants 3-4 and 6-15 had 2 runs each and participant 5 had 3 runs, so there are a total of 29 runs from the 15 participants.</p> <p>We will now combine the fMRI data across runs and participants in our second and third-level analyses.</p> <p>Overview of Workshop 6</p> <p>Topics for this workshop include:</p> <ul> <li>How to combine scans when one participant has been scanned multiple-times (e.g., twice) within the same experiment</li> <li>How to combine scans across participants</li> <li>How to set up different higher-level (second and third-level) fMRI analyses</li> </ul> <p>As in the other workshops we will not discuss in detail why you might choose certain parameters. The aim of this workshop is to familiarise you with some of the available analysis tools.  You are encouraged to read the pop-up help throughout (hold your mouse arrow over FSL GUI buttons and menus when setting your FEAT design), refer to your lecture notes lectures or resource list readings.</p> <p>More information</p> <p>As always, you can also find more information on running higher-level fMRI analyses on the FSL website.</p> <p>The copy of this workshop notes can be found on Canvas 39058 - LM Magnetic Resonance Imaging in Cognitive Neuroscience in Week 07 workshop materials.</p>"},{"location":"workshop8/functional-connectivity/","title":"Functional connectivity analysis of resting-state fMRI data using FSL","text":"<p>This workshop is based upon the excellent FSL fMRI Resting State Seed-based Connectivity tutorial by Dianne Paterson at the University of Arizona, which has been adapted to run on the BEAR systems at the University of Birmingham, with some additional content covering Neurosynth.</p> <p>We will run a group-level functional connectivity analysis on resting-state fMRI data of three participants, specifically examining the functional connectivity of the posterior cingulate cortex (PCC), a region of the default mode network (DMN) that is commonly found to be active in resting-state data. </p> <p>Overview of Workshop 8</p> <p>To do this, we will:</p> <ul> <li>extract a mean-timeseries for a PCC seed region for each participant,</li> <li>run single-subject level analyses, one manually and bash scripting the other two, </li> <li>run a group-level analysis using the single-level results </li> <li>figure out which brain regions our active voxels are in, using atlases in FSL, and Neurosynth.</li> </ul>"},{"location":"workshop8/functional-connectivity/#preparing-the-data","title":"Preparing the data","text":"<p>Navigate to your shared directory within the MRICN folder and copy the data over:</p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx\ncp -r /rds/projects/c/chechlmy-chbh-mricn/aamir_test/SBC .\ncd SBC\nls\n</code></pre> <p>You should now see the following:</p> <pre><code>sub1 sub2 sub3\n</code></pre> <p>Each of the folders has a single resting-state scan, called <code>sub1.nii.gz</code>,<code>sub2.nii.gz</code> and <code>sub3.nii.gz</code> respectively. </p> <p>We will now create our seed region for the PCC. To do this, firstly load FSL and <code>fsleyes</code> in the terminal by running: </p> <pre><code>module load FSL/6.0.5.1-foss-2021a\nmodule load FSLeyes/1.3.3-foss-2021a\n</code></pre> <p>Check that we are in the correct directory (<code>blah/your_username/SBC</code>):</p> <pre><code>pwd\n</code></pre> <p>and create a new directory called <code>seed</code>:</p> <pre><code>mkdir seed\n</code></pre> <p>Now when you run <code>ls</code> you should see:</p> <pre><code>seed sub1 sub2 sub3\n</code></pre> <p>Lets open FSLeyes:</p> <pre><code>fsleyes &amp;\n</code></pre> Creating the PCC mask in FSLeyes <p>We need to open the standard MNI template brain, select the PCC and make a mask.</p> <p>Here are the following steps: </p> <ol> <li>Navigate to the top menu and click on <code>File \u279c Add standard</code> and select <code>MNI152_T1_2mm_brain.nii.gz</code>.</li> <li>When the image is open, click on <code>Settings \u279c Ortho View 1 \u279c Atlases</code>. An atlas panel then opens on the bottom section.</li> <li>Select <code>Atlas information</code> (if it already hasn't loaded).</li> <li>Ensure Harvard-Oxford Cortical Structural Atlas is selected.</li> <li>Go into 'Atlas search' and type <code>cing</code> in the search box. Check the Cingulate Gyrus, posterior division (lower right) so that it is overlaid on the standard brain. (The full name may be obscured, but you can always check which region you have loaded by looking at the panel on the bottom right).</li> </ol> <p> </p> <p> At this point, your window should look something like this: </p> <p> </p> <p> To save the seed, click the save symbol which is the first of three icons on the bottom left of the window.  </p> <p> </p> <p>The window that opens up should be your project SBC directory. Open into the <code>seed</code> folder and save your seed as <code>PCC</code>. </p> Extracting the time-series <p>We now need to binarise the seed and to extract the mean timeseries. To do this, leaving FSLeyes open, go into your terminal (you may have to press Enter if some text about <code>dc.DrawText</code> is there) and type:</p> <pre><code>cd seed\nfslmaths PCC -thr 0.1 -bin PCC_bin\n</code></pre> <p>In FSLeyes now click File \u279c Add from file, and select <code>PCC_bin</code> to compare <code>PCC.nii.gz</code> (before binarization) and <code>PCC_bin.nii.gz</code> (after binarization). You should note that the signal values are all 1.0 for the binarized PCC.</p> <p> </p> <p>You can now close FSLeyes.</p> <p>For each subject, you want to extract the average time series from the region defined by the PCC mask. To calculate this value for <code>sub1</code>, do the following: </p> <pre><code>cd ../sub1\nfslmeants -i sub1 -o sub1_PCC.txt -m ../seed/PCC_bin\n</code></pre> <p>This will generate a file within the <code>sub1</code> folder called <code>sub1_PCC.txt</code>. </p> <p>We can have a look at the contents by running <code>cat sub1_PCC.txt</code>. The terminal will print out a list of numbers with the last five being:</p> <pre><code>20014.25528\n20014.919\n20010.17317\n20030.02886\n20066.05141\n</code></pre> <p>This is the mean level of 'activity' for the PCC at each time-point.</p> <p>Now let's repeat this for the other two subjects. </p> <pre><code>cd ../sub2\nfslmeants -i sub2 -o sub2_PCC.txt -m ../seed/PCC_bin\ncd ../sub3\nfslmeants -i sub3 -o sub3_PCC.txt -m ../seed/PCC_bin\n</code></pre> <p>Now if you go back to the SBC directory and list all of the files within the subject folders:</p> <pre><code>cd ..\nls -R\n</code></pre> <p>You should see the following: </p> <p> </p> <p>This is all we need to run the subject and group-level analyses using FEAT.</p>"},{"location":"workshop8/functional-connectivity/#running-the-feat-analyses","title":"Running the FEAT analyses","text":""},{"location":"workshop8/functional-connectivity/#single-subject-analysis","title":"Single-subject analysisExamining the FEAT outputScripting the other two subjects","text":"<p>Close your terminal, open another one, move to your <code>SBC</code> folder, load FSL and open FEAT: </p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/SBC\nmodule load bear-apps/2022b\nmodule load FSL/6.0.7.6\nsource $FSLDIR/etc/fslconf/fsl.sh\nFeat &amp;\n</code></pre> <p>We will run the first-level analysis for <code>sub1</code>. Set-up the following settings in the respective tabs:</p> <p>Data</p> <p>Number of inputs:</p> <ul> <li>Click 'Select 4D data', then click the folder icon, go into the <code>sub1</code> folder and choose <code>sub1.nii.gz</code>. Click OK. You will see a box saying that the 'Input file has a TR of 1...' this is fine, just click OK again.</li> </ul> <p>Output directory: </p> <ul> <li>Click into the <code>sub1</code> folder and click OK. Nothing will be in the right hand column, but that is because there are no folders within <code>sub1</code>. We will create our <code>.feat</code> folder within <code>sub1</code>. </li> </ul> <p>This is what your data tab should look like (with the input data opened for show).</p> <p> </p> <p>Pre-stats</p> <p>The data has already been pre-processed, so just set 'Motion correction' to 'None' and uncheck BET. Your pre-stats should look like this: </p> <p> </p> <p>Registration</p> <p>Nothing needs to be changed here. </p> <p>Stats</p> <p>Click on 'Full Model Setup' and do the following: </p> <ol> <li>Keep the 'Number of original EVs' as 1.</li> <li>Type PCC for the 'EV' name.</li> <li>Select 'Custom (1 entry per volume)' for the 'Basic' shape. Click into the <code>sub1</code> folder and select <code>sub1_PCC.txt</code>. This is the mean time series of the PCC for sub-001 and is the statistical regressor in our GLM model. This is different from analyses of task-based data which will usually have an <code>events.tsv</code> file with the onset times for each regressor of interest.</li> <li>Select 'None' for 'Convolution', and uncheck both 'Add temporal derivate' and 'Apply temporal filtering'. </li> </ol> <p>What are we doing specifically?</p> <p>The first-level analysis will subsequently identify brain voxels that show a significant correlation with the seed (PCC) time series data.</p> <p>Your window should look like this: </p> <p> </p> <p>In the same General Linear Model window, click the 'Contrast &amp; F-tests' tab, type PCC in the title, and click 'Done'. </p> <p>A blue and red design matrix will then be displayed. You can close it.</p> <p>Post-stats </p> <p>Nothing needs to be changed here.</p> <p>You are ready to run the first-level analysis. Click 'Go' to run. On BEAR, this should only take a few minutes. </p> <p>To actually examine the output, go to the BEAR Portal and at the menu bar select <code>Files \u279c /rds/projects/c/chechlmy-chbh-mricn/</code> </p> <p> </p> <p>  Then go into <code>SBC/sub1.feat</code>, select <code>report.html</code> and click 'View' (top left of the window). Navigate to the 'Post-stats' tab and examine the outputs. It should look like this: </p> <p> </p> <p>We can now run the second and third subjects. As we only have three subjects, we could manually run the other two by just changing three things: </p> <ol> <li>The fMRI data path</li> <li>The output directory</li> <li>The <code>sub_PCC.txt</code> path</li> </ol> <p>Whilst it would probably be quicker to do it manually in this case, it is not practical in other instances (e.g., more subjects, subjects with different number of scans etc.). So, instead we will be scripting the first level FEAT analyses for the other two subjects.</p> <p>The importance of scripting</p> <p>Scripting analyses may seem challenging at first, but it is an essential skill of modern neuroimaging research. It enables you to automate repetitive processing steps, dramatically reduces the chance of human error, and ensures your research is reproducible.</p> <p>To do this, go back into your terminal, you don't need to open a new terminal or close FEAT.</p> <p>The setup for each analysis is saved as a specific file, the <code>design.fsf</code> file within the FEAT output directory. We can see this by opening the <code>design.fsf</code> file for <code>sub1</code>:</p> <pre><code>pwd # make sure you are in your SBC directory e.g., blah/xxx/SBC\ncd sub1.feat\ncat design.fsf\n</code></pre> <p>FEAT acts as a large 'function' with its many variables corresponding to the options that we choose when setting up in the GUI. We just need to change three of these (the three mentioned above). In the <code>design.fsf</code> file this corresponds to:</p> <pre><code>set fmri(outputdir) \"/rds/projects/c/chechlmy-chbh-mricn/xxx/SBC/sub1\"\nset feat_files(1) \"/rds/projects/c/chechlmy-chbh-mricn/xxx/SBC/sub1/sub1/\"\nset fmri(custom1) \"/rds/projects/c/chechlmy-chbh-mricn/xxx/SBC/sub1/sub1_PCC.txt\"\n</code></pre> <p>To run the script, please copy the <code>run_feat.sh</code> script into your own <code>SBC</code> directory:</p> <pre><code>cd ..\npwd # make sure you are in your SBC directory\ncp /rds/projects/c/chechlmy-chbh-mricn/axs2210/SBC/run_feat.sh .\n</code></pre> <p>Viewing the script</p> <p>If you would like, you can have a look at the script yourself by typing <code>cat run_bash.sh</code></p> <p>The first line <code>#!/bin/bash</code> is always needed to run <code>bash</code> scripts. The rest of the code just replaces the 3 things we wanted to change for the defined subjects, <code>sub2</code> and <code>sub3</code>.</p> <p>Run the code (from your SBC directory) by typing <code>bash run_feat.sh</code>. (It will ask you for your University account name, this is your ADF username (axs2210 for me)).</p> <p>The script should take about 5-10 minutes to run on BEAR.</p> <p>After it has finished running, have a look at the <code>report.html</code> file for both directories, they should look like this:</p> <p>sub2</p> <p>sub3</p>"},{"location":"workshop8/functional-connectivity/#group-level-analysis","title":"Group-level analysisExamining the output","text":"<p>Ok, so now that we have our FEAT directories for all three subjects, we can run the group level analysis. Close FEAT and open a new FEAT by running <code>Feat &amp;</code> in your <code>SBC</code> directory. </p> <p>Here are instructions on how to setup the group-level FEAT:</p> <p>Data </p> <ol> <li>Change 'First-level analysis' to 'Higher-level analysis'</li> <li>Keep the default option for 'Inputs are lower-level FEAT directories'.</li> <li>Keep the 'Number of inputs' as 3.</li> <li>Click the 'Select FEAT directories'. Click the yellow folder on the right to select the FEAT folder that you had generated from each first-level analysis.</li> </ol> <p>Your window should look like this (before closing the 'Input' window):</p> <p> </p> <p></p> <p>\u00a0\u00a0\u00a0\u00a05. Keep 'Use lower-level COPEs' ticked.</p> <p>\u00a0\u00a0\u00a0\u00a06. In 'Output directory' stay in your current directory (SBC), and in the bottom bar, type in <code>PCC_group</code> at the end of the file path. </p> <p>Don't worry about it being empty, FSL will fill out the file path for us. </p> <p>If you click the folder again, it should look similar to this (with your ADF username instead of <code>axs2210</code>): </p> <p> </p> <p> Stats</p> <ol> <li>Leave the 'Mixed effects: FLAME 1' and click 'Full model setup'. </li> <li>In the 'General Linear Model' window, name the model 'PCC' and make sure the 'EVs' are all 1s. </li> </ol> <p>The interface should look like this:</p> <p> </p> <p>After that, click 'Done' and close the GLM design matrix that pops up (you don't need to change anything in the 'Contrasts and F-tests' tab).</p> <p>Post-stats</p> <ol> <li>Change the Z-threshold from 3.1 to 2.3.</li> </ol> <p>Lowering our statistical threshold</p> <p>Why do you think we are lowering this to 2.3 in our analysis instead of keeping it at 3.1? The reason is because we only have three subjects, we want to be relatively lenient with our threshold value, otherwise we might not see any activation at all!  For group-level analyses with more subjects, we would be more strict.</p> <p>Click 'Go' to run! </p> <p>This should only take about 2-3 minutes. </p> <p>While this is running, you can load the <code>report.html</code> through the file browser as you did for the individual subjects. </p> <p>Click on the 'Results' tab, and then on 'Lower-level contrast 1 (PCC)'. When the analysis has finished, your results should look like this: </p> <p> </p> <p>These are voxels demonstrating significant functional connectivity with the PCC at a group-level (Z &gt; 2.3).</p> <p>So, we have just ran our group-level analysis. Let's have a closer look at the outputted data. </p> <p>Close FEAT and your terminal, open a new terminal, go to your <code>SBC</code> directory and open FSLeyes: </p> <pre><code>cd /rds/projects/c/chechlmy-chbh-mricn/xxx/SBC\nmodule load FSL/6.0.5.1-foss-2021a\nmodule load FSLeyes/1.3.3-foss-2021a\nfsleyes &amp;\n</code></pre> <p>In FSLeyes, open up the standard brain (Navigate to the top menu and click on 'File \u279c Add standard' and select <code>MNI152_T1_2mm_brain.nii.gz</code>). </p> <p>Then add in our contrast image (File  \u279c Add from file, and then go into the <code>PCC_group.gfeat</code> and then into <code>cope1.feat</code> and open the file <code>thresh_zstat1.nii.gz</code>). </p> <p>When opened, change the colour to 'Red-Yellow' and the 'Minimum' up to 2.3 (The max should be around 3.12). If you set the voxel location to [42, 39, 52] your screen should look like this:</p> <p> </p> <p>This is the map that we saw in the <code>report.html</code> file. In fact we can double check this by changing the voxel co-ordinates to [45, 38, 46].</p> <p>Our thresholded image in fsleyes </p> <p> </p> <p> The FEAT output  Our image matches the one on the far right below:  </p> <p> </p>"},{"location":"workshop8/functional-connectivity/#bonus-identifying-regions-of-interest-with-atlases-and-neurosynth","title":"Bonus: Identifying regions of interest with atlases and Neurosynth","text":"<p>So we know which voxels demonstrate significant correlation with the PCC, but what region(s) of the brain are they located in? </p> <p>Let's go through two ways in which we can work this out. </p> <p>Firstly, as you have already done in the course, we can simply just overlap an atlas on the image and see which regions the activated voxels fall under. </p> <p>To do this:</p> <ol> <li>Navigate to the top menu and click on 'Settings \u279c Ortho View 1 \u279c Atlases'. </li> <li>Then at the bottom middle of the window, select the 'Harvard-Oxford Cortical Structural Atlas' and on the window directly next to it on the right, click 'Show/Hide'. </li> <li>The atlas should have loaded up but is blocking the voxels. Change the 'Opacity' to about a quarter. </li> </ol> <p> </p> <p> By having a look at the 'Location' window (bottom left) we can now see that significant voxels of activity are mainly found in the: </p> <p>Right superior lateral occipital cortex</p> <p>Posterior cingulate cortex (PCC) / precuneus</p> <p>Alternatively, we can also use Neurosynth, a website where you can get the resting-state functional connectivity of any voxel location or brain region. It does this by extracting data from studies and performing a meta-analysis on brain imaging studies that have results associated with your voxel/region of interest.</p> <p>About Neurosynth</p> <p>While Neurosynth has been superseded by Neurosynth Compose we will use the original Neurosynth in this tutorial.</p> <p>If you click the following link, you will see regions demonstrating significant connectivity with the posterior cingulate.</p> <p>If you type [46, -70, 32] as co-ordinates in Neurosynth, and then into the MNI co-ordinates section in FSLeyes, not into the voxel location, because Neurosynth works with MNI space, you can see that in both cases the right superior lateral occipital cortex is activated. </p> <p>Image orientation</p> <p>Note that the orientations of left and right are different between Neurosynth and FSLeyes!</p> <p>Neurosynth</p> <p>FSLeyes</p> <p> This is a great result given that we only have three subjects!</p>"}]}